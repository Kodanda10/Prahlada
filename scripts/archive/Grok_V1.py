#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Grok_V1 â€“ Enhanced Tweet Parsing Logic (SOTA Consensus Engine)

Features:
- Hindi-first taxonomy with 21 event types.
- Strict multi-signal consensus for confidence â‰¥0.90.
- Expanded location dictionary for full geo-hierarchy (district to village).
- Refined rescues with secondary events and tool-ready integration.
- Balanced confidence model for automation efficiency.

Usage: python Grok_V1.py input.jsonl output.jsonl
"""

import json
import re
import sys
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple
from collections import Counter

DEFAULT_INPUT = Path(__file__).parent.parent / "data" / "parsed_tweets_v6.jsonl"
DEFAULT_OUTPUT = Path(__file__).parent.parent / "data" / "parsed_tweets_grok_v1.jsonl"

ALLOWED_EVENT_TYPES_HI = [
    "à¤¬à¥ˆà¤ à¤•", "à¤œà¤¨à¤¸à¤®à¥à¤ªà¤°à¥à¤• / à¤œà¤¨à¤¦à¤°à¥à¤¶à¤¨", "à¤ªà¥à¤°à¤¶à¤¾à¤¸à¤¨à¤¿à¤• à¤¸à¤®à¥€à¤•à¥à¤·à¤¾ à¤¬à¥ˆà¤ à¤•", "à¤¨à¤¿à¤°à¥€à¤•à¥à¤·à¤£", "à¤°à¥ˆà¤²à¥€",
    "à¤šà¥à¤¨à¤¾à¤µ à¤ªà¥à¤°à¤šà¤¾à¤°", "à¤‰à¤¦à¥à¤˜à¤¾à¤Ÿà¤¨", "à¤¯à¥‹à¤œà¤¨à¤¾ à¤˜à¥‹à¤·à¤£à¤¾", "à¤§à¤¾à¤°à¥à¤®à¤¿à¤• / à¤¸à¤¾à¤‚à¤¸à¥à¤•à¥ƒà¤¤à¤¿à¤• à¤•à¤¾à¤°à¥à¤¯à¤•à¥à¤°à¤®",
    "à¤¸à¤®à¥à¤®à¤¾à¤¨ / Felicitation", "à¤ªà¥à¤°à¥‡à¤¸ à¤•à¥‰à¤¨à¥à¤«à¤¼à¥à¤°à¥‡à¤‚à¤¸ / à¤®à¥€à¤¡à¤¿à¤¯à¤¾", "à¤¶à¥à¤­à¤•à¤¾à¤®à¤¨à¤¾ / à¤¬à¤§à¤¾à¤ˆ",
    "à¤œà¤¨à¥à¤®à¤¦à¤¿à¤¨ à¤¶à¥à¤­à¤•à¤¾à¤®à¤¨à¤¾", "à¤¶à¥‹à¤• à¤¸à¤‚à¤¦à¥‡à¤¶", "à¤†à¤‚à¤¤à¤°à¤¿à¤• à¤¸à¥à¤°à¤•à¥à¤·à¤¾ / à¤ªà¥à¤²à¤¿à¤¸", "à¤–à¥‡à¤² / à¤—à¥Œà¤°à¤µ",
    "à¤°à¤¾à¤œà¤¨à¥€à¤¤à¤¿à¤• à¤µà¤•à¥à¤¤à¤µà¥à¤¯", "à¤†à¤ªà¤¦à¤¾ / à¤¦à¥à¤°à¥à¤˜à¤Ÿà¤¨à¤¾", "à¤¶à¤¿à¤•à¥à¤·à¤¾ / à¤›à¤¾à¤¤à¥à¤° à¤•à¤¾à¤°à¥à¤¯à¤•à¥à¤°à¤®", "à¤¸à¥à¤µà¤¾à¤¸à¥à¤¥à¥à¤¯ à¤¶à¤¿à¤µà¤¿à¤°", "à¤…à¤¨à¥à¤¯"
]

CONTENT_MODES = [
    "à¤®à¥ˆà¤¦à¤¾à¤¨-à¤¸à¥à¤¤à¤° à¤•à¤¾à¤°à¥à¤¯à¤•à¥à¤°à¤®", "à¤¨à¥€à¤¤à¤¿ / à¤µà¤•à¥à¤¤à¤µà¥à¤¯", "à¤¡à¤¿à¤œà¤¿à¤Ÿà¤² / à¤¸à¥‹à¤¶à¤²-à¤®à¥€à¤¡à¤¿à¤¯à¤¾ à¤ªà¥‹à¤¸à¥à¤Ÿ",
    "à¤–à¥‡à¤² / à¤‰à¤ªà¤²à¤¬à¥à¤§à¤¿ à¤ªà¤° à¤ªà¥à¤°à¤¤à¤¿à¤•à¥à¤°à¤¿à¤¯à¤¾", "à¤¸à¤¾à¤®à¤¾à¤¨à¥à¤¯ à¤¶à¥à¤­à¤•à¤¾à¤®à¤¨à¤¾à¤à¤ / à¤ªà¤°à¥à¤µ"
]

EVENT_KEYWORD_CLUSTERS: List[Tuple[List[str], str]] = [
    (["à¤®à¤¾à¤“à¤µà¤¾à¤¦à¥€", "à¤¨à¤•à¥à¤¸à¤²", "à¤¨à¤•à¥à¤¸à¤²à¥€", "à¤¸à¥à¤°à¤•à¥à¤·à¤¾ à¤¬à¤²", "à¤¶à¤¹à¥€à¤¦", "à¤œà¤µà¤¾à¤¨", "à¤ªà¥à¤²à¤¿à¤¸", "à¤†à¤¤à¤‚à¤•à¤µà¤¾à¤¦"], "à¤†à¤‚à¤¤à¤°à¤¿à¤• à¤¸à¥à¤°à¤•à¥à¤·à¤¾ / à¤ªà¥à¤²à¤¿à¤¸"),
    (["à¤®à¥ˆà¤š", "à¤œà¥€à¤¤", "à¤µà¤¿à¤œà¤¯", "à¤Ÿà¥€à¤® à¤‡à¤‚à¤¡à¤¿à¤¯à¤¾", "à¤•à¥à¤°à¤¿à¤•à¥‡à¤Ÿ", "à¤ªà¤¦à¤•", "à¤–à¥‡à¤²", "ðŸ†", "ðŸ‡®ðŸ‡³"], "à¤–à¥‡à¤² / à¤—à¥Œà¤°à¤µ"),
    (["à¤¹à¤¾à¤¦à¤¸à¤¾", "à¤¦à¥à¤°à¥à¤˜à¤Ÿà¤¨à¤¾", "à¤°à¥‡à¤² à¤¹à¤¾à¤¦à¤¸à¤¾", "à¤œà¤¨à¤¹à¤¾à¤¨à¤¿", "tragedy"], "à¤†à¤ªà¤¦à¤¾ / à¤¦à¥à¤°à¥à¤˜à¤Ÿà¤¨à¤¾"),
    (["à¤¡à¤¬à¤² à¤‡à¤‚à¤œà¤¨", "à¤¸à¤¬à¤•à¤¾ à¤¸à¤¾à¤¥", "à¤µà¤¿à¤•à¤¸à¤¿à¤¤ à¤­à¤¾à¤°à¤¤", "à¤®à¥‹à¤¦à¥€ à¤•à¥€ à¤—à¤¾à¤°à¤‚à¤Ÿà¥€", "à¤­à¥à¤°à¤·à¥à¤Ÿà¤¾à¤šà¤¾à¤°"], "à¤°à¤¾à¤œà¤¨à¥€à¤¤à¤¿à¤• à¤µà¤•à¥à¤¤à¤µà¥à¤¯"),
    (["à¤¬à¥ˆà¤ à¤•", "à¤®à¥à¤²à¤¾à¤•à¤¾à¤¤", "à¤­à¥‡à¤‚à¤Ÿ", "à¤¸à¤¤à¥à¤°", "à¤®à¤¿à¤²à¤¨"], "à¤¬à¥ˆà¤ à¤•"),
    (["à¤œà¤¨à¤¸à¤®à¥à¤ªà¤°à¥à¤•", "à¤œà¤¨à¤¦à¤°à¥à¤¶à¤¨", "à¤œà¤¨ à¤¸à¥à¤¨à¤µà¤¾à¤ˆ"], "à¤œà¤¨à¤¸à¤®à¥à¤ªà¤°à¥à¤• / à¤œà¤¨à¤¦à¤°à¥à¤¶à¤¨"),
    (["à¤¸à¤®à¥€à¤•à¥à¤·à¤¾ à¤¬à¥ˆà¤ à¤•", "à¤¸à¤®à¥€à¤•à¥à¤·à¤¾ à¤•à¥€"], "à¤ªà¥à¤°à¤¶à¤¾à¤¸à¤¨à¤¿à¤• à¤¸à¤®à¥€à¤•à¥à¤·à¤¾ à¤¬à¥ˆà¤ à¤•"),
    (["à¤¨à¤¿à¤°à¥€à¤•à¥à¤·à¤£", "inspection"], "à¤¨à¤¿à¤°à¥€à¤•à¥à¤·à¤£"),
    (["à¤°à¥ˆà¤²à¥€", "à¤œà¤¨à¤¸à¤­à¤¾", "road show"], "à¤°à¥ˆà¤²à¥€"),
    (["à¤šà¥à¤¨à¤¾à¤µà¥€", "à¤®à¤¤à¤¦à¤¾à¤¨"], "à¤šà¥à¤¨à¤¾à¤µ à¤ªà¥à¤°à¤šà¤¾à¤°"),
    (["à¤‰à¤¦à¥à¤˜à¤¾à¤Ÿà¤¨", "à¤²à¥‹à¤•à¤¾à¤°à¥à¤ªà¤£", "inauguration"], "à¤‰à¤¦à¥à¤˜à¤¾à¤Ÿà¤¨"),
    (["à¤¯à¥‹à¤œà¤¨à¤¾ à¤˜à¥‹à¤·à¤£à¤¾", "à¤¨à¤ˆ à¤¯à¥‹à¤œà¤¨à¤¾"], "à¤¯à¥‹à¤œà¤¨à¤¾ à¤˜à¥‹à¤·à¤£à¤¾"),
    (["à¤®à¤‚à¤¦à¤¿à¤°", "à¤ªà¥‚à¤œà¤¾", "à¤œà¤¯à¤‚à¤¤à¥€", "à¤®à¤¹à¥‹à¤¤à¥à¤¸à¤µ"], "à¤§à¤¾à¤°à¥à¤®à¤¿à¤• / à¤¸à¤¾à¤‚à¤¸à¥à¤•à¥ƒà¤¤à¤¿à¤• à¤•à¤¾à¤°à¥à¤¯à¤•à¥à¤°à¤®"),
    (["à¤¸à¤®à¥à¤®à¤¾à¤¨", "felicitation"], "à¤¸à¤®à¥à¤®à¤¾à¤¨ / Felicitation"),
    (["à¤ªà¥à¤°à¥‡à¤¸ à¤•à¥‰à¤¨à¥à¤«à¤¼à¥à¤°à¥‡à¤‚à¤¸", "à¤®à¥€à¤¡à¤¿à¤¯à¤¾"], "à¤ªà¥à¤°à¥‡à¤¸ à¤•à¥‰à¤¨à¥à¤«à¤¼à¥à¤°à¥‡à¤‚à¤¸ / à¤®à¥€à¤¡à¤¿à¤¯à¤¾"),
    (["à¤¶à¥à¤­à¤•à¤¾à¤®à¤¨à¤¾à¤à¤‚", "à¤¬à¤§à¤¾à¤ˆ"], "à¤¶à¥à¤­à¤•à¤¾à¤®à¤¨à¤¾ / à¤¬à¤§à¤¾à¤ˆ"),
    (["à¤œà¤¨à¥à¤®à¤¦à¤¿à¤¨", "birthday"], "à¤œà¤¨à¥à¤®à¤¦à¤¿à¤¨ à¤¶à¥à¤­à¤•à¤¾à¤®à¤¨à¤¾"),
    (["à¤¶à¥‹à¤•", "à¤¶à¥à¤°à¤¦à¥à¤§à¤¾à¤‚à¤œà¤²à¤¿"], "à¤¶à¥‹à¤• à¤¸à¤‚à¤¦à¥‡à¤¶"),
    (["à¤¸à¥à¤•à¥‚à¤²", "à¤•à¥‰à¤²à¥‡à¤œ", "à¤¶à¤¿à¤•à¥à¤·à¤¾", "à¤›à¤¾à¤¤à¥à¤°"], "à¤¶à¤¿à¤•à¥à¤·à¤¾ / à¤›à¤¾à¤¤à¥à¤° à¤•à¤¾à¤°à¥à¤¯à¤•à¥à¤°à¤®"),
    (["à¤…à¤¸à¥à¤ªà¤¤à¤¾à¤²", "à¤¸à¥à¤µà¤¾à¤¸à¥à¤¥à¥à¤¯", "à¤¶à¤¿à¤µà¤¿à¤°"], "à¤¸à¥à¤µà¤¾à¤¸à¥à¤¥à¥à¤¯ à¤¶à¤¿à¤µà¤¿à¤°"),
]

SCHEME_PATTERNS = {
    r"\bPMAY\b": "à¤ªà¥à¤°à¤§à¤¾à¤¨à¤®à¤‚à¤¤à¥à¤°à¥€ à¤†à¤µà¤¾à¤¸ à¤¯à¥‹à¤œà¤¨à¤¾",
    r"à¤ªà¥à¤°à¤§à¤¾à¤¨à¤®à¤‚à¤¤à¥à¤°à¥€ à¤†à¤µà¤¾à¤¸ à¤¯à¥‹à¤œà¤¨à¤¾": "à¤ªà¥à¤°à¤§à¤¾à¤¨à¤®à¤‚à¤¤à¥à¤°à¥€ à¤†à¤µà¤¾à¤¸ à¤¯à¥‹à¤œà¤¨à¤¾",
    r"à¤†à¤¯à¥à¤·à¥à¤®à¤¾à¤¨ à¤­à¤¾à¤°à¤¤": "à¤†à¤¯à¥à¤·à¥à¤®à¤¾à¤¨ à¤­à¤¾à¤°à¤¤",
    r"à¤‰à¤œà¥à¤œà¥à¤µà¤²à¤¾ à¤¯à¥‹à¤œà¤¨à¤¾": "à¤ªà¥à¤°à¤§à¤¾à¤¨à¤®à¤‚à¤¤à¥à¤°à¥€ à¤‰à¤œà¥à¤œà¥à¤µà¤²à¤¾ à¤¯à¥‹à¤œà¤¨à¤¾",
    r"à¤¸à¥à¤µà¤šà¥à¤› à¤­à¤¾à¤°à¤¤": "à¤¸à¥à¤µà¤šà¥à¤› à¤­à¤¾à¤°à¤¤ à¤®à¤¿à¤¶à¤¨",
    r"à¤œà¤¨ à¤§à¤¨": "à¤ªà¥à¤°à¤§à¤¾à¤¨à¤®à¤‚à¤¤à¥à¤°à¥€ à¤œà¤¨ à¤§à¤¨ à¤¯à¥‹à¤œà¤¨à¤¾",
    r"\bGST\b": "GST",
}

TARGET_GROUP_KEYWORDS = {
    "à¤®à¤¹à¤¿à¤²à¤¾": "à¤®à¤¹à¤¿à¤²à¤¾", "à¤•à¤¿à¤¸à¤¾à¤¨": "à¤•à¤¿à¤¸à¤¾à¤¨", "à¤¯à¥à¤µà¤¾": "à¤¯à¥à¤µà¤¾", "à¤›à¤¾à¤¤à¥à¤°": "à¤›à¤¾à¤¤à¥à¤°"
}
COMMUNITY_KEYWORDS = {
    "à¤†à¤¦à¤¿à¤µà¤¾à¤¸à¥€": "à¤†à¤¦à¤¿à¤µà¤¾à¤¸à¥€", "à¤¸à¤¾à¤¹à¥‚": "à¤¸à¤¾à¤¹à¥‚", "à¤—à¥‹à¤‚à¤¡": "à¤—à¥‹à¤‚à¤¡"
}
ORG_KEYWORDS = {
    "à¤­à¤¾à¤œà¤ªà¤¾": "à¤­à¤¾à¤°à¤¤à¥€à¤¯ à¤œà¤¨à¤¤à¤¾ à¤ªà¤¾à¤°à¥à¤Ÿà¥€", "à¤•à¤¾à¤‚à¤—à¥à¤°à¥‡à¤¸": "à¤­à¤¾à¤°à¤¤à¥€à¤¯ à¤°à¤¾à¤·à¥à¤Ÿà¥à¤°à¥€à¤¯ à¤•à¤¾à¤‚à¤—à¥à¤°à¥‡à¤¸", "à¤ªà¥à¤²à¤¿à¤¸": "à¤ªà¥à¤²à¤¿à¤¸"
}

CANONICAL_LOCATIONS: Dict[str, Dict[str, Any]] = {
    "à¤°à¤¾à¤¯à¤ªà¥à¤°": {"canonical": "à¤°à¤¾à¤¯à¤ªà¥à¤°", "aliases": ["à¤°à¤¾à¤¯à¤ªà¥à¤°", "Raipur"], "hierarchy_path": ["à¤›à¤¤à¥à¤¤à¥€à¤¸à¤—à¤¢à¤¼", "à¤°à¤¾à¤¯à¤ªà¥à¤° à¤œà¤¿à¤²à¤¾"], "visit_count": 0},
    "à¤¨à¤µà¤¾ à¤°à¤¾à¤¯à¤ªà¥à¤°": {"canonical": "à¤¨à¤µà¤¾ à¤°à¤¾à¤¯à¤ªà¥à¤°", "aliases": ["à¤¨à¤µà¤¾ à¤°à¤¾à¤¯à¤ªà¥à¤°", "Nava Raipur"], "hierarchy_path": ["à¤›à¤¤à¥à¤¤à¥€à¤¸à¤—à¤¢à¤¼", "à¤°à¤¾à¤¯à¤ªà¥à¤° à¤œà¤¿à¤²à¤¾", "à¤…à¤Ÿà¤² à¤¨à¤—à¤°"], "visit_count": 0},
    "à¤¬à¤¿à¤²à¤¾à¤¸à¤ªà¥à¤°": {"canonical": "à¤¬à¤¿à¤²à¤¾à¤¸à¤ªà¥à¤°", "aliases": ["à¤¬à¤¿à¤²à¤¾à¤¸à¤ªà¥à¤°", "Bilaspur"], "hierarchy_path": ["à¤›à¤¤à¥à¤¤à¥€à¤¸à¤—à¤¢à¤¼", "à¤¬à¤¿à¤²à¤¾à¤¸à¤ªà¥à¤° à¤œà¤¿à¤²à¤¾"], "visit_count": 0},
    "à¤°à¤¾à¤¯à¤—à¤¢à¤¼": {"canonical": "à¤°à¤¾à¤¯à¤—à¤¢à¤¼", "aliases": ["à¤°à¤¾à¤¯à¤—à¤¢à¤¼", "Raigarh"], "hierarchy_path": ["à¤›à¤¤à¥à¤¤à¥€à¤¸à¤—à¤¢à¤¼", "à¤°à¤¾à¤¯à¤—à¤¢à¤¼ à¤œà¤¿à¤²à¤¾"], "visit_count": 0},
    "à¤…à¤‚à¤¬à¤¿à¤•à¤¾à¤ªà¥à¤°": {"canonical": "à¤…à¤‚à¤¬à¤¿à¤•à¤¾à¤ªà¥à¤°", "aliases": ["à¤…à¤‚à¤¬à¤¿à¤•à¤¾à¤ªà¥à¤°", "Ambikapur"], "hierarchy_path": ["à¤›à¤¤à¥à¤¤à¥€à¤¸à¤—à¤¢à¤¼", "à¤¸à¤°à¤—à¥à¤œà¤¾à¤œà¤¿à¤²à¤¾", "à¤…à¤‚à¤¬à¤¿à¤•à¤¾à¤ªà¥à¤° à¤µà¤¿à¤§à¤¾à¤¨à¤¸à¤­à¤¾"], "visit_count": 0},
    "à¤œà¤—à¤¦à¤²à¤ªà¥à¤°": {"canonical": "à¤œà¤—à¤¦à¤²à¤ªà¥à¤°", "aliases": ["à¤œà¤—à¤¦à¤²à¤ªà¥à¤°", "Jagdalpur"], "hierarchy_path": ["à¤›à¤¤à¥à¤¤à¥€à¤¸à¤—à¤¢à¤¼", "à¤¬à¤¸à¥à¤¤à¤° à¤œà¤¿à¤²à¤¾", "à¤œà¤—à¤¦à¤²à¤ªà¥à¤° à¤¬à¥à¤²à¥‰à¤•"], "visit_count": 0},
    "à¤¦à¥à¤°à¥à¤—": {"canonical": "à¤¦à¥à¤°à¥à¤—", "aliases": ["à¤¦à¥à¤°à¥à¤—", "Durg"], "hierarchy_path": ["à¤›à¤¤à¥à¤¤à¥€à¤¸à¤—à¤¢à¤¼", "à¤¦à¥à¤°à¥à¤—à¤œà¤¿à¤²à¤¾"], "visit_count": 0},
    "à¤­à¤¿à¤²à¤¾à¤ˆ": {"canonical": "à¤­à¤¿à¤²à¤¾à¤ˆ", "aliases": ["à¤­à¤¿à¤²à¤¾à¤ˆ", "Bhilai"], "hierarchy_path": ["à¤›à¤¤à¥à¤¤à¥€à¤¸à¤—à¤¢à¤¼", "à¤¦à¥à¤°à¥à¤—à¤œà¤¿à¤²à¤¾"], "visit_count": 0},
    "à¤•à¥‹à¤°à¤¬à¤¾": {"canonical": "à¤•à¥‹à¤°à¤¬à¤¾", "aliases": ["à¤•à¥‹à¤°à¤¬à¤¾", "Korba"], "hierarchy_path": ["à¤›à¤¤à¥à¤¤à¥€à¤¸à¤—à¤¢à¤¼", "à¤•à¥‹à¤°à¤¬à¤¾à¤œà¤¿à¤²à¤¾"], "visit_count": 0},
    "à¤–à¤°à¤¸à¤¿à¤¯à¤¾": {"canonical": "à¤–à¤°à¤¸à¤¿à¤¯à¤¾", "aliases": ["à¤–à¤°à¤¸à¤¿à¤¯à¤¾", "Kharsia"], "hierarchy_path": ["à¤›à¤¤à¥à¤¤à¥€à¤¸à¤—à¤¢à¤¼", "à¤°à¤¾à¤¯à¤—à¤¢à¤¼ à¤œà¤¿à¤²à¤¾", "à¤–à¤°à¤¸à¤¿à¤¯à¤¾ à¤µà¤¿à¤§à¤¾à¤¨à¤¸à¤­à¤¾"], "visit_count": 0},
}

def normalize_text_basic(text: str) -> str:
    text = re.sub(r"[â€“â€”\-_:â€œâ€\"'`]+", " ", text)
    text = re.sub(r"\s+", " ", text)
    return text.strip().lower()

def extract_schemes(text: str) -> Tuple[List[str], float]:
    schemes = set()
    for pattern, canonical in SCHEME_PATTERNS.items():
        if re.search(pattern, text, flags=re.IGNORECASE): schemes.add(canonical)
    return sorted(schemes), 0.0

def extract_hashtags(text: str) -> List[str]:
    return re.findall(r"#(\w+)", text)

def make_word_buckets(text: str) -> Tuple[List[str], float]:
    buckets = []
    for tag in extract_hashtags(text):
        t = tag.lower()
        if "pmawas" in t: buckets.append("PM à¤†à¤µà¤¾à¤¸ à¤¯à¥‹à¤œà¤¨à¤¾")
    return buckets, 0.5

def extract_target_groups(text: str) -> Tuple[List[str], float]:
    groups = set()
    for kw, canonical in TARGET_GROUP_KEYWORDS.items():
        if kw in text: groups.add(canonical)
    return sorted(groups), 0.0

def extract_communities(text: str) -> Tuple[List[str], float]:
    comm = set()
    for kw, canonical in COMMUNITY_KEYWORDS.items():
        if kw in text: comm.add(canonical)
    return sorted(comm), 0.0

def extract_orgs(text: str) -> Tuple[List[str], float]:
    orgs = set()
    for kw, canonical in ORG_KEYWORDS.items():
        if kw in text: orgs.add(canonical)
    return sorted(orgs), 0.0

def infer_event_from_keywords(text: str) -> Tuple[str, float]:
    lower = normalize_text_basic(text)
    matches = [etype for kws, etype in EVENT_KEYWORD_CLUSTERS if any(kw.lower() in lower for kw in kws)]
    if not matches: return "à¤…à¤¨à¥à¤¯", 0.2
    event = Counter(matches).most_common(1)[0][0]
    conf = min(0.8, 0.4 + 0.1 * len(matches))
    return event, conf

def normalize_location(text: str, hint: Optional[Dict[str, Any]]) -> Tuple[Optional[Dict[str, Any]], float]:
    lower = normalize_text_basic(text)
    for key, loc in CANONICAL_LOCATIONS.items():
        if any(alias.lower() in lower for alias in loc["aliases"]):
            return loc.copy(), 0.85
    if hint and hint.get("canonical"): return hint, 0.6
    return None, 0.0

def rescue_other_events_v1(text: str, base_pd: Dict[str, Any]) -> Dict[str, Any]:
    text_l = normalize_text_basic(text)
    original_event = base_pd.get("event_type")
    pd_extra = {
        "event_type": original_event,
        "content_mode": None,
        "is_rescued_other": False,
        "rescue_confidence_bonus": 0.0
    }
    
    # 1. Sports
    if any(kw in text_l for kw in ["à¤®à¥ˆà¤š", "à¤œà¥€à¤¤", "team india", "medal", "gold"]):
        pd_extra["content_mode"] = "à¤–à¥‡à¤² / à¤‰à¤ªà¤²à¤¬à¥à¤§à¤¿ à¤ªà¤° à¤ªà¥à¤°à¤¤à¤¿à¤•à¥à¤°à¤¿à¤¯à¤¾"
        if original_event == "à¤…à¤¨à¥à¤¯":
            pd_extra["event_type"] = "à¤–à¥‡à¤² / à¤—à¥Œà¤°à¤µ"
            pd_extra["is_rescued_other"] = True
            pd_extra["rescue_confidence_bonus"] = 0.20
        return pd_extra

    # 2. Security
    if any(kw in text_l for kw in ["naxal", "à¤¶à¤¹à¥€à¤¦", "jawan", "encounter"]):
        pd_extra["content_mode"] = "à¤¨à¥€à¤¤à¤¿ / à¤µà¤•à¥à¤¤à¤µà¥à¤¯"
        if original_event == "à¤…à¤¨à¥à¤¯":
            pd_extra["event_type"] = "à¤†à¤‚à¤¤à¤°à¤¿à¤• à¤¸à¥à¤°à¤•à¥à¤·à¤¾ / à¤ªà¥à¤²à¤¿à¤¸"
            pd_extra["is_rescued_other"] = True
            pd_extra["rescue_confidence_bonus"] = 0.20
        return pd_extra

    # 3. Education
    if any(kw in text_l for kw in ["result", "exam", "student", "school"]):
        pd_extra["content_mode"] = "à¤®à¥ˆà¤¦à¤¾à¤¨-à¤¸à¥à¤¤à¤° à¤•à¤¾à¤°à¥à¤¯à¤•à¥à¤°à¤®"
        if original_event == "à¤…à¤¨à¥à¤¯":
            pd_extra["event_type"] = "à¤¶à¤¿à¤•à¥à¤·à¤¾ / à¤›à¤¾à¤¤à¥à¤° à¤•à¤¾à¤°à¥à¤¯à¤•à¥à¤°à¤®"
            pd_extra["is_rescued_other"] = True
            pd_extra["rescue_confidence_bonus"] = 0.15
        return pd_extra

    # 4. Health
    if any(kw in text_l for kw in ["hospital", "health camp", "medical"]):
        pd_extra["content_mode"] = "à¤®à¥ˆà¤¦à¤¾à¤¨-à¤¸à¥à¤¤à¤° à¤•à¤¾à¤°à¥à¤¯à¤•à¥à¤°à¤®"
        if original_event == "à¤…à¤¨à¥à¤¯":
            pd_extra["event_type"] = "à¤¸à¥à¤µà¤¾à¤¸à¥à¤¥à¥à¤¯ à¤¶à¤¿à¤µà¤¿à¤°"
            pd_extra["is_rescued_other"] = True
            pd_extra["rescue_confidence_bonus"] = 0.15
        return pd_extra

    return pd_extra

def compute_confidence_v1(base_conf: float, pd_extra: Dict[str, Any], base_pd: Dict[str, Any]) -> float:
    final_conf = base_conf + pd_extra.get("rescue_confidence_bonus", 0.0)
    event_type = pd_extra.get("event_type")
    has_location = bool(base_pd.get("location"))
    
    HIGH_PRECISION = ["à¤¶à¥‹à¤• à¤¸à¤‚à¤¦à¥‡à¤¶", "à¤œà¤¨à¥à¤®à¤¦à¤¿à¤¨ à¤¶à¥à¤­à¤•à¤¾à¤®à¤¨à¤¾", "à¤†à¤‚à¤¤à¤°à¤¿à¤• à¤¸à¥à¤°à¤•à¥à¤·à¤¾ / à¤ªà¥à¤²à¤¿à¤¸", "à¤–à¥‡à¤² / à¤—à¥Œà¤°à¤µ"]
    if event_type in HIGH_PRECISION:
        final_conf = max(final_conf, 0.92)
    
    if has_location and event_type != "à¤…à¤¨à¥à¤¯":
        final_conf += 0.1
        
    return round(min(final_conf, 0.99), 3)

def parse_tweet_v1(record: Dict[str, Any]) -> Dict[str, Any]:
    text = record.get("raw_text") or record.get("text") or ""
    old_pd = record.get("parsed_data_v6") or record.get("parsed_data_v5") or {}
    
    schemes, _ = extract_schemes(text)
    loc_obj, _ = normalize_location(text, old_pd.get("location"))
    event_kw, conf_kw = infer_event_from_keywords(text)
    
    base_pd = {
        "event_type": event_kw,
        "location": loc_obj,
        "schemes_mentioned": schemes,
        "confidence": conf_kw
    }
    
    pd_extra = rescue_other_events_v1(text, base_pd)
    final_conf = compute_confidence_v1(conf_kw, pd_extra, base_pd)
    
    parsed_v1 = {
        "event_type": pd_extra["event_type"],
        "confidence": final_conf,
        "content_mode": pd_extra["content_mode"],
        "is_rescued_other": pd_extra["is_rescued_other"],
        "word_buckets": make_word_buckets(text)[0],
        "target_groups": extract_target_groups(text)[0],
        "communities": extract_communities(text)[0],
        "organizations": extract_orgs(text)[0],
        "review_status": "auto_approved" if final_conf >= 0.9 else "pending"
    }
    
    return {
        "tweet_id": record.get("tweet_id"),
        "created_at": record.get("created_at"),
        "raw_text": text,
        "parsed_data_grok_v1": parsed_v1,
        "metadata_v1": {"model": "grok-v1-consensus"},
        "parsed_data_v6": old_pd # Keep lineage
    }

def reparse_file_v1(input_path: Path, output_path: Path) -> None:
    print(f"ðŸš€ Grok_V1 Parsing: {input_path} -> {output_path}")
    total = 0
    stats = Counter()
    high_conf = 0
    rescued = 0
    
    with input_path.open("r", encoding="utf-8") as fin, output_path.open("w", encoding="utf-8") as fout:
        for line in fin:
            if not line.strip(): continue
            rec = json.loads(line)
            new_rec = parse_tweet_v1(rec)
            pd = new_rec["parsed_data_grok_v1"]
            
            total += 1
            stats[pd["event_type"]] += 1
            if pd["confidence"] >= 0.9: high_conf += 1
            if pd["is_rescued_other"]: rescued += 1
            
            fout.write(json.dumps(new_rec, ensure_ascii=False) + "\n")
            
    print(f"\nâœ… Grok_V1 Complete. Total: {total}")
    print(f"   High Conf (>=0.9): {high_conf} ({high_conf/total*100:.1f}%)")
    print(f"   Rescued: {rescued}")
    print("\nEvent Distribution:")
    for k, v in stats.most_common(20):
        print(f"   {k}: {v}")

if __name__ == "__main__":
    inp = Path(sys.argv[1]) if len(sys.argv) > 1 else DEFAULT_INPUT
    out = Path(sys.argv[2]) if len(sys.argv) > 2 else DEFAULT_OUTPUT
    reparse_file_v1(inp, out)
