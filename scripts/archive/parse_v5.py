#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Project Dhruv â€“ Parsing Logic V5 (Hindi-first, robust, single script)

à¤®à¥à¤–à¥à¤¯ à¤µà¤¿à¤¶à¥‡à¤·à¤¤à¤¾à¤à¤:
- Hindi-first event_type taxonomy
- Location normalization + inline pattern based extraction
- Schemes, word buckets, target groups, communities, organizations
- Confidence model (base V4 style) + V5 rescue bonus
- "à¤…à¤¨à¥à¤¯" tweets à¤•à¥‡ à¤²à¤¿à¤ special rescue logic
- à¤¨à¤¯à¤¾ axis: content_mode (à¤®à¥ˆà¤¦à¤¾à¤¨-à¤¸à¥à¤¤à¤° à¤•à¤¾à¤°à¥à¤¯à¤•à¥à¤°à¤® / à¤¨à¥€à¤¤à¤¿ / à¤¡à¤¿à¤œà¤¿à¤Ÿà¤² / à¤–à¥‡à¤² / à¤¶à¥à¤­à¤•à¤¾à¤®à¤¨à¤¾à¤à¤)
- Review flags: is_other_original, is_rescued_other, rescue_tag

à¤‡à¤¨à¤ªà¥à¤Ÿ JSONL (à¤ªà¥à¤°à¤¤à¤¿ à¤²à¤¾à¤‡à¤¨ à¤à¤• tweet record) â€“ minimally:
{
  "tweet_id": "...",
  "created_at": "2025-11-20T10:30:00Z",
  "raw_text": "..."   # à¤¯à¤¾ "text"
  # à¤µà¥ˆà¤•à¤²à¥à¤ªà¤¿à¤•:
  # "parsed_data_v4" / "parsed_data_v3" / "parsed_data_v2" ...
}

à¤†à¤‰à¤Ÿà¤ªà¥à¤Ÿ JSONL:
{
  "tweet_id": "...",
  "created_at": "...",
  "raw_text": "...",
  "parsed_data_v5": { ... },
  "metadata_v5": { ... },
  # à¤µà¥ˆà¤•à¤²à¥à¤ªà¤¿à¤• à¤°à¥‚à¤ª à¤¸à¥‡ à¤ªà¥à¤°à¤¾à¤¨à¥‡ parsed_data_x à¤­à¥€ preserve à¤¹à¥‹ à¤¸à¤•à¤¤à¥‡ à¤¹à¥ˆà¤‚ à¤…à¤—à¤° input à¤®à¥‡à¤‚ à¤¹à¥ˆà¤‚
}
"""

import json
import re
import sys
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple
from collections import Counter, defaultdict

# -------------------------
# Paths
# -------------------------

DEFAULT_INPUT = Path("/mnt/data/parsed_tweets_v4.jsonl")
DEFAULT_OUTPUT = Path("/mnt/data/parsed_tweets_v5.jsonl")

# -------------------------
# Taxonomies / Enums
# -------------------------

ALLOWED_EVENT_TYPES_HI = [
    "à¤¬à¥ˆà¤ à¤•",
    "à¤œà¤¨à¤¸à¤®à¥à¤ªà¤°à¥à¤• / à¤œà¤¨à¤¦à¤°à¥à¤¶à¤¨",
    "à¤ªà¥à¤°à¤¶à¤¾à¤¸à¤¨à¤¿à¤• à¤¸à¤®à¥€à¤•à¥à¤·à¤¾ à¤¬à¥ˆà¤ à¤•",
    "à¤¨à¤¿à¤°à¥€à¤•à¥à¤·à¤£",
    "à¤°à¥ˆà¤²à¥€",
    "à¤šà¥à¤¨à¤¾à¤µ à¤ªà¥à¤°à¤šà¤¾à¤°",
    "à¤‰à¤¦à¥à¤˜à¤¾à¤Ÿà¤¨",
    "à¤¯à¥‹à¤œà¤¨à¤¾ à¤˜à¥‹à¤·à¤£à¤¾",
    "à¤§à¤¾à¤°à¥à¤®à¤¿à¤• / à¤¸à¤¾à¤‚à¤¸à¥à¤•à¥ƒà¤¤à¤¿à¤• à¤•à¤¾à¤°à¥à¤¯à¤•à¥à¤°à¤®",
    "à¤¸à¤®à¥à¤®à¤¾à¤¨ / Felicitation",
    "à¤ªà¥à¤°à¥‡à¤¸ à¤•à¥‰à¤¨à¥à¤«à¤¼à¥à¤°à¥‡à¤‚à¤¸ / à¤®à¥€à¤¡à¤¿à¤¯à¤¾",
    "à¤¶à¥à¤­à¤•à¤¾à¤®à¤¨à¤¾ / à¤¬à¤§à¤¾à¤ˆ",
    "à¤œà¤¨à¥à¤®à¤¦à¤¿à¤¨ à¤¶à¥à¤­à¤•à¤¾à¤®à¤¨à¤¾",
    "à¤¶à¥‹à¤• à¤¸à¤‚à¤¦à¥‡à¤¶",
    "à¤…à¤¨à¥à¤¯",
]

CONTENT_MODES = [
    "à¤®à¥ˆà¤¦à¤¾à¤¨-à¤¸à¥à¤¤à¤° à¤•à¤¾à¤°à¥à¤¯à¤•à¥à¤°à¤®",
    "à¤¨à¥€à¤¤à¤¿ / à¤µà¤•à¥à¤¤à¤µà¥à¤¯",
    "à¤¡à¤¿à¤œà¤¿à¤Ÿà¤² / à¤¸à¥‹à¤¶à¤²-à¤®à¥€à¤¡à¤¿à¤¯à¤¾ à¤ªà¥‹à¤¸à¥à¤Ÿ",
    "à¤–à¥‡à¤² / à¤‰à¤ªà¤²à¤¬à¥à¤§à¤¿ à¤ªà¤° à¤ªà¥à¤°à¤¤à¤¿à¤•à¥à¤°à¤¿à¤¯à¤¾",
    "à¤¸à¤¾à¤®à¤¾à¤¨à¥à¤¯ à¤¶à¥à¤­à¤•à¤¾à¤®à¤¨à¤¾à¤à¤ / à¤ªà¤°à¥à¤µ",
]

# keyword clusters â†’ Hindi label (base event detection)
EVENT_KEYWORD_CLUSTERS: List[Tuple[List[str], str]] = [
    # à¤¬à¥ˆà¤ à¤• / à¤®à¥à¤²à¤¾à¤•à¤¼à¤¾à¤¤ / à¤¸à¤¤à¥à¤°
    (["à¤¬à¥ˆà¤ à¤•", "à¤®à¥à¤²à¤¾à¤•à¤¾à¤¤", "à¤­à¥‡à¤‚à¤Ÿ", "à¤¬à¥ˆà¤ à¤• à¤²à¥€", "à¤¬à¥ˆà¤ à¤• à¤®à¥‡à¤‚", "à¤¬à¥ˆà¤ à¤• à¤•à¤¾", "à¤…à¤§à¥à¤¯à¤•à¥à¤·à¤¤à¤¾ à¤•à¥€", "à¤¸à¤¤à¥à¤°", "à¤¸à¤¦à¤¨ à¤•à¥€ à¤•à¤¾à¤°à¥à¤¯à¤µà¤¾à¤¹à¥€"], "à¤¬à¥ˆà¤ à¤•"),
    # à¤œà¤¨à¤¸à¤®à¥à¤ªà¤°à¥à¤• / à¤œà¤¨à¤¦à¤°à¥à¤¶à¤¨
    (["à¤œà¤¨à¤¸à¤®à¥à¤ªà¤°à¥à¤•", "à¤œà¤¨ à¤¸à¤‚à¤ªà¤°à¥à¤•", "à¤œà¤¨à¤¸à¤‚à¤ªà¤°à¥à¤•", "à¤œà¤¨à¤¦à¤°à¥à¤¶à¤¨", "à¤œà¤¨-à¤¦à¤°à¥à¤¶à¤¨", "à¤œà¤¨ à¤¸à¥à¤¨à¤µà¤¾à¤ˆ", "à¤œà¤¨à¤¸à¥à¤¨à¤µà¤¾à¤ˆ"], "à¤œà¤¨à¤¸à¤®à¥à¤ªà¤°à¥à¤• / à¤œà¤¨à¤¦à¤°à¥à¤¶à¤¨"),
    # à¤ªà¥à¤°à¤¶à¤¾à¤¸à¤¨à¤¿à¤• à¤¸à¤®à¥€à¤•à¥à¤·à¤¾ / à¤µà¤¿à¤­à¤¾à¤—à¥€à¤¯
    (["à¤¸à¤®à¥€à¤•à¥à¤·à¤¾ à¤¬à¥ˆà¤ à¤•", "à¤¸à¤®à¥€à¤•à¥à¤·à¤¾ à¤•à¥€", "à¤¸à¤®à¥€à¤•à¥à¤·à¤¾ à¤•à¥€ à¤—à¤ˆ", "à¤…à¤§à¤¿à¤•à¤¾à¤°à¤¿à¤¯à¥‹à¤‚ à¤•à¥‡ à¤¸à¤¾à¤¥", "à¤µà¤¿à¤­à¤¾à¤—à¥€à¤¯ à¤¬à¥ˆà¤ à¤•", "à¤•à¤²à¥‡à¤•à¥à¤Ÿà¤°", "à¤•à¤²à¥‡à¤•à¥à¤Ÿà¤°à¥‡à¤Ÿ", "à¤¸à¤®à¥€à¤•à¥à¤·à¤¾ à¤•à¤¾à¤°à¥à¤¯"], "à¤ªà¥à¤°à¤¶à¤¾à¤¸à¤¨à¤¿à¤• à¤¸à¤®à¥€à¤•à¥à¤·à¤¾ à¤¬à¥ˆà¤ à¤•"),
    # à¤¨à¤¿à¤°à¥€à¤•à¥à¤·à¤£
    (["à¤¨à¤¿à¤°à¥€à¤•à¥à¤·à¤£", "à¤¨à¤¿à¤°à¥€à¤•à¥à¤·à¤£ à¤•à¤¿à¤¯à¤¾", "à¤¨à¤¿à¤°à¥€à¤•à¥à¤·à¤£ à¤¹à¥‡à¤¤à¥", "inspection"], "à¤¨à¤¿à¤°à¥€à¤•à¥à¤·à¤£"),
    # à¤°à¥ˆà¤²à¥€ â€“ political à¤¸à¤‚à¤¦à¤°à¥à¤­
    (["à¤°à¥ˆà¤²à¥€", "à¤œà¤¨à¤¸à¤­à¤¾", "public rally", "road show", "à¤°à¥‹à¤¡ à¤¶à¥‹"], "à¤°à¥ˆà¤²à¥€"),
    # à¤šà¥à¤¨à¤¾à¤µ à¤ªà¥à¤°à¤šà¤¾à¤°
    (["à¤šà¥à¤¨à¤¾à¤µà¥€", "à¤®à¤¤à¤¦à¤¾à¤¤à¤¾", "à¤®à¤¤à¤¦à¤¾à¤¨", "à¤šà¥à¤¨à¤¾à¤µ à¤ªà¥à¤°à¤šà¤¾à¤°", "poll campaign"], "à¤šà¥à¤¨à¤¾à¤µ à¤ªà¥à¤°à¤šà¤¾à¤°"),
    # à¤‰à¤¦à¥à¤˜à¤¾à¤Ÿà¤¨ / à¤²à¥‹à¤•à¤¾à¤°à¥à¤ªà¤£
    (["à¤‰à¤¦à¥à¤˜à¤¾à¤Ÿà¤¨", "à¤²à¥‹à¤•à¤¾à¤°à¥à¤ªà¤£", "inauguration", "inaugurated", "à¤¶à¤¿à¤²à¤¾à¤¨à¥à¤¯à¤¾à¤¸"], "à¤‰à¤¦à¥à¤˜à¤¾à¤Ÿà¤¨"),
    # à¤¯à¥‹à¤œà¤¨à¤¾ à¤˜à¥‹à¤·à¤£à¤¾
    (["à¤˜à¥‹à¤·à¤£à¤¾", "à¤¨à¤ˆ à¤¯à¥‹à¤œà¤¨à¤¾", "à¤¯à¥‹à¤œà¤¨à¤¾ à¤•à¥€ à¤œà¤¾à¤¨à¤•à¤¾à¤°à¥€", "à¤¯à¥‹à¤œà¤¨à¤¾ à¤•à¤¾ à¤²à¤¾à¤­"], "à¤¯à¥‹à¤œà¤¨à¤¾ à¤˜à¥‹à¤·à¤£à¤¾"),
    # à¤§à¤¾à¤°à¥à¤®à¤¿à¤• / à¤¸à¤¾à¤‚à¤¸à¥à¤•à¥ƒà¤¤à¤¿à¤•
    (["à¤®à¤‚à¤¦à¤¿à¤°", "à¤ªà¥‚à¤œà¤¾", "à¤†à¤°à¤¤à¥€", "à¤—à¥à¤°à¥à¤¦à¥à¤µà¤¾à¤°à¤¾", "à¤—à¥à¤°à¥ à¤¨à¤¾à¤¨à¤•", "à¤®à¤¸à¥à¤œà¤¿à¤¦", "à¤§à¤¾à¤°à¥à¤®à¤¿à¤•", "à¤¸à¤¾à¤‚à¤¸à¥à¤•à¥ƒà¤¤à¤¿à¤• à¤•à¤¾à¤°à¥à¤¯à¤•à¥à¤°à¤®", "à¤œà¤¯à¤‚à¤¤à¥€"], "à¤§à¤¾à¤°à¥à¤®à¤¿à¤• / à¤¸à¤¾à¤‚à¤¸à¥à¤•à¥ƒà¤¤à¤¿à¤• à¤•à¤¾à¤°à¥à¤¯à¤•à¥à¤°à¤®"),
    # à¤¸à¤®à¥à¤®à¤¾à¤¨ / Felicitation
    (["à¤¸à¤®à¥à¤®à¤¾à¤¨", "à¤¸à¤®à¥à¤®à¤¾à¤¨à¤¿à¤¤", "à¤¶à¥‰à¤²", "à¤¶à¥à¤°à¥€à¤«à¤²", "à¤¸à¤®à¤¾à¤°à¥‹à¤¹", "felicitation"], "à¤¸à¤®à¥à¤®à¤¾à¤¨ / Felicitation"),
    # à¤ªà¥à¤°à¥‡à¤¸ / à¤®à¥€à¤¡à¤¿à¤¯à¤¾
    (["à¤ªà¥à¤°à¥‡à¤¸ à¤µà¤¾à¤°à¥à¤¤à¤¾", "à¤ªà¥à¤°à¥‡à¤¸ à¤•à¥‰à¤¨à¥à¤«à¤¼à¥à¤°à¥‡à¤‚à¤¸", "à¤®à¥€à¤¡à¤¿à¤¯à¤¾ à¤¬à¥à¤°à¤¿à¤«à¤¿à¤‚à¤—", "à¤®à¥€à¤¡à¤¿à¤¯à¤¾ à¤¸à¥‡ à¤¬à¤¾à¤¤à¤šà¥€à¤¤"], "à¤ªà¥à¤°à¥‡à¤¸ à¤•à¥‰à¤¨à¥à¤«à¤¼à¥à¤°à¥‡à¤‚à¤¸ / à¤®à¥€à¤¡à¤¿à¤¯à¤¾"),
    # à¤¶à¥à¤­à¤•à¤¾à¤®à¤¨à¤¾ / à¤¬à¤§à¤¾à¤ˆ
    (["à¤¶à¥à¤­à¤•à¤¾à¤®à¤¨à¤¾à¤à¤‚", "à¤¶à¥à¤­à¤•à¤¾à¤®à¤¨à¤¾à¤à¤", "à¤¬à¤§à¤¾à¤ˆ", "congratulations"], "à¤¶à¥à¤­à¤•à¤¾à¤®à¤¨à¤¾ / à¤¬à¤§à¤¾à¤ˆ"),
    # à¤œà¤¨à¥à¤®à¤¦à¤¿à¤¨ à¤¶à¥à¤­à¤•à¤¾à¤®à¤¨à¤¾
    (["à¤œà¤¨à¥à¤®à¤¦à¤¿à¤¨", "birthday"], "à¤œà¤¨à¥à¤®à¤¦à¤¿à¤¨ à¤¶à¥à¤­à¤•à¤¾à¤®à¤¨à¤¾"),
    # à¤¶à¥‹à¤• à¤¸à¤‚à¤¦à¥‡à¤¶
    (["à¤¶à¥à¤°à¤¦à¥à¤§à¤¾à¤‚à¤œà¤²à¤¿", "à¤¶à¥‹à¤• à¤¸à¤‚à¤¦à¥‡à¤¶", "à¤¦à¤¿à¤µà¤‚à¤—à¤¤", "à¤…à¤‚à¤¤à¤¿à¤® à¤¯à¤¾à¤¤à¥à¤°à¤¾", "à¤ªà¥à¤£à¥à¤¯à¤¤à¤¿à¤¥à¤¿", "condolence", "à¤¹à¤¾à¤¦à¤¸à¥‡", "à¤®à¥ƒà¤¤à¥à¤¯à¥", "à¤¨à¤¿à¤§à¤¨", "à¤¬à¤²à¤¿à¤¦à¤¾à¤¨", "à¤¶à¤¹à¥€à¤¦"], "à¤¶à¥‹à¤• à¤¸à¤‚à¤¦à¥‡à¤¶"),
]

SCHEME_PATTERNS = {
    r"\bPMAY\b": "à¤ªà¥à¤°à¤§à¤¾à¤¨à¤®à¤‚à¤¤à¥à¤°à¥€ à¤†à¤µà¤¾à¤¸ à¤¯à¥‹à¤œà¤¨à¤¾",
    r"à¤ªà¥à¤°à¤§à¤¾à¤¨à¤®à¤‚à¤¤à¥à¤°à¥€ à¤†à¤µà¤¾à¤¸ à¤¯à¥‹à¤œà¤¨à¤¾": "à¤ªà¥à¤°à¤§à¤¾à¤¨à¤®à¤‚à¤¤à¥à¤°à¥€ à¤†à¤µà¤¾à¤¸ à¤¯à¥‹à¤œà¤¨à¤¾",
    r"à¤ªà¥à¤°à¤§à¤¾à¤¨ à¤®à¤‚à¤¤à¥à¤°à¥€ à¤†à¤µà¤¾à¤¸ à¤¯à¥‹à¤œà¤¨à¤¾": "à¤ªà¥à¤°à¤§à¤¾à¤¨à¤®à¤‚à¤¤à¥à¤°à¥€ à¤†à¤µà¤¾à¤¸ à¤¯à¥‹à¤œà¤¨à¤¾",
    r"PM Awas": "à¤ªà¥à¤°à¤§à¤¾à¤¨à¤®à¤‚à¤¤à¥à¤°à¥€ à¤†à¤µà¤¾à¤¸ à¤¯à¥‹à¤œà¤¨à¤¾",
    r"à¤†à¤µà¤¾à¤¸ à¤¯à¥‹à¤œà¤¨à¤¾": "à¤ªà¥à¤°à¤§à¤¾à¤¨à¤®à¤‚à¤¤à¥à¤°à¥€ à¤†à¤µà¤¾à¤¸ à¤¯à¥‹à¤œà¤¨à¤¾",

    r"à¤†à¤¯à¥à¤·à¥à¤®à¤¾à¤¨ à¤­à¤¾à¤°à¤¤": "à¤†à¤¯à¥à¤·à¥à¤®à¤¾à¤¨ à¤­à¤¾à¤°à¤¤",
    r"\bAyushman\b": "à¤†à¤¯à¥à¤·à¥à¤®à¤¾à¤¨ à¤­à¤¾à¤°à¤¤",
    r"Ayushman Bharat": "à¤†à¤¯à¥à¤·à¥à¤®à¤¾à¤¨ à¤­à¤¾à¤°à¤¤",

    r"à¤‰à¤œà¥à¤œà¥à¤µà¤²à¤¾ à¤¯à¥‹à¤œà¤¨à¤¾": "à¤ªà¥à¤°à¤§à¤¾à¤¨à¤®à¤‚à¤¤à¥à¤°à¥€ à¤‰à¤œà¥à¤œà¥à¤µà¤²à¤¾ à¤¯à¥‹à¤œà¤¨à¤¾",
    r"\bUjjwala\b": "à¤ªà¥à¤°à¤§à¤¾à¤¨à¤®à¤‚à¤¤à¥à¤°à¥€ à¤‰à¤œà¥à¤œà¤²à¤¾ à¤¯à¥‹à¤œà¤¨à¤¾",

    r"à¤¸à¥à¤µà¤šà¥à¤› à¤­à¤¾à¤°à¤¤": "à¤¸à¥à¤µà¤šà¥à¤› à¤­à¤¾à¤°à¤¤ à¤®à¤¿à¤¶à¤¨",
    r"\bSBM\b": "à¤¸à¥à¤µà¤šà¥à¤› à¤­à¤¾à¤°à¤¤ à¤®à¤¿à¤¶à¤¨",

    r"à¤œà¤¨ à¤§à¤¨": "à¤ªà¥à¤°à¤§à¤¾à¤¨à¤®à¤‚à¤¤à¥à¤°à¥€ à¤œà¤¨ à¤§à¤¨ à¤¯à¥‹à¤œà¤¨à¤¾",
    r"\bJan Dhan\b": "à¤ªà¥à¤°à¤§à¤¾à¤¨à¤®à¤‚à¤¤à¥à¤°à¥€ à¤œà¤¨ à¤§à¤¨ à¤¯à¥‹à¤œà¤¨à¤¾",

    r"\bGST\b": "GST",
}

TARGET_GROUP_KEYWORDS = {
    "à¤®à¤¹à¤¿à¤²à¤¾": "à¤®à¤¹à¤¿à¤²à¤¾",
    "à¤®à¤¹à¤¿à¤²à¤¾à¤“à¤‚": "à¤®à¤¹à¤¿à¤²à¤¾",
    "à¤¨à¤¾à¤°à¥€": "à¤®à¤¹à¤¿à¤²à¤¾",
    "à¤¯à¥à¤µà¤¾": "à¤¯à¥à¤µà¤¾",
    "à¤¯à¥à¤µà¤¾à¤“à¤‚": "à¤¯à¥à¤µà¤¾",
    "à¤•à¤¿à¤¸à¤¾à¤¨": "à¤•à¤¿à¤¸à¤¾à¤¨",
    "à¤•à¤¿à¤¸à¤¾à¤¨à¥‹à¤‚": "à¤•à¤¿à¤¸à¤¾à¤¨",
    "à¤–à¥‡à¤¤à¥€": "à¤•à¤¿à¤¸à¤¾à¤¨",
    "à¤›à¤¾à¤¤à¥à¤°": "à¤›à¤¾à¤¤à¥à¤°",
    "à¤µà¤¿à¤¦à¥à¤¯à¤¾à¤°à¥à¤¥à¥€": "à¤›à¤¾à¤¤à¥à¤°",
    "à¤¸à¥à¤Ÿà¥‚à¤¡à¥‡à¤‚à¤Ÿ": "à¤›à¤¾à¤¤à¥à¤°",
    "à¤®à¤œà¤¦à¥‚à¤°": "à¤®à¤œà¤¼à¤¦à¥‚à¤°",
    "à¤®à¤œà¤¦à¥‚à¤°à¥‹à¤‚": "à¤®à¤œà¤¼à¤¦à¥‚à¤°",
    "à¤µà¥à¤¯à¤¾à¤ªà¤¾à¤°à¥€": "à¤µà¥à¤¯à¤¾à¤ªà¤¾à¤°à¥€",
    "à¤µà¥à¤¯à¤¾à¤ªà¤¾à¤°à¤¿à¤¯à¥‹à¤‚": "à¤µà¥à¤¯à¤¾à¤ªà¤¾à¤°à¥€",
    "à¤—à¤°à¥€à¤¬": "à¤—à¤°à¥€à¤¬",
    "à¤†à¤°à¥à¤¥à¤¿à¤• à¤°à¥‚à¤ª à¤¸à¥‡ à¤•à¤®à¤œà¥‹à¤°": "à¤—à¤°à¥€à¤¬",
    "à¤¬à¥à¤œà¥à¤°à¥à¤—": "à¤¬à¥à¤œà¤¼à¥à¤°à¥à¤—",
    "à¤µà¤°à¤¿à¤·à¥à¤  à¤¨à¤¾à¤—à¤°à¤¿à¤•": "à¤¬à¥à¤œà¤¼à¥à¤°à¥à¤—",
    "à¤¸à¤°à¤•à¤¾à¤°à¥€ à¤•à¤°à¥à¤®à¤šà¤¾à¤°à¥€": "à¤¸à¤°à¤•à¤¾à¤°à¥€ à¤•à¤°à¥à¤®à¤šà¤¾à¤°à¥€",
    "à¤¶à¤¾à¤¸à¤•à¥€à¤¯ à¤•à¤°à¥à¤®à¤šà¤¾à¤°à¥€": "à¤¸à¤°à¤•à¤¾à¤°à¥€ à¤•à¤°à¥à¤®à¤šà¤¾à¤°à¥€",
}

COMMUNITY_KEYWORDS = {
    "à¤¸à¤¾à¤¹à¥‚": "à¤¸à¤¾à¤¹à¥‚",
    "à¤—à¥‹à¤‚à¤¡": "à¤—à¥‹à¤‚à¤¡",
    "à¤†à¤¦à¤¿à¤µà¤¾à¤¸à¥€": "à¤†à¤¦à¤¿à¤µà¤¾à¤¸à¥€",
    "à¤—à¥‹à¤‚à¤¡à¤µà¤¾à¤¨à¤¾": "à¤—à¥‹à¤‚à¤¡",
    "à¤µà¥ˆà¤¶à¥à¤¯": "à¤µà¥ˆà¤¶à¥à¤¯",
    "à¤¬à¥à¤°à¤¾à¤¹à¥à¤®à¤£": "à¤¬à¥à¤°à¤¾à¤¹à¥à¤®à¤£",
    "à¤•à¥à¤°à¥à¤®à¥€": "à¤•à¥à¤°à¥à¤®à¥€",
    "à¤¤à¥‡à¤²à¥€": "à¤¤à¥‡à¤²à¥€",
    "à¤ à¤¾à¤•à¥à¤°": "à¤ à¤¾à¤•à¥à¤°",
    "à¤•à¥à¤¶à¤µà¤¾à¤¹à¤¾": "à¤•à¥à¤¶à¤µà¤¾à¤¹à¤¾",
    "à¤¦à¤²à¤¿à¤¤": "à¤¦à¤²à¤¿à¤¤",
    "à¤…à¤¨à¥à¤¸à¥‚à¤šà¤¿à¤¤ à¤œà¤¾à¤¤à¤¿": "à¤…à¤¨à¥à¤¸à¥‚à¤šà¤¿à¤¤ à¤œà¤¾à¤¤à¤¿",
    "à¤…à¤¨à¥à¤¸à¥‚à¤šà¤¿à¤¤ à¤œà¤¨à¤œà¤¾à¤¤à¤¿": "à¤…à¤¨à¥à¤¸à¥‚à¤šà¤¿à¤¤ à¤œà¤¨à¤œà¤¾à¤¤à¤¿",
    "à¤“à¤¬à¥€à¤¸à¥€": "à¤“à¤¬à¥€à¤¸à¥€",
    "à¤®à¥à¤¸à¥à¤²à¤¿à¤®": "à¤®à¥à¤¸à¥à¤²à¤¿à¤®",
    "à¤‡à¤¸à¥à¤²à¤¾à¤®": "à¤®à¥à¤¸à¥à¤²à¤¿à¤®",
    "à¤ˆà¤¸à¤¾à¤ˆ": "à¤ˆà¤¸à¤¾à¤ˆ",
    "à¤•à¥à¤°à¤¿à¤¶à¥à¤šà¤¿à¤¯à¤¨": "à¤ˆà¤¸à¤¾à¤ˆ",
    "à¤¸à¤¿à¤–": "à¤¸à¤¿à¤–",
    "à¤œà¥ˆà¤¨": "à¤œà¥ˆà¤¨",
    "à¤¬à¥Œà¤¦à¥à¤§": "à¤¬à¥Œà¤¦à¥à¤§",
}

ORG_KEYWORDS = {
    "à¤­à¤¾à¤œà¤ªà¤¾": "à¤­à¤¾à¤°à¤¤à¥€à¤¯ à¤œà¤¨à¤¤à¤¾ à¤ªà¤¾à¤°à¥à¤Ÿà¥€",
    "BJP": "à¤­à¤¾à¤°à¤¤à¥€à¤¯ à¤œà¤¨à¤¤à¤¾ à¤ªà¤¾à¤°à¥à¤Ÿà¥€",
    "à¤­à¤¾à¤°à¤¤à¥€à¤¯ à¤œà¤¨à¤¤à¤¾ à¤ªà¤¾à¤°à¥à¤Ÿà¥€": "à¤­à¤¾à¤°à¤¤à¥€à¤¯ à¤œà¤¨à¤¤à¤¾ à¤ªà¤¾à¤°à¥à¤Ÿà¥€",
    "à¤•à¤¾à¤‚à¤—à¥à¤°à¥‡à¤¸": "à¤­à¤¾à¤°à¤¤à¥€à¤¯ à¤°à¤¾à¤·à¥à¤Ÿà¥à¤°à¥€à¤¯ à¤•à¤¾à¤‚à¤—à¥à¤°à¥‡à¤¸",
    "INC": " à¤­à¤¾à¤°à¤¤à¥€à¤¯ à¤°à¤¾à¤·à¥à¤Ÿà¥à¤°à¥€à¤¯ à¤•à¤¾à¤‚à¤—à¥à¤°à¥‡à¤¸",
    "Indian National Congress": "à¤­à¤¾à¤°à¤¤à¥€à¤¯ à¤°à¤¾à¤·à¥à¤Ÿà¥à¤°à¥€à¤¯ à¤•à¤¾à¤‚à¤—à¥à¤°à¥‡à¤¸",
    "RSS": "à¤°à¤¾à¤·à¥à¤Ÿà¥à¤°à¥€à¤¯ à¤¸à¥à¤µà¤¯à¤‚à¤¸à¥‡à¤µà¤• à¤¸à¤‚à¤˜",
    "à¤†à¤°à¤à¤¸à¤à¤¸": "à¤°à¤¾à¤·à¥à¤Ÿà¥à¤°à¥€à¤¯ à¤¸à¥à¤µà¤¯à¤‚à¤¸à¥‡à¤µà¤• à¤¸à¤‚à¤˜",
    "à¤¸à¤°à¤•à¤¾à¤°": "à¤¸à¤°à¤•à¤¾à¤°",
    "à¤•à¥‡à¤‚à¤¦à¥à¤° à¤¸à¤°à¤•à¤¾à¤°": "à¤•à¥‡à¤‚à¤¦à¥à¤° à¤¸à¤°à¤•à¤¾à¤°",
    "à¤°à¤¾à¤œà¥à¤¯ à¤¸à¤°à¤•à¤¾à¤°": "à¤°à¤¾à¤œà¥à¤¯ à¤¸à¤°à¤•à¤¾à¤°",
    "à¤­à¤¾à¤°à¤¤à¥€à¤¯ à¤¸à¥‡à¤¨à¤¾": "à¤­à¤¾à¤°à¤¤à¥€à¤¯ à¤¸à¥‡à¤¨à¤¾",
    "Indian Army": "à¤­à¤¾à¤°à¤¤à¥€à¤¯ à¤¸à¥‡à¤¨à¤¾",
}

# --- Global Geo Data (Comprehensive) ---
GLOBAL_GEO_HIERARCHY_V5 = {}
GLOBAL_LOCATION_LOOKUP_V5 = {} # Stores all levels of hierarchy
GLOBAL_ALIAS_TO_CANONICAL_V5 = {} # Maps all aliases to canonical Hindi name and type

# Helper functions for text normalization
NUKTA_MAP = str.maketrans({
  'à¥˜':'à¤•','à¥™':'à¤–','à¥š':'à¤—','à¥›':'à¤œ','à¥ž':'à¤«','à¥œ':'à¤¡','à¥':'à¤¢','à¤±':'à¤°','à¥Ÿ':'à¤¯'
})

COMBINING = re.compile(r"[\u093C\u094D\u200C\u200D\uFE00-\uFE0F]")
MATRA_MAP = {
  'à¤¾': 'aa', 'à¤¿': 'i', 'à¥€': 'ii', 'à¥': 'u', 'à¥‚': 'uu',
  'à¥‡': 'e', 'à¥ˆ': 'ai', 'à¥‹': 'o', 'à¥Œ': 'au', 'à¥ƒ': 'ri',
  'à¥‰': 'o', 'à¥…': 'ae'
}


def fold_nukta(s: str) -> str:
  return COMBINING.sub('', s.translate(NUKTA_MAP))


def translit_basic(dev: str) -> str:
  # Minimal conservative transliteration for bootstrap; improved later
  m = {
    'à¤…':'a','à¤†':'aa','à¤‡':'i','à¤ˆ':'ii','à¤‰':'u','à¤Š':'uu','à¤':'e','à¤':'ai','à¤“':'o','à¤”':'au',
    'à¤•':'k','à¤–':'kh','à¤—':'g','gh','à¤š':'ch','à¤›':'chh','à¤œ':'j','à¤':'jh','à¤Ÿ':'t','à¤ ':'th','à¤¡':'d','à¤¢':'dh','à¤£':'n',
    'à¤¤':'t','à¤¥':'th','à¤¦':'d','à¤§':'dh','à¤¨':'n','à¤ª':'p','à¤«':'ph','à¤¬':'b','à¤­':'bh','à¤®':'m','à¤¯':'y','à¤°':'r','à¤²':'l','à¤µ':'v','à¤¶':'sh','à¤·':'sh','à¤¸':'s','à¤¹':'h'
  }
  out = []
  for ch in dev:
    if ch in MATRA_MAP:
      out.append(MATRA_MAP[ch])
    else:
      out.append(m.get(ch, ch))
  return ''.join(out)

# Helper functions for location matching (adapted from location_matcher.py)
def _generate_variants(name: str) -> List[str]:
    """
    Generate all possible variants of a location name (adapted from location_matcher.py).
    """
    if not name or not name.strip():
        return []
    
    variants = set()
    
    # Original
    variants.add(name.lower().strip())
    
    # Nukta-folded
    variants.add(fold_nukta(name.lower().strip()))
    
    # Transliterated
    variants.add(translit_basic(name.lower().strip()))
    
    # Remove empty strings
    variants.discard('')
    
    return list(variants)

def build_location_lookup_tables(geo_data: dict) -> Tuple[Dict[str, dict], Dict[str, Tuple[str, str, str]]]:
    """
    Builds a comprehensive lookup for all geographical entities and their aliases.
    Returns (GLOBAL_LOCATION_LOOKUP, GLOBAL_ALIAS_TO_CANONICAL).
    Each entry in GLOBAL_LOCATION_LOOKUP will also store the full hierarchy details.
    Each entry in GLOBAL_ALIAS_TO_CANONICAL maps alias -> (canonical_hindi_name, type, canonical_key).
    """
    lookup = {}
    alias_to_canonical = {}
    
    state_name_hindi = geo_data.get('state', '')
    state_code = geo_data.get('state_code', '')

    # Helper to add to lookup and alias_to_canonical
    def add_location_to_lookups(lookup_dict, alias_dict, name_hindi, name_english, type_str, hierarchy_list, canonical_key, original_data={}):
        full_aliases = set()
        if name_hindi:
            full_aliases.update(_generate_variants(name_hindi))
        if name_english: # Add english name as an alias too
            full_aliases.add(name_english.lower().strip())
        
        record = {
            'type': type_str,
            'name_hindi': name_hindi,
            'name_english': name_english,
            'hierarchy_list': hierarchy_list,
            'canonical_key': canonical_key,
            'aliases': list(full_aliases),
            'original_data': original_data
        }
        
        lookup_dict[name_hindi] = record

        for alias in full_aliases:
            alias_dict[alias] = (name_hindi, type_str, canonical_key)
    
    # Add state itself
    canonical_key_state = f"{state_code}"
    add_location_to_lookups(lookup, alias_to_canonical, state_name_hindi, "Chhattisgarh", "state", 
                            [state_name_hindi], canonical_key_state, {})

    for district in geo_data.get('districts', []):
        district_name_hindi = district.get('name', '')
        district_name_english = "" # We don't have this in chhattisgarh_complete_geography.json
        canonical_key_district = f"{state_code}_{district_name_hindi.replace(' ','_')}"
        
        add_location_to_lookups(lookup, alias_to_canonical, district_name_hindi, district_name_english, "district", 
                                [state_name_hindi, f"{district_name_hindi} à¤œà¤¿à¤²à¤¾"], canonical_key_district, district)
        
        for ac in district.get('acs', []):
            ac_name_hindi = ac.get('name', '')
            ac_name_english = "" # Not available
            canonical_key_ac = f"{state_code}_{district_name_hindi.replace(' ','_')}_{ac_name_hindi.replace(' ','_')}"

            add_location_to_lookups(lookup, alias_to_canonical, ac_name_hindi, ac_name_english, "assembly", 
                                    [state_name_hindi, f"{district_name_hindi} à¤œà¤¿à¤²à¤¾", f"{ac_name_hindi} à¤µà¤¿à¤§à¤¾à¤¨à¤¸à¤­à¤¾"], canonical_key_ac, ac)
            
            for block in ac.get('blocks', []):
                block_name_hindi = block.get('name', '')
                block_name_english = "" # Not available
                canonical_key_block = f"{state_code}_{district_name_hindi.replace(' ','_')}_{ac_name_hindi.replace(' ','_')}_{block_name_hindi.replace(' ','_')}"

                add_location_to_lookups(lookup, alias_to_canonical, block_name_hindi, block_name_english, "block", 
                                        [state_name_hindi, f"{district_name_hindi} à¤œà¤¿à¤²à¤¾", f"{ac_name_hindi} à¤µà¤¿à¤§à¤¾à¤¨à¤¸à¤­à¤¾", f"{block_name_hindi} à¤µà¤¿à¤•à¤¾à¤¸à¤–à¤‚à¤¡"], canonical_key_block, block)
                
                for gp in block.get('gps', []):
                    gp_name_hindi = gp.get('name', '')
                    gp_name_english = "" # Not available
                    canonical_key_gp = f"{state_code}_{district_name_hindi.replace(' ','_')}_{ac_name_hindi.replace(' ','_')}_{block_name_hindi.replace(' ','_')}_{gp_name_hindi.replace(' ','_')}"

                    add_location_to_lookups(lookup, alias_to_canonical, gp_name_hindi, gp_name_english, "gp", 
                                            [state_name_hindi, f"{district_name_hindi} à¤œà¤¿à¤²à¤¾", f"{ac_name_hindi} à¤µà¤¿à¤§à¤¾à¤¨à¤¸à¤­à¤¾", f"{block_name_hindi} à¤µà¤¿à¤•à¤¾à¤¸à¤–à¤‚à¤¡", f"{gp_name_hindi} à¤—à¥à¤°à¤¾à¤® à¤ªà¤‚à¤šà¤¾à¤¯à¤¤"], canonical_key_gp, gp)
                    
                    for village in gp.get('villages', []):
                        village_name_hindi = village.get('name', '')
                        village_name_english = "" # Not available
                        canonical_key_village = f"{state_code}_{district_name_hindi.replace(' ','_')}_{ac_name_hindi.replace(' ','_')}_{block_name_hindi.replace(' ','_')}_{gp_name_hindi.replace(' ','_')}_{village_name_hindi.replace(' ','_')}"

                        add_location_to_lookups(lookup, alias_to_canonical, village_name_hindi, village_name_english, "village", 
                                                [state_name_hindi, f"{district_name_hindi} à¤œà¤¿à¤²à¤¾", f"{ac_name_hindi} à¤µà¤¿à¤§à¤¾à¤¨à¤¸à¤­à¤¾", f"{block_name_hindi} à¤µà¤¿à¤•à¤¾à¤¸à¤–à¤‚à¤¡", f"{gp_name_hindi} à¤—à¥à¤°à¤¾à¤® à¤ªà¤‚à¤šà¤¾à¤¯à¤¤", f"{village_name_hindi} à¤—à¤¾à¤à¤µ"], canonical_key_village, village)

        for ulb in district.get('ulbs', []): # Added ULBs under district
            ulb_name_hindi = ulb.get('name', '')
            ulb_name_english = "" # Not available
            canonical_key_ulb = f"{state_code}_{district_name_hindi.replace(' ','_')}_{ulb_name_hindi.replace(' ','_')}_ULB"
            add_location_to_lookups(lookup, alias_to_canonical, ulb_name_hindi, ulb_name_english, "ulb",
                                    [state_name_hindi, f"{district_name_hindi} à¤œà¤¿à¤²à¤¾", f"{ulb_name_hindi} à¤¨à¤—à¤° à¤¨à¤¿à¤—à¤®"], canonical_key_ulb, ulb)
            
    return lookup, alias_to_canonical

def load_geo_data_v5():
    """
    Loads the comprehensive Chhattisgarh geography data and builds lookup tables.
    """
    global GLOBAL_GEO_HIERARCHY_V5, GLOBAL_LOCATION_LOOKUP_V5, GLOBAL_ALIAS_TO_CANONICAL_V5
    geo_file = Path('KnowledgeBank/geo-data/chhattisgarh_complete_geography.json')
    try:
        with open(geo_file, 'r', encoding='utf-8') as f:
            GLOBAL_GEO_HIERARCHY_V5 = json.load(f)
        GLOBAL_LOCATION_LOOKUP_V5, GLOBAL_ALIAS_TO_CANONICAL_V5 = build_location_lookup_tables(GLOBAL_GEO_HIERARCHY_V5)
        print(f"âœ… Loaded comprehensive geo data from {geo_file}")
    except FileNotFoundError:
        print(f"âš ï¸ Geo data file not found: {geo_file}. Location features will be limited.")
        GLOBAL_GEO_HIERARCHY_V5 = {"state": "à¤›à¤¤à¥à¤¤à¥€à¤¸à¤—à¤¢à¤¼", "state_code": "CG", "districts": []}
        GLOBAL_LOCATION_LOOKUP_V5 = {"à¤›à¤¤à¥à¤¤à¥€à¤¸à¤—à¤¢à¤¼": {"canonical": "à¤›à¤¤à¥à¤¤à¥€à¤¸à¤—à¤¢à¤¼", "aliases": ["chhattisgarh", "à¤›à¤¤à¥à¤¤à¥€à¤¸à¤—à¤¢à¤¼"], "hierarchy": ["à¤›à¤¤à¥à¤¤à¥€à¤¸à¤—à¤¢à¤¼"], "canonical_key": "CG"}}
        GLOBAL_ALIAS_TO_CANONICAL_V5 = {}
        for alias in GLOBAL_LOCATION_LOOKUP_V5["à¤›à¤¤à¥à¤¤à¥€à¤¸à¤—à¤¢à¤¼"]["aliases"]:
             GLOBAL_ALIAS_TO_CANONICAL_V5[alias.lower()] = ("à¤›à¤¤à¥à¤¤à¥€à¤¸à¤—à¤¢à¤¼", "state", "CG")
    except json.JSONDecodeError:
        print(f"âš ï¸ Could not decode JSON from {geo_file}. Location features will be limited.")
        GLOBAL_GEO_HIERARCHY_V5 = {"state": "à¤›à¤¤à¥à¤¤à¥€à¤¸à¤—à¤¢à¤¼", "state_code": "CG", "districts": []}
        GLOBAL_LOCATION_LOOKUP_V5 = {"à¤›à¤¤à¥à¤¤à¥€à¤¸à¤—à¤¢à¤¼": {"canonical": "à¤›à¤¤à¥à¤¤à¥€à¤¸à¤—à¤¢à¤¼", "aliases": ["chhattisgarh", "à¤›à¤¤à¥à¤¤à¥€à¤¸à¤—à¤¢à¤¼"], "hierarchy": ["à¤›à¤¤à¥à¤¤à¥€à¤¸à¤—à¤¢à¤¼"], "canonical_key": "CG"}}
        GLOBAL_ALIAS_TO_CANONICAL_V5 = {}
        for alias in GLOBAL_LOCATION_LOOKUP_V5["à¤›à¤¤à¥à¤¤à¥€à¤¸à¤—à¤¢à¤¼"]["aliases"]:
             GLOBAL_ALIAS_TO_CANONICAL_V5[alias.lower()] = ("à¤›à¤¤à¥à¤¤à¥€à¤¸à¤—à¤¢à¤¼", "state", "CG")


# -------------------------
# Feature extractors (schemes, groups, buckets)
# -------------------------

def normalize_event_type_base(raw_event_type_hi: Optional[str], text: str, schemes: List[str]) -> Tuple[str, float]:
    """
    Base event detection (V4-style) â€“ keyword clusters + à¤ªà¥à¤°à¤¾à¤¨à¥‡ label + schemesà¥¤
    """
    text_lower = text.lower()
    candidate: Optional[str] = None
    best_conf = 0.0

    # 1) keyword clusters
    for keywords, label in EVENT_KEYWORD_CLUSTERS:
        for kw in keywords:
            if kw.lower() in text_lower:
                base_conf = 0.8
                if label in ("à¤ªà¥à¤°à¤¶à¤¾à¤¸à¤¨à¤¿à¤• à¤¸à¤®à¥€à¤•à¥à¤·à¤¾ à¤¬à¥ˆà¤ à¤•", "à¤œà¤¨à¤¸à¤®à¥à¤ªà¤°à¥à¤• / à¤œà¤¨à¤¦à¤°à¥à¤¶à¤¨", "à¤šà¥à¤¨à¤¾à¤µ à¤ªà¥à¤°à¤šà¤¾à¤°"):
                    base_conf = 0.87
                if base_conf > best_conf:
                    best_conf = base_conf
                    candidate = label
                break

    # 2) à¤ªà¥à¤°à¤¾à¤¨à¥‡ event_type à¤•à¥‹ consider à¤•à¤°à¥‹
    if raw_event_type_hi and raw_event_type_hi in ALLOWED_EVENT_TYPES_HI and raw_event_type_hi != "à¤…à¤¨à¥à¤¯":
        if candidate is None:
            candidate = raw_event_type_hi
            best_conf = max(best_conf, 0.75)
        elif raw_event_type_hi == candidate:
            best_conf = max(best_conf, 0.93)

    # 3) schemes à¤¹à¥‹à¤‚ à¤”à¤° event à¤…à¤­à¥€ à¤­à¥€ empty/à¤…à¤¨à¥à¤¯ à¤¹à¥‹ â†’ à¤¯à¥‹à¤œà¤¨à¤¾ à¤˜à¥‹à¤·à¤£à¤¾
    if (candidate is None or candidate == "à¤…à¤¨à¥à¤¯") and schemes:
        candidate = "à¤¯à¥‹à¤œà¤¨à¤¾ à¤˜à¥‹à¤·à¤£à¤¾"
        best_conf = max(best_conf, 0.8)

    # 4) fallback
    if candidate is None:
        candidate = "à¤…à¤¨à¥à¤¯"
        best_conf = max(best_conf, 0.45)

    return candidate, best_conf


def extract_schemes(text: str) -> Tuple[List[str], float]:
    schemes = set()
    for pattern, canonical in SCHEME_PATTERNS.items():
        if re.search(pattern, text, flags=re.IGNORECASE):
            schemes.add(canonical)
    if not schemes:
        return [], 0.0
    conf = min(0.96, 0.65 + 0.08 * len(schemes))
    return sorted(schemes), conf


def extract_target_groups(text: str) -> Tuple[List[str], float]:
    groups = set()
    for kw, canonical in TARGET_GROUP_KEYWORDS.items():
        if kw in text:
            groups.add(canonical)
    if not groups:
        return [], 0.0
    conf = min(0.9, 0.65 + 0.05 * len(groups))
    return sorted(groups), conf


def extract_communities(text: str) -> Tuple[List[str], float]:
    communities = set()
    for kw, canonical in COMMUNITY_KEYWORDS.items():
        if kw in text:
            communities.add(canonical)
    if not communities:
        return [], 0.0
    conf = min(0.9, 0.65 + 0.05 * len(communities))
    return sorted(communities), conf


def extract_orgs(text: str) -> Tuple[List[str], float]:
    orgs = set()
    lowered = text.lower()
    for kw, canonical in ORG_KEYWORDS.items():
        if kw.lower() in lowered:
            orgs.add(canonical)
    if not orgs:
        return [], 0.0
    conf = min(0.9, 0.65 + 0.05 * len(orgs))
    return sorted(orgs), conf


def extract_hashtags(text: str) -> List[str]:
    return re.findall(r"#(\w+)", text)


def make_word_buckets(text: str) -> Tuple[List[str], float]:
    buckets: List[str] = []

    # hashtags à¤¸à¥‡ buckets
    for tag in extract_hashtags(text):
        t = tag.lower()
        if "pmawas" in t or "pmay" in t:
            buckets.append("PM à¤†à¤µà¤¾à¤¸ à¤¯à¥‹à¤œà¤¨à¤¾")
        elif "ayushman" in t:
            buckets.append("à¤†à¤¯à¥à¤·à¥à¤®à¤¾à¤¨ à¤­à¤¾à¤°à¤¤")
        elif "gst" in t:
            buckets.append("GST")
        elif "kisan" in t or "farmer" in t:
            buckets.append("à¤•à¥ƒà¤·à¤¿ / à¤•à¤¿à¤¸à¤¾à¤¨")
        elif "youth" in t or "yuva" in t:
            buckets.append("à¤¯à¥à¤µà¤¾")
        elif "mahila" in t or "women" in t:
            buckets.append("à¤®à¤¹à¤¿à¤²à¤¾ à¤¸à¤¶à¤•à¥à¤¤à¤¿à¤•à¤°à¤£")

    # text-based topics
    topic_map = [
        (["à¤•à¤¿à¤¸à¤¾à¤¨", "à¤«à¤¸à¤²", "à¤–à¥‡à¤¤à¥€", "à¤•à¥ƒà¤·à¤¿"], "à¤•à¥ƒà¤·à¤¿ / à¤•à¤¿à¤¸à¤¾à¤¨"),
        (["à¤®à¤¹à¤¿à¤²à¤¾", "à¤®à¤¹à¤¿à¤²à¤¾à¤“à¤‚", "à¤¨à¤¾à¤°à¥€"], "à¤®à¤¹à¤¿à¤²à¤¾ à¤¸à¤¶à¤•à¥à¤¤à¤¿à¤•à¤°à¤£"),
        (["à¤¶à¤¿à¤•à¥à¤·à¤¾", "à¤¸à¥à¤•à¥‚à¤²", "à¤•à¥‰à¤²à¥‡à¤œ", "à¤µà¤¿à¤¦à¥à¤¯à¤¾à¤²à¤¯"], "à¤¶à¤¿à¤•à¥à¤·à¤¾"),
        (["à¤¸à¥à¤µà¤¾à¤¸à¥à¤¥à¥à¤¯", "à¤…à¤¸à¥à¤ªà¤¤à¤¾à¤²", "à¤šà¤¿à¤•à¤¿à¤¤à¥à¤¸à¤¾", "à¤¸à¥à¤µà¤¾à¤¸à¥à¤¥à¥à¤¯ à¤¶à¤¿à¤µà¤¿à¤°"], "à¤¸à¥à¤µà¤¾à¤¸à¥à¤¥à¥à¤¯"),
        (["à¤¬à¤¿à¤œà¤²à¥€", "à¤°à¥‹à¤¶à¤¨à¥€", "à¤µà¤¿à¤¦à¥à¤¯à¥à¤¤"], "à¤¬à¤¿à¤œà¤²à¥€"),
        (["à¤¸à¤¡à¤¼à¤•", "à¤®à¤¾à¤°à¥à¤—", "highway", "à¤ªà¥à¤²"], "à¤¸à¤¡à¤¼à¤• / à¤‡à¤¨à¥à¤«à¥à¤°à¤¾"),
        (["à¤¨à¥Œà¤•à¤°à¥€", "à¤°à¥‹à¤œà¤¼à¤—à¤¾à¤°", "à¤°à¥‹à¤œà¤—à¤¾à¤°"], "à¤°à¥‹à¤œà¤¼à¤—à¤¾à¤°"),
        (["à¤¯à¥à¤µà¤¾", "à¤¯à¥à¤µà¤¾ à¤¸à¤®à¥à¤®à¥‡à¤²à¤¨"], "à¤¯à¥à¤µà¤¾"),
        (["à¤‰à¤¦à¥à¤¯à¤®à¥€", "à¤µà¥à¤¯à¤¾à¤ªà¤¾à¤°", "à¤‰à¤¦à¥à¤¯à¥‹à¤—"], "à¤‰à¤¦à¥à¤¯à¥‹à¤— / à¤µà¥à¤¯à¤¾à¤ªà¤¾à¤°"),
    ]
    lower = text.lower()
    for words, bucket in topic_map:
        if any(w.lower() in lower for w in words):
            buckets.append(bucket)

    buckets = sorted(set(buckets))
    if not buckets:
        return [], 0.0
    conf = min(0.92, 0.55 + 0.05 * len(buckets))
    return buckets, conf

# -------------------------
# Location helpers
# -------------------------

def extract_inline_location_candidates(text: str) -> List[str]:

    """

    "XYZ à¤œà¤¿à¤²à¤¾", "XYZ à¤¬à¥à¤²à¥‰à¤•", "XYZ à¤¨à¤—à¤° à¤¨à¤¿à¤—à¤®", "XYZ à¤—à¥à¤°à¤¾à¤® à¤ªà¤‚à¤šà¤¾à¤¯à¤¤", "XYZ à¤—à¥à¤°à¤¾à¤®/à¤—à¤¾à¤à¤µ" à¤¸à¥‡ raw location phrases à¤¨à¤¿à¤•à¤¾à¤²à¥‹

    """

    candidates: List[str] = []

    patterns = [

        r"([à¤…-à¤¹à¥˜-à¥ŸA-Za-z]+)\s+à¤œà¤¿à¤²à¤¾",

        r"([à¤…-à¤¹à¥˜-à¥ŸA-Za-z]+)\s+à¤µà¤¿à¤§à¤¾à¤¨à¤¸à¤­à¤¾",

        r"([à¤…-à¤¹à¥˜-à¥ŸA-Za-z]+)\s+à¤¬à¥à¤²à¥‰à¤•",

        r"([à¤…-à¤¹à¥˜-à¥ŸA-Za-z]+)\s+à¤¨à¤—à¤° à¤¨à¤¿à¤—à¤®",

        r"([à¤…-à¤¹à¥˜-à¥ŸA-Za-z]+)\s+à¤¨à¤—à¤° à¤ªà¤¾à¤²à¤¿à¤•à¤¾",

        r"([à¤…-à¤¹à¥˜-à¥ŸA-Za-z]+)\s+à¤¨à¤—à¤° à¤ªà¤‚à¤šà¤¾à¤¯à¤¤",

        r"([à¤…-à¤¹à¥˜-à¥ŸA-Za-z]+)\s+à¤—à¥à¤°à¤¾à¤® à¤ªà¤‚à¤šà¤¾à¤¯à¤¤",

        r"([à¤…-à¤¹à¥˜-à¥ŸA-Za-z]+)\s+à¤—à¥à¤°à¤¾à¤®",

        r"([à¤…-à¤¹à¥˜-à¥ŸA-Za-z]+)\s+à¤—à¤¾à¤à¤µ",

    ]

    for pat in patterns:

        for m in re.finditer(pat, text):

            name = m.group(1).strip()

            if len(name) >= 2:

                candidates.append(name)

    return candidates



def normalize_location(text: str, old_location: Optional[Dict[str, Any]]) -> Tuple[Optional[Dict[str, Any]], float]:
    text_lower = text.lower()
    
    # Stores tuples of (canonical_hindi_name, type_str, canonical_key, match_quality)
    found_locations_info = [] 

    # 1. Prioritize explicit inline location candidates
    inline_candidates = extract_inline_location_candidates(text)
    for candidate in inline_candidates:
        # Check canonical_key and canonical_hindi_name against candidates from the inline candidates
        for canonical_name_hindi, record in GLOBAL_LOCATION_LOOKUP_V5.items():
            if record["type"] not in ["district", "assembly", "block", "gp", "village", "ulb"]:
                continue
            
            # Check if the candidate exactly matches the canonical name or one of its aliases
            if candidate.lower() == canonical_name_hindi.lower() or candidate.lower() in [a.lower() for a in record.get('aliases', [])]:
                # Assign a very high match quality for explicit inline mentions
                match_quality = 2.0 
                found_locations_info.append((canonical_name_hindi, record["type"], record["canonical_key"], match_quality))
                
    # 2. Search GLOBAL_ALIAS_TO_CANONICAL_V5 for other matches (now with word boundaries)
    for alias, (canonical_hindi_name, type_str, canonical_key) in GLOBAL_ALIAS_TO_CANONICAL_V5.items():
        if re.search(r"\b" + re.escape(alias) + r"\b", text_lower):
            # Assign match quality based on type for prioritization, lower than inline candidates
            match_quality = 1.0 # Default for direct alias match
            if type_str == "village": match_quality += 0.05
            elif type_str == "gp": match_quality += 0.04
            elif type_str == "block": match_quality += 0.03
            elif type_str == "assembly": match_quality += 0.02
            elif type_str == "district": match_quality += 0.01

            found_locations_info.append((canonical_hindi_name, type_str, canonical_key, match_quality))
    
    # If multiple matches, prioritize the one with highest match_quality (e.g., most specific type or explicit inline)
    if found_locations_info:
        found_locations_info.sort(key=lambda x: x[3], reverse=True)
        best_match_name_hindi, best_match_type, best_match_canonical_key, _ = found_locations_info[0]
        
        loc_record = GLOBAL_LOCATION_LOOKUP_V5.get(best_match_name_hindi)
        if loc_record:
            # Construct loc_obj from the detailed record
            hierarchy_path = loc_record.get('hierarchy_list', [])
            
            district = next((h.replace(" à¤œà¤¿à¤²à¤¾", "") for h in hierarchy_path if "à¤œà¤¿à¤²à¤¾" in h), None)
            assembly = next((h.replace(" à¤µà¤¿à¤§à¤¾à¤¨à¤¸à¤­à¤¾", "") for h in hierarchy_path if "à¤µà¤¿à¤§à¤¾à¤¨à¤¸à¤­à¤¾" in h), None)
            block = next((h.replace(" à¤µà¤¿à¤•à¤¾à¤¸à¤–à¤‚à¤¡", "") for h in hierarchy_path if "à¤µà¤¿à¤•à¤¾à¤¸à¤–à¤‚à¤¡" in h), None)
            gp = next((h.replace(" à¤—à¥à¤°à¤¾à¤® à¤ªà¤‚à¤šà¤¾à¤¯à¤¤", "") for h in hierarchy_path if "à¤—à¥à¤°à¤¾à¤® à¤ªà¤‚à¤šà¤¾à¤¯à¤¤" in h), None)
            village = next((h.replace(" à¤—à¤¾à¤à¤µ", "") for h in hierarchy_path if "à¤—à¤¾à¤à¤µ" in h), None)
            ulb = next((h.replace(" à¤¨à¤—à¤° à¤¨à¤¿à¤—à¤®", "") for h in hierarchy_path if "à¤¨à¤—à¤° à¤¨à¤¿à¤—à¤®" in h), None) # Assuming ULB is identified this way

            loc_obj = {
                "district": district,
                "assembly": assembly,
                "block": block,
                "gp": gp,
                "village": village,
                "ulb": ulb,
                "zone": None, # Not directly available from current hierarchy
                "ward": None, # Not directly available from current hierarchy
                "canonical_key": best_match_canonical_key,
                "canonical": best_match_name_hindi,
                "aliases": loc_record.get('aliases', []),
                "hierarchy_path": hierarchy_path,
                "visit_count": 1, # Placeholder, actual count logic might be needed
                "type": best_match_type # Add the type of location found
            }
            return loc_obj, 0.9 # High confidence for a direct match from geo data

    # Fallback to old_location hints if no new match found
    if old_location and old_location.get("canonical"):
        return old_location, 0.6 # Reduced confidence for old/unverified hint

    return None, 0.0

# -------------------------
# Confidence + Review (base)
# -------------------------

def compute_confidence_base(
    c_event: float,
    c_location: float,
    c_schemes: float,
    c_topics: float,
    c_targets: float,
    c_communities: float,
    c_orgs: float,
    event_type: str,
    location_obj: Optional[Dict[str, Any]],
    schemes: List[str],
    word_buckets: List[str],
    target_groups: List[str],
    communities: List[str],
    organizations: List[str],
) -> float:
    """
    V4-style base confidence (à¤¬à¤¿à¤¨à¤¾ rescue bonus)
    """
    good_event = event_type != "à¤…à¤¨à¥à¤¯"
    good_loc = bool(location_obj and location_obj.get("canonical"))

    base = 0.4
    if good_event:
        base += 0.25
    if good_loc:
        base += 0.2
    if schemes:
        base += 0.05
    if word_buckets:
        base += 0.05
    if target_groups or communities:
        base += 0.05
    if organizations:
        base += 0.05

    avg_signals = (c_event + c_location + c_schemes + c_topics + c_targets + c_communities + c_orgs) / 7.0
    score = max(base, base * 0.7 + avg_signals * 0.3)
    score = min(0.99, max(0.0, score))
    return round(score, 3)


def decide_review_status(conf: float) -> Tuple[str, bool]:
    if conf >= 0.9:
        return "auto_approved", False
    if conf >= 0.75:
        return "pending", False
    return "pending", True

# -------------------------
# â€œà¤…à¤¨à¥à¤¯â€ Rescue â€“ helper detectors
# -------------------------

def _looks_like_sports_tweet(text_l: str) -> bool:
    SPORTS_KW = [
        "à¤®à¥ˆà¤š", "à¤œà¥€à¤¤", "à¤µà¤¿à¤œà¤¯", "à¤Ÿà¥€à¤® à¤‡à¤‚à¤¡à¤¿à¤¯à¤¾",
        "world cup", "à¤µà¤°à¥à¤²à¥à¤¡ à¤•à¤ª", "à¤Ÿà¥€20", "t20",
        "ipl", "à¤µà¤¨à¤¡à¥‡", "odi"
    ]
    EMOJIS = [" ðŸ", "ðŸ†", "ðŸ‡®ðŸ‡³"]
    return any(kw.lower() in text_l for kw in SPORTS_KW) or any(e in text_l for e in EMOJIS)


def _looks_like_policy_statement(text_l: str, pd4: Dict[str, Any]) -> bool:


    POLICY_KW = [


        "à¤¸à¤¬à¤•à¤¾ à¤¸à¤¾à¤¥ à¤¸à¤¬à¤•à¤¾ à¤µà¤¿à¤•à¤¾à¤¸",


        "à¤¸à¤¬à¤•à¤¾ à¤¸à¤¾à¤¥-à¤¸à¤¬à¤•à¤¾ à¤µà¤¿à¤•à¤¾à¤¸", # Added hyphenated version


        "à¤¨à¤¯à¤¾ à¤­à¤¾à¤°à¤¤",


        "à¤µà¤¿à¤•à¤¸à¤¿à¤¤ à¤­à¤¾à¤°à¤¤",


        "à¤ªà¥à¤°à¤§à¤¾à¤¨à¤®à¤‚à¤¤à¥à¤°à¥€", "à¤ªà¥à¤°à¤§à¤¾à¤¨ à¤®à¤‚à¤¤à¥à¤°à¥€",


        "à¤¦à¥‡à¤¶à¤µà¤¾à¤¸à¤¿à¤¯à¥‹à¤‚", "à¤¨à¤¾à¤—à¤°à¤¿à¤•à¥‹à¤‚",


        "à¤¯à¥à¤µà¤¾ à¤¶à¤•à¥à¤¤à¤¿",


        "à¤µà¤¿à¤•à¤¾à¤¸ à¤•à¥‡ à¤ªà¤¥ à¤ªà¤°", # Added new phrase


        "à¤µà¤¿à¤•à¤¾à¤¸ à¤•à¥€ à¤¨à¤ˆ", # Added new phrase


        "à¤†à¤¤à¥à¤®à¤¨à¤¿à¤°à¥à¤­à¤°", # Added new phrase


    ]


    has_policy_kw = any(kw.lower() in text_l for kw in POLICY_KW)


    EVENT_HINTS = ["à¤¬à¥ˆà¤ à¤•", "à¤°à¥ˆà¤²à¥€", "à¤‰à¤¦à¥à¤˜à¤¾à¤Ÿà¤¨", "à¤¨à¤¿à¤°à¥€à¤•à¥à¤·à¤£", "à¤œà¤¨à¤¦à¤°à¥à¤¶à¤¨"]


    has_hard_event = any(kw in text_l for kw in EVENT_HINTS)


    return has_policy_kw and not has_hard_event


def _looks_like_security_context(text_l: str) -> bool:
    SECURITY_KW = ["à¤®à¤¾à¤“à¤µà¤¾à¤¦à¥€", "à¤®à¤¾à¤“à¤µà¤¾à¤¦", "à¤¨à¤•à¥à¤¸à¤²", "à¤¨à¤•à¥à¤¸à¤²à¤µà¤¾à¤¦", "à¤†à¤¤à¤‚à¤•", "à¤†à¤¤à¤‚à¤•à¤µà¤¾à¤¦", "à¤‰à¤—à¥à¤°à¤µà¤¾à¤¦"]
    return any(kw in text_l for kw in SECURITY_KW)


def _looks_like_pure_greetings(text_l: str, pd4: Dict[str, Any]) -> bool:
    GREET_KW = ["à¤¶à¥à¤­à¤•à¤¾à¤®à¤¨", "à¤¬à¤§à¤¾à¤ˆ", "à¤®à¥à¤¬à¤¾à¤°à¤•", "à¤¶à¥à¤­à¥‡à¤šà¥à¤›à¤¾", "best wishes", "congratulations"]
    FESTIVAL_HINTS = ["à¤¦à¥€à¤ªà¤¾à¤µà¤²à¥€", "à¤¹à¥‹à¤²à¥€", "à¤°à¤•à¥à¤·à¤¾ à¤¬à¤‚à¤§à¤¨", "à¤¸à¥à¤µà¤¤à¤‚à¤¤à¥à¤°à¤¤à¤¾ à¤¦à¤¿à¤µà¤¸", "à¤—à¤£à¤¤à¤‚à¤¤à¥à¤° à¤¦à¤¿à¤µà¤¸"]
    has_greet = any(kw.lower() in text_l for kw in GREET_KW)
    has_fest = any(kw.lower() in text_l for kw in FESTIVAL_HINTS)
    EVENT_HINTS = ["à¤¬à¥ˆà¤ à¤•", "à¤°à¥ˆà¤²à¥€", "à¤‰à¤¦à¥à¤˜à¤¾à¤Ÿà¤¨", "à¤¨à¤¿à¤°à¥€à¤•à¥à¤·à¤£", "à¤œà¤¨à¤¦à¤°à¥à¤¶à¤¨"]
    has_hard_event = any(kw in text_l for kw in EVENT_HINTS)
    return (has_greet or has_fest) and not has_hard_event


def _looks_like_digital_only(text_l: str, pd4: Dict[str, Any]) -> bool:
    loc = pd4.get("location") or {}
    has_loc = bool(loc.get("canonical"))
    DIGITAL_KW = ["online", "live", "à¤œà¥à¤¡à¤¼à¥‡à¤‚", "join us live", "link in bio"]
    EVENT_HINTS = ["à¤¬à¥ˆà¤ à¤•", "à¤°à¥ˆà¤²à¥€", "à¤‰à¤¦à¥à¤˜à¤¾à¤Ÿà¤¨", "à¤¨à¤¿à¤°à¥€à¤•à¥à¤·à¤£", "à¤œà¤¨à¤¦à¤°à¥à¤¶à¤¨"]
    has_digital_kw = any(kw.lower() in text_l for kw in DIGITAL_KW)
    has_hard_event = any(kw in text_l for kw in EVENT_HINTS)
    return (not has_loc) and has_digital_kw and not has_hard_event


def _guess_fallback_content_mode(text_l: str, pd4: Dict[str, Any]) -> str:
    EVENT_HINTS = ["à¤¬à¥ˆà¤ à¤•", "à¤°à¥ˆà¤²à¥€", "à¤‰à¤¦à¥à¤˜à¤¾à¤Ÿà¤¨", "à¤¨à¤¿à¤°à¥€à¤•à¥à¤·à¤£", "à¤œà¤¨à¤¦à¤°à¥à¤¶à¤¨"]
    loc = pd4.get("location") or {}
    has_loc = bool(loc.get("canonical"))
    if has_loc and any(kw in text_l for kw in EVENT_HINTS):
        return "à¤®à¥ˆà¤¦à¤¾à¤¨-à¤¸à¥à¤¤à¤° à¤•à¤¾à¤°à¥à¤¯à¤•à¥à¤°à¤®"
    return "à¤¡à¤¿à¤œà¤¿à¤Ÿà¤² / à¤¸à¥‹à¤¶à¤²-à¤®à¥€à¤¡à¤¿à¤¯à¤¾ à¤ªà¥‹à¤¸à¥à¤Ÿ"

# -------------------------
# â€œà¤…à¤¨à¥à¤¯â€ Rescue core
# -------------------------

def rescue_other_events_v5(text: str, base_pd: Dict[str, Any]) -> Dict[str, Any]:
    """
    à¤¸à¤¿à¤°à¥à¤«à¤¼ event_type/content_mode/conf bonus à¤•à¥€ responsibility à¤¯à¤¹à¤¾à¤ à¤¹à¥ˆà¥¤
    à¤¬à¤¾à¤•à¥€ fields (location, buckets, groups...) base_pd à¤¸à¥‡ à¤¹à¥€ à¤†à¤¤à¥‡ à¤¹à¥ˆà¤‚à¥¤
    """
    text_l = text.lower()
    original_event = base_pd.get("event_type")
    pd5_extra: Dict[str, Any] = {
        "event_type": original_event,
        "content_mode": None,
        "is_other_original": (original_event == "à¤…à¤¨à¥à¤¯"),
        "is_rescued_other": False,
        "rescue_tag": None,
        "rescue_confidence_bonus": 0.0,
    }

    # 1) Sports / Match
    if _looks_like_sports_tweet(text_l):
        pd5_extra["content_mode"] = "à¤–à¥‡à¤² / à¤‰à¤ªà¤²à¤¬à¥à¤§à¤¿ à¤ªà¤° à¤ªà¥à¤°à¤¤à¤¿à¤•à¥à¤°à¤¿à¤¯à¤¾"
        if original_event == "à¤…à¤¨à¥à¤¯":
            pd5_extra["event_type"] = "à¤¶à¥à¤­à¤•à¤¾à¤®à¤¨à¤¾ / à¤¬à¤§à¤¾à¤ˆ"
            pd5_extra["is_rescued_other"] = True
            pd5_extra["rescue_tag"] = "sports"
            pd5_extra["rescue_confidence_bonus"] = 0.15
        return pd5_extra

    # 2) Policy / Narrative
    if _looks_like_policy_statement(text_l, base_pd):
        pd5_extra["content_mode"] = "à¤¨à¥€à¤¤à¤¿ / à¤µà¤•à¥à¤¤à¤µà¥à¤¯"
        has_scheme = bool(base_pd.get("schemes_mentioned"))
        if original_event == "à¤…à¤¨à¥à¤¯":
            if has_scheme:
                pd5_extra["event_type"] = "à¤¯à¥‹à¤œà¤¨à¤¾ à¤˜à¥‹à¤·à¤£à¤¾"
                pd5_extra["rescue_tag"] = "policy_scheme"
                pd5_extra["rescue_confidence_bonus"] = 0.12
            else:
                # à¤¯à¤¹à¤¾à¤ taxonomy stable à¤°à¤–à¤¨à¤¾ à¤¹à¥ˆ, à¤‡à¤¸à¤²à¤¿à¤ event_type "à¤…à¤¨à¥à¤¯" à¤°à¤¹à¤¨à¥‡ à¤¦à¥‡ à¤¸à¤•à¤¤à¥‡ à¤¹à¥ˆà¤‚
                pd5_extra["event_type"] = "à¤…à¤¨à¥à¤¯"
                pd5_extra["rescue_tag"] = "policy_statement"
                pd5_extra["rescue_confidence_bonus"] = 0.06
            pd5_extra["is_rescued_other"] = True
        return pd5_extra

    # 3) Security / Naxal / Terror
    if _looks_like_security_context(text_l):
        pd5_extra["content_mode"] = "à¤¨à¥€à¤¤à¤¿ / à¤µà¤•à¥à¤¤à¤µà¥à¤¯"
        if original_event == "à¤…à¤¨à¥à¤¯":
            pd5_extra["event_type"] = "à¤…à¤¨à¥à¤¯"
            pd5_extra["is_rescued_other"] = True
            pd5_extra["rescue_tag"] = "security"
            pd5_extra["rescue_confidence_bonus"] = 0.05
        return pd5_extra

    # 4) Pure greetings / festival
    if _looks_like_pure_greetings(text_l, base_pd):
        pd5_extra["content_mode"] = "à¤¸à¤¾à¤®à¤¾à¤¨à¥à¤¯ à¤¶à¥à¤­à¤•à¤¾à¤®à¤¨à¤¾à¤à¤ / à¤ªà¤°à¥à¤µ"
        if original_event == "à¤…à¤¨à¥à¤¯":
            pd5_extra["event_type"] = "à¤¶à¥à¤­à¤•à¤¾à¤®à¤¨à¤¾ / à¤¬à¤§à¤¾à¤ˆ"
            pd5_extra["is_rescued_other"] = True
            pd5_extra["rescue_tag"] = "greetings"
            pd5_extra["rescue_confidence_bonus"] = 0.10
        return pd5_extra

    # 5) Digital-only social posts
    if _looks_like_digital_only(text_l, base_pd):
        pd5_extra["content_mode"] = "à¤¡à¤¿à¤œà¤¿à¤Ÿà¤² / à¤¸à¥‹à¤¶à¤²-à¤®à¥€à¤¡à¤¿à¤¯à¤¾ à¤ªà¥‹à¤¸à¥à¤Ÿ"
        if original_event == "à¤…à¤¨à¥à¤¯":
            pd5_extra["is_rescued_other"] = True
            pd5_extra["rescue_tag"] = "digital"
            pd5_extra["rescue_confidence_bonus"] = 0.04
        return pd5_extra

    # Fallback â€“ à¤…à¤¨à¥à¤®à¤¾à¤¨à¤¿à¤¤ content_mode
    pd5_extra["content_mode"] = _guess_fallback_content_mode(text_l, base_pd)
    return pd5_extra


def compute_confidence_v5(base_conf: float, pd5_extra: Dict[str, Any]) -> float:
    bonus = pd5_extra.get("rescue_confidence_bonus", 0.0)
    event_type = pd5_extra.get("event_type")
    content_mode = pd5_extra.get("content_mode")

    # à¤…à¤—à¤° event_type à¤…à¤¬ à¤­à¥€ "à¤…à¤¨à¥à¤¯" à¤¹à¥ˆ à¤²à¥‡à¤•à¤¿à¤¨ content_mode à¤¸à¤¾à¤«à¤¼ à¤¹à¥ˆ (à¤¨à¥€à¤¤à¤¿/à¤¡à¤¿à¤œà¤¿à¤Ÿà¤²),
    # à¤¤à¥‹ à¤¹à¤²à¥à¤•à¤¾ normalization à¤¬à¥‹à¤¨à¤¸à¥¤
    if event_type == "à¤…à¤¨à¥à¤¯" and content_mode in ("à¤¨à¥€à¤¤à¤¿ / à¤µà¤•à¥à¤¤à¤µà¥à¤¯", "à¤¡à¤¿à¤œà¤¿à¤Ÿà¤² / à¤¸à¥‹à¤¶à¤²-à¤®à¥€à¤¡à¤¿à¤¯à¤¾ à¤ªà¥‹à¤¸à¥à¤Ÿ"):
        bonus += 0.03

    conf = min(0.99, max(0.0, base_conf + bonus))
    return round(conf, 3)

# -------------------------
# Base parsing (V4 logic) â€“ used inside V5
# -------------------------

def base_parse_v4(text: str, created_at: Optional[str], old_pd: Dict[str, Any]) -> Tuple[Dict[str, Any], Dict[str, Any]]:
    """
    V4-style base parse â€“ event/location/groups/... + base_confidence.
    """
    old_loc = old_pd.get("location") or {}

    schemes, c_schemes = extract_schemes(text)
    word_buckets, c_topics = make_word_buckets(text)
    target_groups, c_targets = extract_target_groups(text)
    communities, c_communities = extract_communities(text)
    organizations, c_orgs = extract_orgs(text)

    old_event_hi = old_pd.get("event_type")
    event_type, c_event = normalize_event_type_base(old_event_hi, text, schemes)
    location_obj, c_location = normalize_location(text, old_loc)

    event_date = created_at[:10] if created_at else None

    people_mentioned = old_pd.get("people_mentioned") or []
    people_canonical = people_mentioned[:]

    validation_errors: List[str] = []
    if not event_type:
        validation_errors.append("à¤ˆà¤µà¥‡à¤‚à¤Ÿ à¤ªà¥à¤°à¤•à¤¾à¤° à¤¨à¤¹à¥€à¤‚ à¤®à¤¿à¤² à¤¸à¤•à¤¾")
    if not location_obj:
        validation_errors.append("à¤¸à¥à¤¥à¤¾à¤¨ à¤¨à¤¹à¥€à¤‚ à¤®à¤¿à¤² à¤¸à¤•à¤¾")

    base_confidence = compute_confidence_base(
        c_event=c_event,
        c_location=c_location,
        c_schemes=c_schemes,
        c_topics=c_topics,
        c_targets=c_targets,
        c_communities=c_communities,
        c_orgs=c_orgs,
        event_type=event_type,
        location_obj=location_obj,
        schemes=schemes,
        word_buckets=word_buckets,
        target_groups=target_groups,
        communities=communities,
        organizations=organizations,
    )

    base_pd = {
        "event_type": event_type,
        "event_type_secondary": [],
        "event_date": event_date,
        "location": location_obj,
        "people_mentioned": people_mentioned,
        "people_canonical": people_canonical,
        "word_buckets": word_buckets,
        "target_groups": target_groups,
        "communities": communities,
        "organizations": organizations,
        "schemes_mentioned": schemes,
        "hierarchy_path": location_obj.get("hierarchy_path") if location_obj else [],
        "visit_count": location_obj.get("visit_count") if location_obj else 0,
        "vector_embedding_id": (
            f"faiss://{location_obj.get('canonical_key')}"
            if location_obj and location_obj.get("canonical_key")
            else None
        ),
        "confidence": base_confidence,
        "review_status": "",   # later set
        "needs_review": True,  # later set
    }

    meta_v4 = {
        "model_used": "rule+dictionary-hindi-v4-base",
        "processing_time_ms": 0,
        "faiss_round_trips": 0,
        "validation_errors": validation_errors,
        "c_event": c_event,
        "c_location": c_location,
        "c_schemes": c_schemes,
        "c_topics": c_topics,
        "c_targets": c_targets,
        "c_communities": c_communities,
        "c_orgs": c_orgs,
    }

    return base_pd, meta_v4

# -------------------------
# Full V5 parsing per tweet
# -------------------------

def parse_tweet_v5(record: Dict[str, Any]) -> Dict[str, Any]:
    tweet_id = record.get("tweet_id")
    created_at = record.get("created_at")
    text = record.get("raw_text") or record.get("text") or ""

    # à¤ªà¥à¤°à¤¾à¤¨à¥‡ parsed data à¤…à¤—à¤° à¤¹à¥‹à¤‚ à¤¤à¥‹ hints à¤•à¥‡ à¤¤à¥Œà¤° à¤ªà¤° à¤²à¥‹
    old_pd = (
        record.get("parsed_data_v4")
        or record.get("parsed_data_v3")
        or record.get("parsed_data_v2")
        or record.get("parsed_data")
        or {}
    )

    # 1) Base V4-style parse
    base_pd, meta_v4 = base_parse_v4(text, created_at, old_pd)

    # 2) Rescue / content_mode layer (focus on "à¤…à¤¨à¥à¤¯")
    pd5_extra = rescue_other_events_v5(text, base_pd)

    # 3) Confidence V5
    base_conf = base_pd.get("confidence", 0.0)
    final_conf = compute_confidence_v5(base_conf, pd5_extra)

    review_status, needs_review = decide_review_status(final_conf)

    # 4) Merge into final parsed_data_v5
    parsed_data_v5 = {
        **base_pd,
        "event_type": pd5_extra["event_type"],
        "confidence": final_conf,
        "review_status": review_status,
        "needs_review": needs_review,
        "content_mode": pd5_extra["content_mode"],
        "is_other_original": pd5_extra["is_other_original"],
        "is_rescued_other": pd5_extra["is_rescued_other"],
        "rescue_tag": pd5_extra["rescue_tag"],
        "rescue_confidence_bonus": pd5_extra["rescue_confidence_bonus"],
    }

    metadata_v5 = {
        "model_used": "rule+dictionary-hindi-v5",
        "processing_time_ms": 0,
        "faiss_round_trips": 0,
        "validation_errors": meta_v4.get("validation_errors", []),
        "base_confidence_v4": base_conf,
        "rescue_info": {
            "is_other_original": pd5_extra["is_other_original"],
            "is_rescued_other": pd5_extra["is_rescued_other"],
            "rescue_tag": pd5_extra["rescue_tag"],
            "rescue_confidence_bonus": pd5_extra["rescue_confidence_bonus"],
        },
    }

    # Output record â€“ à¤ªà¥à¤°à¤¾à¤¨à¥‡ parsed_data_x à¤•à¥‹ preserve à¤•à¤°à¤¤à¥‡ à¤¹à¥à¤
    out: Dict[str, Any] = {
        "tweet_id": tweet_id,
        "created_at": created_at,
        "raw_text": text,
        "parsed_data_v5": parsed_data_v5,
        "metadata_v5": metadata_v5,
    }
    # à¤ªà¥à¤°à¤¾à¤¨à¥‡ parsed_data_x à¤…à¤—à¤° à¤¹à¥‹à¤‚ à¤¤à¥‹ à¤¸à¤¾à¤¥ à¤®à¥‡à¤‚ à¤°à¤– à¤¦à¥‹
    for key in ("parsed_data_v4", "parsed_data_v3", "parsed_data_v2", "parsed_data"):
        if key in record:
            out[key] = record[key]
    for key in ("metadata_v4", "metadata_v3", "metadata_v2"):
        if key in record:
            out[key] = record[key]

    return out

# -------------------------
# File-level driver
# -------------------------

def reparse_file_v5(input_path: Path, output_path: Path) -> None:
    total = 0
    high_conf = mid_conf = low_conf = 0
    event_counter: Counter = Counter()
    loc_cov = scheme_cov = bucket_cov = tg_cov = comm_cov = 0
    other_original = rescued_other = hard_other = 0

    with input_path.open("r", encoding="utf-8") as fin, \
         output_path.open("w", encoding="utf-8") as fout:

        for line in fin:
            line = line.strip()
            if not line:
                continue
            try:
                rec = json.loads(line)
            except Exception:
                continue

            total += 1
            new_rec = parse_tweet_v5(rec)
            pd5 = new_rec["parsed_data_v5"]

            conf = pd5.get("confidence", 0.0)
            if conf >= 0.9:
                high_conf += 1
            elif conf >= 0.7:
                mid_conf += 1
            else:
                low_conf += 1

            et = pd5.get("event_type") or ""
            event_counter[et] += 1

            if pd5.get("location") and pd5["location"].get("canonical"):
                loc_cov += 1
            if pd5.get("schemes_mentioned"):
                scheme_cov += 1
            if pd5.get("word_buckets"):
                bucket_cov += 1
            if pd5.get("target_groups"):
                tg_cov += 1
            if pd5.get("communities"):
                comm_cov += 1

            if pd5.get("is_other_original"):
                other_original += 1
                if pd5.get("is_rescued_other"):
                    rescued_other += 1
                else:
                    hard_other += 1

            fout.write(json.dumps(new_rec, ensure_ascii=False) + "\n")

    # Summary print
    print("âœ… V5 Re-parsing complete (single-pass robust parser)")
    print(f"  à¤•à¥à¤² à¤Ÿà¥à¤µà¥€à¤Ÿ: {total}")
    if total:
        print(f"  High confidence (>= 0.9): {high_conf} ({high_conf*100/total:.2f}%)")
        print(f"  Medium confidence (0.7â€“0.9): {mid_conf} ({mid_conf*100/total:.2f}%)")
        print(f"  Low confidence (< 0.7): {low_conf} ({low_conf*100/total:.2f}%)")
        print()
        print(f"  Location coverage (canonical à¤®à¥Œà¤œà¥‚à¤¦): {loc_cov} ({loc_cov*100/total:.2f}%)")
        print(f"  Schemes detected: {scheme_cov} ({scheme_cov*100/total:.2f}%)")
        print(f"  Word buckets non-empty: {bucket_cov} ({bucket_cov*100/total:.2f}%)")
        print(f"  Target groups non-empty: {tg_cov} ({tg_cov*100/total:.2f}%)")
        print(f"  Communities non-empty: {comm_cov} ({comm_cov*100/total:.2f}%)")
        print()
        print(f"  Original 'à¤…à¤¨à¥à¤¯' tweets: {other_original}")
        print(f"    Rescued Others: {rescued_other}")
        print(f"    Hard Others (no pattern): {hard_other}")
        print()
        print("  Event type distribution (top):")
        for label, count in event_counter.most_common(15):
            if not label:
                continue
            print(f"    {label}: {count} ({count*100/total:.2f}%)")

# -------------------------
# Entrypoint
# -------------------------

def main(argv: List[str]) -> None:
    # Load comprehensive geo data once
    load_geo_data_v5()

    if len(argv) >= 2:
        input_path = Path(argv[1])
    else:
        input_path = DEFAULT_INPUT
    if len(argv) >= 3:
        output_path = Path(argv[2])
    else:
        output_path = DEFAULT_OUTPUT

    if not input_path.exists():
        print(f"âš ï¸ à¤‡à¤¨à¤ªà¥à¤Ÿ à¤«à¤¼à¤¾à¤‡à¤² à¤¨à¤¹à¥€à¤‚ à¤®à¤¿à¤²à¥€: {input_path}")
        sys.exit(1)

    print(f"â–¶ï¸ Input:  {input_path}")
    print(f"â–¶ï¸ Output: {output_path}")
    reparse_file_v5(input_path, output_path)


if __name__ == "__main__":
    main(sys.argv)
