#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Gemini Parser V1 - Hybrid Rule-Based + Semantic + Entity Resolution + Geo-Hierarchy

Consolidated single-file parser for tweet processing.
"""

import json
import re
import time
import hashlib
import argparse
import unittest
from pathlib import Path
from typing import List, Dict, Tuple, Any, Optional, Union, Set
from collections import Counter

# ==========================================
# CONFIGURATION & CONSTANTS
# ==========================================

import sys

# Base paths
# Assuming this script is in scripts/gemini_parser_v1.py, so PROJECT_ROOT is up one level
PROJECT_ROOT = Path(__file__).resolve().parent.parent
sys.path.append(str(PROJECT_ROOT))

DATA_DIR = PROJECT_ROOT / "data"

# Data files
FULL_VILLAGES_PATH = DATA_DIR / "full_villages.json"
CONSTITUENCIES_PATH = DATA_DIR / "constituencies.json"
URBAN_DATA_PATH = DATA_DIR / "datasets" / "chhattisgarh_urban.ndjson"

# Semantic Search Thresholds
SEMANTIC_SIMILARITY_THRESHOLD = 0.75
SEMANTIC_LOCATION_LIMIT = 3

# Dictionary Lookup Thresholds
DICTIONARY_HIGH_CONFIDENCE = 0.88
CONFIDENCE_THRESHOLD_HIGH = 0.88

# Deduplication
DEDUPLICATION_ENABLED = True
FINGERPRINT_TOP_PEOPLE = 3

# Confidence Scoring Weights
CONFIDENCE_WEIGHTS = {
    'base_event': 0.7,
    'location': 0.15,
    'schemes': 0.08,
    'target_groups': 0.06,
    'communities': 0.04,
    'orgs': 0.04,
    'people': 0.03,
}

# Confidence Thresholds
CONFIDENCE_AUTO_APPROVE = 0.90
CONFIDENCE_NEEDS_REVIEW = 0.75

# Text Validation
MIN_SUBSTANTIAL_LENGTH = 20
SHORT_TWEET_PENALTY = 0.85

# High-Precision Event Types
HIGH_PRECISION_EVENTS = [
    "‡§∂‡•ã‡§ï ‡§∏‡§Ç‡§¶‡•á‡§∂",
    "‡§ú‡§®‡•ç‡§Æ‡§¶‡§ø‡§® ‡§∂‡•Å‡§≠‡§ï‡§æ‡§Æ‡§®‡§æ",
    "‡§Ü‡§Ç‡§§‡§∞‡§ø‡§ï ‡§∏‡•Å‡§∞‡§ï‡•ç‡§∑‡§æ / ‡§™‡•Å‡§≤‡§ø‡§∏",
    "‡§ñ‡•á‡§≤ / ‡§ó‡•å‡§∞‡§µ",
    "‡§Ü‡§™‡§¶‡§æ / ‡§¶‡•Å‡§∞‡•ç‡§ò‡§ü‡§®‡§æ"
]

HIGH_PRECISION_CONFIDENCE_FLOOR = 0.92

# Rescue Logic
RESCUE_CONFIDENCE_BONUSES = {
    'sports': 0.18,
    'security': 0.20,
    'admin': 0.15,
    'election': 0.17,
    'infra_dev': 0.16,
    'scheme': 0.15,
    'political': 0.15,
    'cultural': 0.14,
    'greetings': 0.10
}

# Geo-Hierarchy Resolution
GEO_HIERARCHY_ENABLED = True

# Performance
MAX_FAISS_QUERIES_PER_TWEET = 5
PROCESSING_TIMEOUT_MS = 150

# Output
OUTPUT_ENCODING = "utf-8"
OUTPUT_INDENT = None

# ==========================================
# SHARED UTILS
# ==========================================

def load_ndjson(path: Union[str, Path]) -> List[Dict[str, Any]]:
    """Load NDJSON file."""
    data = []
    if not Path(path).exists():
        return []
    with open(path, "r", encoding="utf-8") as f:
        for line in f:
            if line.strip():
                data.append(json.loads(line))
    return data

def load_json(path: Union[str, Path]) -> Any:
    """Load JSON file."""
    if not Path(path).exists():
        return {}
    with open(path, "r", encoding="utf-8") as f:
        return json.load(f)

def clean_text(text: str) -> str:
    """Basic text cleaning."""
    if not text:
        return ""
    return re.sub(r'\s+', ' ', text).strip()

# ==========================================
# TAXONOMIES
# ==========================================

ALLOWED_EVENT_TYPES_HI = [
    "‡§¨‡•à‡§†‡§ï",
    "‡§ú‡§®‡§∏‡§Æ‡•ç‡§™‡§∞‡•ç‡§ï / ‡§ú‡§®‡§¶‡§∞‡•ç‡§∂‡§®",
    "‡§™‡•ç‡§∞‡§∂‡§æ‡§∏‡§®‡§ø‡§ï ‡§∏‡§Æ‡•Ä‡§ï‡•ç‡§∑‡§æ ‡§¨‡•à‡§†‡§ï",
    "‡§®‡§ø‡§∞‡•Ä‡§ï‡•ç‡§∑‡§£",
    "‡§∞‡•à‡§≤‡•Ä",
    "‡§ö‡•Å‡§®‡§æ‡§µ ‡§™‡•ç‡§∞‡§ö‡§æ‡§∞",
    "‡§â‡§¶‡•ç‡§ò‡§æ‡§ü‡§®",
    "‡§Ø‡•ã‡§ú‡§®‡§æ ‡§ò‡•ã‡§∑‡§£‡§æ",
    "‡§ß‡§æ‡§∞‡•ç‡§Æ‡§ø‡§ï / ‡§∏‡§æ‡§Ç‡§∏‡•ç‡§ï‡•É‡§§‡§ø‡§ï ‡§ï‡§æ‡§∞‡•ç‡§Ø‡§ï‡•ç‡§∞‡§Æ",
    "‡§∏‡§Æ‡•ç‡§Æ‡§æ‡§® / Felicitation",
    "‡§™‡•ç‡§∞‡•á‡§∏ ‡§ï‡•â‡§®‡•ç‡§´‡§º‡•ç‡§∞‡•á‡§Ç‡§∏ / ‡§Æ‡•Ä‡§°‡§ø‡§Ø‡§æ",
    "‡§∂‡•Å‡§≠‡§ï‡§æ‡§Æ‡§®‡§æ / ‡§¨‡§ß‡§æ‡§à",
    "‡§ú‡§®‡•ç‡§Æ‡§¶‡§ø‡§® ‡§∂‡•Å‡§≠‡§ï‡§æ‡§Æ‡§®‡§æ",
    "‡§∂‡•ã‡§ï ‡§∏‡§Ç‡§¶‡•á‡§∂",
    "‡§Ü‡§Ç‡§§‡§∞‡§ø‡§ï ‡§∏‡•Å‡§∞‡§ï‡•ç‡§∑‡§æ / ‡§™‡•Å‡§≤‡§ø‡§∏",
    "‡§ñ‡•á‡§≤ / ‡§ó‡•å‡§∞‡§µ",
    "‡§∞‡§æ‡§ú‡§®‡•Ä‡§§‡§ø‡§ï ‡§µ‡§ï‡•ç‡§§‡§µ‡•ç‡§Ø",
    "‡§Ü‡§™‡§¶‡§æ / ‡§¶‡•Å‡§∞‡•ç‡§ò‡§ü‡§®‡§æ",
    "‡§Ö‡§®‡•ç‡§Ø",
]

CONTENT_MODES = [
    "‡§Æ‡•à‡§¶‡§æ‡§®-‡§∏‡•ç‡§§‡§∞ ‡§ï‡§æ‡§∞‡•ç‡§Ø‡§ï‡•ç‡§∞‡§Æ",
    "‡§®‡•Ä‡§§‡§ø / ‡§µ‡§ï‡•ç‡§§‡§µ‡•ç‡§Ø",
    "‡§°‡§ø‡§ú‡§ø‡§ü‡§≤ / ‡§∏‡•ã‡§∂‡§≤-media ‡§™‡•ã‡§∏‡•ç‡§ü",
    "‡§ñ‡•á‡§≤ / ‡§â‡§™‡§≤‡§¨‡•ç‡§ß‡§ø ‡§™‡§∞ ‡§™‡•ç‡§∞‡§§‡§ø‡§ï‡•ç‡§∞‡§ø‡§Ø‡§æ",
    "‡§∏‡§æ‡§Æ‡§æ‡§®‡•ç‡§Ø ‡§∂‡•Å‡§≠‡§ï‡§æ‡§Æ‡§®‡§æ‡§è‡§Å / ‡§™‡§∞‡•ç‡§µ",
]

EVENT_KEYWORD_CLUSTERS: List[Tuple[List[str], str]] = [
    (["‡§Æ‡§æ‡§ì‡§µ‡§æ‡§¶", "‡§®‡§ï‡•ç‡§∏‡§≤", "‡§®‡§ï‡•ç‡§∏‡§≤‡•Ä", "‡§≤‡§æ‡§≤ ‡§Ü‡§§‡§Ç‡§ï", "‡§∏‡•Å‡§∞‡§ï‡•ç‡§∑‡§æ ‡§¨‡§≤", "‡§ú‡§µ‡§æ‡§®‡•ã‡§Ç", "‡§∂‡§π‡•Ä‡§¶",
      "‡§Ü‡§§‡•ç‡§Æ‡§∏‡§Æ‡§∞‡•ç‡§™‡§£", "‡§¨‡§∏‡•ç‡§§‡§∞ ‡§ì‡§≤‡§Ç‡§™‡§ø‡§ï", "‡§ë‡§™‡§∞‡•á‡§∂‡§®", "‡§™‡•Å‡§≤‡§ø‡§∏ ‡§∏‡•ç‡§Æ‡•É‡§§‡§ø", "police", "jawan", "encounter", "ied"], "‡§Ü‡§Ç‡§§‡§∞‡§ø‡§ï ‡§∏‡•Å‡§∞‡§ï‡•ç‡§∑‡§æ / ‡§™‡•Å‡§≤‡§ø‡§∏"),

    (["‡§Æ‡•à‡§ö ‡§ú‡•Ä‡§§", "‡§ü‡•Ä‡§Æ ‡§á‡§Ç‡§°‡§ø‡§Ø‡§æ", "‡§ï‡•ç‡§∞‡§ø‡§ï‡•á‡§ü", "‡§™‡§¶‡§ï", "‡§∏‡•ç‡§µ‡§∞‡•ç‡§£ ‡§™‡§¶‡§ï", "‡§ñ‡§ø‡§≤‡§æ‡§°‡§º‡•Ä",
      "‡§ì‡§≤‡§Ç‡§™‡§ø‡§ï", "‡§ñ‡•á‡§≤", "tournament", "‡§ö‡•à‡§Ç‡§™‡§ø‡§Ø‡§Ç‡§∏ ‡§ü‡•ç‡§∞‡•â‡§´‡•Ä", "‡§ó‡§∞‡•ç‡§µ ‡§ï‡§æ ‡§ï‡•ç‡§∑‡§£", "medal", "won", "winner", "bcci"], "‡§ñ‡•á‡§≤ / ‡§ó‡•å‡§∞‡§µ"),

    (["‡§π‡§æ‡§¶‡§∏‡§æ", "‡§¶‡•Å‡§∞‡•ç‡§ò‡§ü‡§®‡§æ", "‡§∞‡•á‡§≤ ‡§π‡§æ‡§¶‡§∏‡§æ", "‡§¨‡§∏ ‡§π‡§æ‡§¶‡§∏‡§æ", "‡§Ü‡§ó‡§ú‡§®‡•Ä", "‡§ß‡•ç‡§µ‡§∏‡•ç‡§§", "‡§ú‡§®‡§π‡§æ‡§®‡§ø", "tragedy", "accident", "collision"], "‡§Ü‡§™‡§¶‡§æ / ‡§¶‡•Å‡§∞‡•ç‡§ò‡§ü‡§®‡§æ"),

    (["‡§°‡§¨‡§≤ ‡§á‡§Ç‡§ú‡§®", "‡§ï‡§æ‡§Ç‡§ó‡•ç‡§∞‡•á‡§∏ ‡§∏‡§∞‡§ï‡§æ‡§∞", "‡§≠‡•ç‡§∞‡§∑‡•ç‡§ü‡§æ‡§ö‡§æ‡§∞", "‡§§‡•Å‡§∑‡•ç‡§ü‡§ø‡§ï‡§∞‡§£", "‡§Ü‡§™‡§æ‡§§‡§ï‡§æ‡§≤",
      "‡§µ‡§ø‡§ï‡§∏‡§ø‡§§ ‡§≠‡§æ‡§∞‡§§", "‡§Æ‡•ã‡§¶‡•Ä ‡§ï‡•Ä ‡§ó‡§æ‡§∞‡§Ç‡§ü‡•Ä", "‡§µ‡§ø‡§™‡§ï‡•ç‡§∑", "‡§Ü‡§∞‡•ã‡§™", "statement", "political", "manifesto"], "‡§∞‡§æ‡§ú‡§®‡•Ä‡§§‡§ø‡§ï ‡§µ‡§ï‡•ç‡§§‡§µ‡•ç‡§Ø"),

    (["‡§¨‡•à‡§†‡§ï", "‡§Æ‡•Å‡§≤‡§æ‡§ï‡§æ‡§§", "‡§≠‡•á‡§Ç‡§ü", "‡§¨‡•à‡§†‡§ï ‡§≤‡•Ä", "‡§¨‡•à‡§†‡§ï ‡§Æ‡•á‡§Ç", "‡§¨‡•à‡§†‡§ï ‡§ï‡§æ", "‡§Ö‡§ß‡•ç‡§Ø‡§ï‡•ç‡§∑‡§§‡§æ ‡§ï‡•Ä", "‡§∏‡§§‡•ç‡§∞", "‡§∏‡§¶‡§® ‡§ï‡•Ä ‡§ï‡§æ‡§∞‡•ç‡§Ø‡§µ‡§æ‡§π‡•Ä"], "‡§¨‡•à‡§†‡§ï"),
    (["‡§ú‡§®‡§∏‡§Æ‡•ç‡§™‡§∞‡•ç‡§ï", "‡§ú‡§®‡§∏‡§Ç‡§™‡§∞‡•ç‡§ï", "‡§ú‡§® ‡§∏‡§Ç‡§™‡§∞‡•ç‡§ï", "‡§ú‡§®‡§¶‡§∞‡•ç‡§∂‡§®", "‡§ú‡§®-‡§¶‡§∞‡•ç‡§∂‡§®", "‡§ú‡§® ‡§∏‡•Å‡§®‡§µ‡§æ‡§à", "‡§ú‡§®‡§∏‡•Å‡§®‡§µ‡§æ‡§à"], "‡§ú‡§®‡§∏‡§Æ‡•ç‡§™‡§∞‡•ç‡§ï / ‡§ú‡§®‡§¶‡§∞‡•ç‡§∂‡§®"),
    (["‡§∏‡§Æ‡•Ä‡§ï‡•ç‡§∑‡§æ ‡§¨‡•à‡§†‡§ï", "‡§∏‡§Æ‡•Ä‡§ï‡•ç‡§∑‡§æ ‡§ï‡•Ä", "‡§∏‡§Æ‡•Ä‡§ï‡•ç‡§∑‡§æ ‡§ï‡•Ä ‡§ó‡§à", "‡§Ö‡§ß‡§ø‡§ï‡§æ‡§∞‡§ø‡§Ø‡•ã‡§Ç ‡§ï‡•á ‡§∏‡§æ‡§•", "‡§µ‡§ø‡§≠‡§æ‡§ó‡•Ä‡§Ø ‡§¨‡•à‡§†‡§ï", "‡§ï‡§≤‡•á‡§ï‡•ç‡§ü‡§∞", "‡§ï‡§≤‡•á‡§ï‡•ç‡§ü‡§∞‡•á‡§ü", "‡§∏‡§Æ‡•Ä‡§ï‡•ç‡§∑‡§æ ‡§ï‡§æ‡§∞‡•ç‡§Ø"], "‡§™‡•ç‡§∞‡§∂‡§æ‡§∏‡§®‡§ø‡§ï ‡§∏‡§Æ‡•Ä‡§ï‡•ç‡§∑‡§æ ‡§¨‡•à‡§†‡§ï"),
    (["‡§®‡§ø‡§∞‡•Ä‡§ï‡•ç‡§∑‡§£", "inspection", "‡§®‡§ø‡§∞‡•Ä‡§ï‡•ç‡§∑‡§£ ‡§ï‡§ø‡§Ø‡§æ"], "‡§®‡§ø‡§∞‡•Ä‡§ï‡•ç‡§∑‡§£"),
    (["‡§∞‡•à‡§≤‡•Ä", "‡§ú‡§®‡§∏‡§≠‡§æ", "public rally", "road show", "‡§∞‡•ã‡§° ‡§∂‡•ã"], "‡§∞‡•à‡§≤‡•Ä"),
    (["‡§ö‡•Å‡§®‡§æ‡§µ‡•Ä", "‡§Æ‡§§‡§¶‡§æ‡§®", "‡§Æ‡§§‡§¶‡§æ‡§§‡§æ", "‡§ö‡•Å‡§®‡§æ‡§µ ‡§™‡•ç‡§∞‡§ö‡§æ‡§∞", "poll campaign", "voting", "polling"], "‡§ö‡•Å‡§®‡§æ‡§µ ‡§™‡•ç‡§∞‡§ö‡§æ‡§∞"),
    (["‡§â‡§¶‡•ç‡§ò‡§æ‡§ü‡§®", "‡§≤‡•ã‡§ï‡§æ‡§∞‡•ç‡§™‡§£", "inauguration", "inaugurated", "‡§∂‡§ø‡§≤‡§æ‡§®‡•ç‡§Ø‡§æ‡§∏", "dedication"], "‡§â‡§¶‡•ç‡§ò‡§æ‡§ü‡§®"),
    (["‡§ò‡•ã‡§∑‡§£‡§æ", "‡§®‡§à ‡§Ø‡•ã‡§ú‡§®‡§æ", "‡§Ø‡•ã‡§ú‡§®‡§æ ‡§ï‡•Ä ‡§ú‡§æ‡§®‡§ï‡§æ‡§∞‡•Ä", "‡§Ø‡•ã‡§ú‡§®‡§æ ‡§ï‡§æ ‡§≤‡§æ‡§≠", "scheme launch"], "‡§Ø‡•ã‡§ú‡§®‡§æ ‡§ò‡•ã‡§∑‡§£‡§æ"),
    (["‡§Æ‡§Ç‡§¶‡§ø‡§∞", "‡§™‡•Ç‡§ú‡§æ", "‡§Ü‡§∞‡§§‡•Ä", "‡§ó‡•Å‡§∞‡•Å‡§¶‡•ç‡§µ‡§æ‡§∞‡§æ", "‡§ó‡•Å‡§∞‡•Å ‡§®‡§æ‡§®‡§ï", "‡§Æ‡§∏‡•ç‡§ú‡§ø‡§¶", "‡§ß‡§æ‡§∞‡•ç‡§Æ‡§ø‡§ï", "‡§∏‡§æ‡§Ç‡§∏‡•ç‡§ï‡•É‡§§‡§ø‡§ï ‡§ï‡§æ‡§∞‡•ç‡§Ø‡§ï‡•ç‡§∞‡§Æ", "‡§ú‡§Ø‡§Ç‡§§‡•Ä", "pujya", "saints"], "‡§ß‡§æ‡§∞‡•ç‡§Æ‡§ø‡§ï / ‡§∏‡§æ‡§Ç‡§∏‡•ç‡§ï‡•É‡§§‡§ø‡§ï ‡§ï‡§æ‡§∞‡•ç‡§Ø‡§ï‡•ç‡§∞‡§Æ"),
    (["‡§∏‡§Æ‡•ç‡§Æ‡§æ‡§®", "‡§∏‡§Æ‡•ç‡§Æ‡§æ‡§®‡§ø‡§§", "‡§∂‡•â‡§≤", "‡§∂‡•ç‡§∞‡•Ä‡§´‡§≤", "‡§∏‡§Æ‡§æ‡§∞‡•ã‡§π", "felicitation", "award"], "‡§∏‡§Æ‡•ç‡§Æ‡§æ‡§® / Felicitation"),
    (["‡§™‡•ç‡§∞‡•á‡§∏ ‡§µ‡§æ‡§∞‡•ç‡§§‡§æ", "‡§™‡•ç‡§∞‡•á‡§∏ ‡§ï‡•â‡§®‡•ç‡§´‡§º‡•ç‡§∞‡•á‡§Ç‡§∏", "‡§Æ‡•Ä‡§°‡§ø‡§Ø‡§æ ‡§∏‡•á ‡§¨‡§æ‡§§‡§ö‡•Ä‡§§"], "‡§™‡•ç‡§∞‡•á‡§∏ ‡§ï‡•â‡§®‡•ç‡§´‡§º‡•ç‡§∞‡•á‡§Ç‡§∏ / ‡§Æ‡•Ä‡§°‡§ø‡§Ø‡§æ"),
    (["‡§∂‡•Å‡§≠‡§ï‡§æ‡§Æ‡§®‡§æ‡§è‡§Ç", "‡§∂‡•Å‡§≠‡§ï‡§æ‡§Æ‡§®‡§æ‡§è‡§Å", "‡§¨‡§ß‡§æ‡§à", "congratulations", "best wishes", "greetings"], "‡§∂‡•Å‡§≠‡§ï‡§æ‡§Æ‡§®‡§æ / ‡§¨‡§ß‡§æ‡§à"),
    (["‡§ú‡§®‡•ç‡§Æ‡§¶‡§ø‡§®", "birthday", "‡§Ö‡§µ‡§§‡§∞‡§£ ‡§¶‡§ø‡§µ‡§∏"], "‡§ú‡§®‡•ç‡§Æ‡§¶‡§ø‡§® ‡§∂‡•Å‡§≠‡§ï‡§æ‡§Æ‡§®‡§æ"),
    (["‡§∂‡•ç‡§∞‡§¶‡•ç‡§ß‡§æ‡§Ç‡§ú‡§≤‡§ø", "‡§∂‡•ã‡§ï ‡§∏‡§Ç‡§¶‡•á‡§∂", "‡§¶‡§ø‡§µ‡§Ç‡§ó‡§§", "‡§Ö‡§Ç‡§§‡§ø‡§Æ ‡§Ø‡§æ‡§§‡•ç‡§∞‡§æ", "‡§™‡•Å‡§£‡•ç‡§Ø‡§§‡§ø‡§•‡§ø", "condolence", "tribute", "rip"], "‡§∂‡•ã‡§ï ‡§∏‡§Ç‡§¶‡•á‡§∂"),
]

SCHEME_PATTERNS = {
    r"\bPMAY\b": "‡§™‡•ç‡§∞‡§ß‡§æ‡§®‡§Æ‡§Ç‡§§‡•ç‡§∞‡•Ä ‡§Ü‡§µ‡§æ‡§∏ ‡§Ø‡•ã‡§ú‡§®‡§æ", r"‡§™‡•ç‡§∞‡§ß‡§æ‡§®‡§Æ‡§Ç‡§§‡•ç‡§∞‡•Ä ‡§Ü‡§µ‡§æ‡§∏ ‡§Ø‡•ã‡§ú‡§®‡§æ": "‡§™‡•ç‡§∞‡§ß‡§æ‡§®‡§Æ‡§Ç‡§§‡•ç‡§∞‡•Ä ‡§Ü‡§µ‡§æ‡§∏ ‡§Ø‡•ã‡§ú‡§®‡§æ",
    r"PM Awas": "‡§™‡•ç‡§∞‡§ß‡§æ‡§®‡§Æ‡§Ç‡§§‡•ç‡§∞‡•Ä ‡§Ü‡§µ‡§æ‡§∏ ‡§Ø‡•ã‡§ú‡§®‡§æ", r"‡§Ü‡§Ø‡•Å‡§∑‡•ç‡§Æ‡§æ‡§® ‡§≠‡§æ‡§∞‡§§": "‡§Ü‡§Ø‡•Å‡§∑‡•ç‡§Æ‡§æ‡§® ‡§≠‡§æ‡§∞‡§§",
    r"\bAyushman\b": "‡§Ü‡§Ø‡•Å‡§∑‡•ç‡§Æ‡§æ‡§® ‡§≠‡§æ‡§∞‡§§", r"‡§â‡§ú‡•ç‡§ú‡•ç‡§µ‡§≤‡§æ ‡§Ø‡•ã‡§ú‡§®‡§æ": "‡§™‡•ç‡§∞‡§ß‡§æ‡§®‡§Æ‡§Ç‡§§‡•ç‡§∞‡•Ä ‡§â‡§ú‡•ç‡§ú‡•ç‡§µ‡§≤‡§æ ‡§Ø‡•ã‡§ú‡§®‡§æ",
    r"‡§∏‡•ç‡§µ‡§ö‡•ç‡§õ ‡§≠‡§æ‡§∞‡§§": "‡§∏‡•ç‡§µ‡§ö‡•ç‡§õ ‡§≠‡§æ‡§∞‡§§ ‡§Æ‡§ø‡§∂‡§®", r"‡§ú‡§® ‡§ß‡§®": "‡§™‡•ç‡§∞‡§ß‡§æ‡§®‡§Æ‡§Ç‡§§‡•ç‡§∞‡•Ä ‡§ú‡§® ‡§ß‡§® ‡§Ø‡•ã‡§ú‡§®‡§æ",
    r"\bJan Dhan\b": "‡§™‡•ç‡§∞‡§ß‡§æ‡§®‡§Æ‡§Ç‡§§‡•ç‡§∞‡•Ä ‡§ú‡§® ‡§ß‡§® ‡§Ø‡•ã‡§ú‡§®‡§æ", r"\bGST\b": "GST",
}

# ==========================================
# LOCATION DICTIONARIES
# ==========================================

CANONICAL_LOCATIONS: Dict[str, Dict[str, Any]] = {
    "‡§∞‡§æ‡§Ø‡§ó‡§¢‡§º": {"canonical": "‡§∞‡§æ‡§Ø‡§ó‡§¢‡§º", "hierarchy": ["‡§õ‡§§‡•ç‡§§‡•Ä‡§∏‡§ó‡§¢‡§º", "‡§∞‡§æ‡§Ø‡§ó‡§¢‡§º ‡§ú‡§ø‡§≤‡§æ"]},
    "Raigarh": {"canonical": "‡§∞‡§æ‡§Ø‡§ó‡§¢‡§º", "hierarchy": ["‡§õ‡§§‡•ç‡§§‡•Ä‡§∏‡§ó‡§¢‡§º", "‡§∞‡§æ‡§Ø‡§ó‡§¢‡§º ‡§ú‡§ø‡§≤‡§æ"]},
    "‡§ñ‡§∞‡§∏‡§ø‡§Ø‡§æ": {"canonical": "‡§ñ‡§∞‡§∏‡§ø‡§Ø‡§æ", "hierarchy": ["‡§õ‡§§‡•ç‡§§‡•Ä‡§∏‡§ó‡§¢‡§º", "‡§∞‡§æ‡§Ø‡§ó‡§¢‡§º ‡§ú‡§ø‡§≤‡§æ", "‡§ñ‡§∞‡§∏‡§ø‡§Ø‡§æ ‡§µ‡§ø‡§ß‡§æ‡§®‡§∏‡§≠‡§æ"]},
    "Kharsia": {"canonical": "‡§ñ‡§∞‡§∏‡§ø‡§Ø‡§æ", "hierarchy": ["‡§õ‡§§‡•ç‡§§‡•Ä‡§∏‡§ó‡§¢‡§º", "‡§∞‡§æ‡§Ø‡§ó‡§¢‡§º ‡§ú‡§ø‡§≤‡§æ", "‡§ñ‡§∞‡§∏‡§ø‡§Ø‡§æ ‡§µ‡§ø‡§ß‡§æ‡§®‡§∏‡§≠‡§æ"]},
    "‡§∞‡§æ‡§Ø‡§™‡•Å‡§∞": {"canonical": "‡§∞‡§æ‡§Ø‡§™‡•Å‡§∞", "hierarchy": ["‡§õ‡§§‡•ç‡§§‡•Ä‡§∏‡§ó‡§¢‡§º", "‡§∞‡§æ‡§Ø‡§™‡•Å‡§∞ ‡§ú‡§ø‡§≤‡§æ"]},
    "Raipur": {"canonical": "‡§∞‡§æ‡§Ø‡§™‡•Å‡§∞", "hierarchy": ["‡§õ‡§§‡•ç‡§§‡•Ä‡§∏‡§ó‡§¢‡§º", "‡§∞‡§æ‡§Ø‡§™‡•Å‡§∞ ‡§ú‡§ø‡§≤‡§æ"]},
    "‡§®‡§Ø‡§æ ‡§∞‡§æ‡§Ø‡§™‡•Å‡§∞": {"canonical": "‡§®‡§Ø‡§æ ‡§∞‡§æ‡§Ø‡§™‡•Å‡§∞", "hierarchy": ["‡§õ‡§§‡•ç‡§§‡•Ä‡§∏‡§ó‡§¢‡§º", "‡§∞‡§æ‡§Ø‡§™‡•Å‡§∞ ‡§ú‡§ø‡§≤‡§æ"]},
    "‡§¨‡§ø‡§≤‡§æ‡§∏‡§™‡•Å‡§∞": {"canonical": "‡§¨‡§ø‡§≤‡§æ‡§∏‡§™‡•Å‡§∞", "hierarchy": ["‡§õ‡§§‡•ç‡§§‡•Ä‡§∏‡§ó‡§¢‡§º", "‡§¨‡§ø‡§≤‡§æ‡§∏‡§™‡•Å‡§∞ ‡§ú‡§ø‡§≤‡§æ"]},
    "Bilaspur": {"canonical": "‡§¨‡§ø‡§≤‡§æ‡§∏‡§™‡•Å‡§∞", "hierarchy": ["‡§õ‡§§‡•ç‡§§‡•Ä‡§∏‡§ó‡§¢‡§º", "‡§¨‡§ø‡§≤‡§æ‡§∏‡§™‡•Å‡§∞ ‡§ú‡§ø‡§≤‡§æ"]},
    "‡§ï‡•ã‡§∞‡§¨‡§æ": {"canonical": "‡§ï‡•ã‡§∞‡§¨‡§æ", "hierarchy": ["‡§õ‡§§‡•ç‡§§‡•Ä‡§∏‡§ó‡§¢‡§º", "‡§ï‡•ã‡§∞‡§¨‡§æ‡§ú‡§ø‡§≤‡§æ"]},
    "Korba": {"canonical": "‡§ï‡•ã‡§∞‡§¨‡§æ", "hierarchy": ["‡§õ‡§§‡•ç‡§§‡•Ä‡§∏‡§ó‡§¢‡§º", "‡§ï‡•ã‡§∞‡§¨‡§æ‡§ú‡§ø‡§≤‡§æ"]},
    "‡§∞‡§§‡§®‡§™‡•Å‡§∞": {"canonical": "‡§∞‡§§‡§®‡§™‡•Å‡§∞", "hierarchy": ["‡§õ‡§§‡•ç‡§§‡•Ä‡§∏‡§ó‡§¢‡§º", "‡§¨‡§ø‡§≤‡§æ‡§∏‡§™‡•Å‡§∞‡§ú‡§ø‡§≤‡§æ"]},
    "‡§¶‡•Å‡§∞‡•ç‡§ó": {"canonical": "‡§¶‡•Å‡§∞‡•ç‡§ó", "hierarchy": ["‡§õ‡§§‡•ç‡§§‡•Ä‡§∏‡§ó‡§¢‡§º", "‡§¶‡•Å‡§∞‡•ç‡§ó‡§ú‡§ø‡§≤‡§æ"]},
    "Durg": {"canonical": "‡§¶‡•Å‡§∞‡•ç‡§ó", "hierarchy": ["‡§õ‡§§‡•ç‡§§‡•Ä‡§∏‡§ó‡§¢‡§º", "‡§¶‡•Å‡§∞‡•ç‡§ó‡§ú‡§ø‡§≤‡§æ"]},
    "‡§≠‡§ø‡§≤‡§æ‡§à": {"canonical": "‡§≠‡§ø‡§≤‡§æ‡§à", "hierarchy": ["‡§õ‡§§‡•ç‡§§‡•Ä‡§∏‡§ó‡§¢‡§º", "‡§¶‡•Å‡§∞‡•ç‡§ó‡§ú‡§ø‡§≤‡§æ"]},
    "Bhilai": {"canonical": "‡§≠‡§ø‡§≤‡§æ‡§à", "hierarchy": ["‡§õ‡§§‡•ç‡§§‡•Ä‡§∏‡§ó‡§¢‡§º", "‡§¶‡•Å‡§∞‡•ç‡§ó‡§ú‡§ø‡§≤‡§æ"]},
    "‡§Ö‡§Ç‡§¨‡§ø‡§ï‡§æ‡§™‡•Å‡§∞": {"canonical": "‡§Ö‡§Ç‡§¨‡§ø‡§ï‡§æ‡§™‡•Å‡§∞", "hierarchy": ["‡§õ‡§§‡•ç‡§§‡•Ä‡§∏‡§ó‡§¢‡§º", "‡§∏‡•Å‡§∞‡§ú‡§™‡•Å‡§∞/‡§∏‡§∞‡§ó‡•Å‡§ú‡§æ ‡§ï‡•ç‡§∑‡•á‡§§‡•ç‡§∞"]},
    "Ambikapur": {"canonical": "‡§Ö‡§Ç‡§¨‡§ø‡§ï‡§æ‡§™‡•Å‡§∞", "hierarchy": ["‡§õ‡§§‡•ç‡§§‡•Ä‡§∏‡§ó‡§¢‡§º", "‡§∏‡•Å‡§∞‡§ú‡§™‡•Å‡§∞/‡§∏‡§∞‡§ó‡•Å‡§ú‡§æ ‡§ï‡•ç‡§∑‡•á‡§§‡•ç‡§∞"]},
    "‡§∏‡•Å‡§∞‡§ú‡§™‡•Å‡§∞": {"canonical": "‡§∏‡•Å‡§∞‡§ú‡§™‡•Å‡§∞", "hierarchy": ["‡§õ‡§§‡•ç‡§§‡•Ä‡§∏‡§ó‡§¢‡§º", "‡§∏‡•Å‡§∞‡§ú‡§™‡•Å‡§∞‡§ú‡§ø‡§≤‡§æ"]},
    "‡§ú‡§ó‡§¶‡§≤‡§™‡•Å‡§∞": {"canonical": "‡§ú‡§ó‡§¶‡§≤‡§™‡•Å‡§∞", "hierarchy": ["‡§õ‡§§‡•ç‡§§‡•Ä‡§∏‡§ó‡§¢‡§º", "‡§¨‡§∏‡•ç‡§§‡§∞‡§ú‡§ø‡§≤‡§æ"]},
    "Jagdalpur": {"canonical": "‡§ú‡§ó‡§¶‡§≤‡§™‡•Å‡§∞", "hierarchy": ["‡§õ‡§§‡•ç‡§§‡•Ä‡§∏‡§ó‡§¢‡§º", "‡§¨‡§∏‡•ç‡§§‡§∞‡§ú‡§ø‡§≤‡§æ"]},
    "‡§ï‡•ã‡§Ç‡§°‡§æ‡§ó‡§æ‡§Å‡§µ": {"canonical": "‡§ï‡•ã‡§Ç‡§°‡§æ‡§ó‡§æ‡§Å‡§µ", "hierarchy": ["‡§õ‡§§‡•ç‡§§‡•Ä‡§∏‡§ó‡§¢‡§º", "‡§ï‡•ã‡§Ç‡§°‡§æ‡§ó‡§æ‡§Å‡§µ‡§ú‡§ø‡§≤‡§æ"]},
    "‡§®‡§æ‡§∞‡§æ‡§Ø‡§£‡§™‡•Å‡§∞": {"canonical": "‡§®‡§æ‡§∞‡§æ‡§Ø‡§£‡§™‡•Å‡§∞", "hierarchy": ["‡§õ‡§§‡•ç‡§§‡•Ä‡§∏‡§ó‡§¢‡§º", "‡§®‡§æ‡§∞‡§æ‡§Ø‡§£‡§™‡•Å‡§∞‡§ú‡§ø‡§≤‡§æ"]},
    "‡§ú‡§æ‡§Ç‡§ú‡§ó‡•Ä‡§∞": {"canonical": "‡§ú‡§æ‡§Ç‡§ú‡§ó‡•Ä‡§∞", "hierarchy": ["‡§õ‡§§‡•ç‡§§‡•Ä‡§∏‡§ó‡§¢‡§º", "‡§ú‡§æ‡§Ç‡§ú‡§ó‡•Ä‡§∞-‡§ö‡§Ç‡§™‡§æ‡§ú‡§ø‡§≤‡§æ"]},
    "Janjgir": {"canonical": "‡§ú‡§æ‡§Ç‡§ú‡§ó‡•Ä‡§∞", "hierarchy": ["‡§õ‡§§‡•ç‡§§‡•Ä‡§∏‡§ó‡§¢‡§º", "‡§ú‡§æ‡§Ç‡§ú‡§ó‡•Ä‡§∞-‡§ö‡§Ç‡§™‡§æ‡§ú‡§ø‡§≤‡§æ"]},
    "‡§ö‡§Ç‡§™‡§æ": {"canonical": "‡§ö‡§Ç‡§™‡§æ", "hierarchy": ["‡§õ‡§§‡•ç‡§§‡•Ä‡§∏‡§ó‡§¢‡§º", "‡§ú‡§æ‡§Ç‡§ú‡§ó‡•Ä‡§∞-‡§ö‡§Ç‡§™‡§æ‡§ú‡§ø‡§≤‡§æ"]},
    "‡§∞‡§æ‡§ú‡§®‡§æ‡§Ç‡§¶‡§ó‡§æ‡§Å‡§µ": {"canonical": "‡§∞‡§æ‡§ú‡§®‡§æ‡§Ç‡§¶‡§ó‡§æ‡§Å‡§µ", "hierarchy": ["‡§õ‡§§‡•ç‡§§‡•Ä‡§∏‡§ó‡§¢‡§º", "‡§∞‡§æ‡§ú‡§®‡§æ‡§Ç‡§¶‡§ó‡§æ‡§Å‡§µ‡§ú‡§ø‡§≤‡§æ"]},
    "‡§Æ‡§π‡§æ‡§∏‡§Æ‡•Å‡§Ç‡§¶": {"canonical": "‡§Æ‡§π‡§æ‡§∏‡§Æ‡•Å‡§Ç‡§¶", "hierarchy": ["‡§õ‡§§‡•ç‡§§‡•Ä‡§∏‡§ó‡§¢‡§º", "‡§Æ‡§π‡§æ‡§∏‡§Æ‡•Å‡§Ç‡§¶‡§ú‡§ø‡§≤‡§æ"]},
    "‡§ß‡§Æ‡§§‡§∞‡•Ä": {"canonical": "‡§ß‡§Æ‡§§‡§∞‡•Ä", "hierarchy": ["‡§õ‡§§‡•ç‡§§‡•Ä‡§∏‡§ó‡§¢‡§º", "‡§ß‡§Æ‡§§‡§∞‡•Ä‡§ú‡§ø‡§≤‡§æ"]},
    "‡§¨‡§æ‡§≤‡•ã‡§¶": {"canonical": "‡§¨‡§æ‡§≤‡•ã‡§¶", "hierarchy": ["‡§õ‡§§‡•ç‡§§‡•Ä‡§∏‡§ó‡§¢‡§º", "‡§¨‡§æ‡§≤‡•ã‡§¶‡§ú‡§ø‡§≤‡§æ"]},
    "‡§ó‡§∞‡§ø‡§Ø‡§æ‡§¨‡§Ç‡§¶": {"canonical": "‡§ó‡§∞‡§ø‡§Ø‡§æ‡§¨‡§Ç‡§¶", "hierarchy": ["‡§õ‡§§‡•ç‡§§‡•Ä‡§∏‡§ó‡§¢‡§º", "‡§ó‡§∞‡§ø‡§Ø‡§æ‡§¨‡§Ç‡§¶‡§ú‡§ø‡§≤‡§æ"]},
    "‡§¨‡•Ä‡§ú‡§æ‡§™‡•Å‡§∞": {"canonical": "‡§¨‡•Ä‡§ú‡§æ‡§™‡•Å‡§∞", "hierarchy": ["‡§õ‡§§‡•ç‡§§‡•Ä‡§∏‡§ó‡§¢‡§º", "‡§¨‡•Ä‡§ú‡§æ‡§™‡•Å‡§∞‡§ú‡§ø‡§≤‡§æ"]},
    "‡§¶‡§Ç‡§§‡•á‡§µ‡§æ‡§°‡§º‡§æ": {"canonical": "‡§¶‡§Ç‡§§‡•á‡§µ‡§æ‡§°‡§º‡§æ", "hierarchy": ["‡§õ‡§§‡•ç‡§§‡•Ä‡§∏‡§ó‡§¢‡§º", "‡§¶‡§Ç‡§§‡•á‡§µ‡§æ‡§°‡§º‡§æ‡§ú‡§ø‡§≤‡§æ"]},
    "‡§∏‡•Å‡§ï‡§Æ‡§æ": {"canonical": "‡§∏‡•Å‡§ï‡§Æ‡§æ", "hierarchy": ["‡§õ‡§§‡•ç‡§§‡•Ä‡§∏‡§ó‡§¢‡§º", "‡§∏‡•Å‡§ï‡§Æ‡§æ‡§ú‡§ø‡§≤‡§æ"]},
    "‡§¨‡§≤‡•å‡§¶‡§æ‡§¨‡§æ‡§ú‡§æ‡§∞": {"canonical": "‡§¨‡§≤‡•å‡§¶‡§æ‡§¨‡§æ‡§ú‡§æ‡§∞", "hierarchy": ["‡§õ‡§§‡•ç‡§§‡•Ä‡§∏‡§ó‡§¢‡§º", "‡§¨‡§≤‡•å‡§¶‡§æ‡§¨‡§æ‡§ú‡§æ‡§∞‡§ú‡§ø‡§≤‡§æ"]},
    "‡§≠‡§æ‡§ü‡§æ‡§™‡§æ‡§∞‡§æ": {"canonical": "‡§¨‡§≤‡•å‡§¶‡§æ‡§¨‡§æ‡§ú‡§æ‡§∞", "hierarchy": ["‡§õ‡§§‡•ç‡§§‡•Ä‡§∏‡§ó‡§¢‡§º", "‡§¨‡§≤‡•å‡§¶‡§æ‡§¨‡§æ‡§ú‡§æ‡§∞‡§ú‡§ø‡§≤‡§æ"]},
    "‡§ï‡§µ‡§∞‡•ç‡§ß‡§æ": {"canonical": "‡§ï‡§µ‡§∞‡•ç‡§ß‡§æ", "hierarchy": ["‡§õ‡§§‡•ç‡§§‡•Ä‡§∏‡§ó‡§¢‡§º", "‡§ï‡§¨‡•Ä‡§∞‡§ß‡§æ‡§Æ‡§ú‡§ø‡§≤‡§æ"]},
    "‡§ï‡§æ‡§Ç‡§ï‡•á‡§∞": {"canonical": "‡§ï‡§æ‡§Ç‡§ï‡•á‡§∞", "hierarchy": ["‡§õ‡§§‡•ç‡§§‡•Ä‡§∏‡§ó‡§¢‡§º", "‡§ï‡§æ‡§Ç‡§ï‡•á‡§∞‡§ú‡§ø‡§≤‡§æ"]},
    "‡§ï‡•ã‡§∞‡§ø‡§Ø‡§æ": {"canonical": "‡§ï‡•ã‡§∞‡§ø‡§Ø‡§æ", "hierarchy": ["‡§õ‡§§‡•ç‡§§‡•Ä‡§∏‡§ó‡§¢‡§º", "‡§ï‡•ã‡§∞‡§ø‡§Ø‡§æ‡§ú‡§ø‡§≤‡§æ"]},
    "‡§ú‡§∂‡§™‡•Å‡§∞": {"canonical": "‡§ú‡§∂‡§™‡•Å‡§∞", "hierarchy": ["‡§õ‡§§‡•ç‡§§‡•Ä‡§∏‡§ó‡§¢‡§º", "‡§ú‡§∂‡§™‡•Å‡§∞‡§ú‡§ø‡§≤‡§æ"]},
    "‡§Æ‡•Å‡§Ç‡§ó‡•á‡§≤‡•Ä": {"canonical": "‡§Æ‡•Å‡§Ç‡§ó‡•á‡§≤‡•Ä", "hierarchy": ["‡§õ‡§§‡•ç‡§§‡•Ä‡§∏‡§ó‡§¢‡§º", "‡§Æ‡•Å‡§Ç‡§ó‡•á‡§≤‡•Ä‡§ú‡§ø‡§≤‡§æ"]},
    "‡§¨‡•á‡§Æ‡•á‡§§‡§∞‡§æ": {"canonical": "‡§¨‡•á‡§Æ‡•á‡§§‡§∞‡§æ", "hierarchy": ["‡§õ‡§§‡•ç‡§§‡•Ä‡§∏‡§ó‡§¢‡§º", "‡§¨‡•á‡§Æ‡•á‡§§‡§∞‡§æ‡§ú‡§ø‡§≤‡§æ"]},
    "‡§ó‡•å‡§∞‡•á‡§≤‡§æ": {"canonical": "‡§ó‡•å‡§∞‡•á‡§≤‡§æ-‡§™‡•á‡§Ç‡§°‡•ç‡§∞‡§æ-‡§Æ‡§∞‡§µ‡§æ‡§π‡•Ä", "hierarchy": ["‡§õ‡§§‡•ç‡§§‡•Ä‡§∏‡§ó‡§¢‡§º", "GPM‡§ú‡§ø‡§≤‡§æ"]},
    "‡§™‡•á‡§Ç‡§°‡•ç‡§∞‡§æ": {"canonical": "‡§ó‡•å‡§∞‡•á‡§≤‡§æ-‡§™‡•á‡§Ç‡§°‡•ç‡§∞‡§æ-‡§Æ‡§∞‡§µ‡§æ‡§π‡•Ä", "hierarchy": ["‡§õ‡§§‡•ç‡§§‡•Ä‡§∏‡§ó‡§¢‡§º", "GPM‡§ú‡§ø‡§≤‡§æ"]},
    "‡§∏‡§æ‡§∞‡§Ç‡§ó‡§¢‡§º": {"canonical": "‡§∏‡§æ‡§∞‡§Ç‡§ó‡§¢‡§º-‡§¨‡§ø‡§≤‡§æ‡§à‡§ó‡§¢‡§º", "hierarchy": ["‡§õ‡§§‡•ç‡§§‡•Ä‡§∏‡§ó‡§¢‡§º", "‡§∏‡§æ‡§∞‡§Ç‡§ó‡§¢‡§º-‡§¨‡§ø‡§≤‡§æ‡§à‡§ó‡§¢‡§º‡§ú‡§ø‡§≤‡§æ"]},
    "‡§Æ‡•ã‡§π‡§≤‡§æ": {"canonical": "‡§Æ‡•ã‡§π‡§≤‡§æ-‡§Æ‡§æ‡§®‡§™‡•Å‡§∞", "hierarchy": ["‡§õ‡§§‡•ç‡§§‡•Ä‡§∏‡§ó‡§¢‡§º", "‡§Æ‡•ã‡§π‡§≤‡§æ-‡§Æ‡§æ‡§®‡§™‡•Å‡§∞‡§ú‡§ø‡§≤‡§æ"]},
    "‡§∂‡§ï‡•ç‡§§‡§ø": {"canonical": "‡§∂‡§ï‡•ç‡§§‡§ø", "hierarchy": ["‡§õ‡§§‡•ç‡§§‡•Ä‡§∏‡§ó‡§¢‡§º", "‡§∂‡§ï‡•ç‡§§‡§ø‡§ú‡§ø‡§≤‡§æ"]},
    "‡§ñ‡•à‡§∞‡§æ‡§ó‡§¢‡§º": {"canonical": "‡§ñ‡•à‡§∞‡§æ‡§ó‡§¢‡§º", "hierarchy": ["‡§õ‡§§‡•ç‡§§‡•Ä‡§∏‡§ó‡§¢‡§º", "‡§ñ‡•à‡§∞‡§æ‡§ó‡§¢‡§º‡§ú‡§ø‡§≤‡§æ"]},
    "‡§Æ‡§®‡•á‡§Ç‡§¶‡•ç‡§∞‡§ó‡§¢‡§º": {"canonical": "‡§Æ‡§®‡•á‡§Ç‡§¶‡•ç‡§∞‡§ó‡§¢‡§º", "hierarchy": ["‡§õ‡§§‡•ç‡§§‡•Ä‡§∏‡§ó‡§¢‡§º", "MCB‡§ú‡§ø‡§≤‡§æ"]},
}

# ==========================================
# GEO HIERARCHY RESOLVER
# ==========================================

class GeoHierarchyResolver:
    """Resolve complete administrative hierarchy: District->Block->GP->Village/ULB->Ward"""
    
    def __init__(self):
        # Load comprehensive geography datasets
        self.villages_data = self._load_villages_data()
        self.constituencies = load_json(CONSTITUENCIES_PATH)
        self.urban_data = self._load_urban_data()
        
        # Build indexes
        self.village_index = self._build_village_index()      # ~20K villages
        self.ulb_index = self._build_ulb_index()              # ULBs
        self.district_map = self._build_district_map()         # 33 districts
    
    def _load_villages_data(self) -> List[Dict]:
        if FULL_VILLAGES_PATH.exists():
            data = load_json(FULL_VILLAGES_PATH)
            return data.get("villages", [])
        return []

    def _load_urban_data(self) -> List[Dict]:
        if URBAN_DATA_PATH.exists():
            return load_ndjson(URBAN_DATA_PATH)
        return []

    def _build_village_index(self) -> Dict[str, Dict]:
        """Index: village_name -> {district, block, gp, ac, pc, hierarchy}"""
        index = {}
        for row in self.villages_data:
            village = row.get("name")
            if village:
                # Normalize name if needed
                index[village] = {
                    "district": row.get("district"),
                    "block": row.get("block"),
                    "gp": row.get("gram_panchayat"),
                    "assembly": row.get("assembly_constituency"),
                    "parliamentary": row.get("parliamentary_constituency"),
                    "hierarchy_path": [
                        "‡§õ‡§§‡•ç‡§§‡•Ä‡§∏‡§ó‡§¢‡§º",
                        f"{row.get('district', '')} ‡§ú‡§ø‡§≤‡§æ",
                        row.get('assembly_constituency', ''),
                        f"{row.get('block', '')} ‡§¨‡•ç‡§≤‡•â‡§ï",
                        f"{row.get('gram_panchayat', '')} ‡§™‡§Ç‡§ö‡§æ‡§Ø‡§§"
                    ],
                    "type": "rural"
                }
        return index
    
    def _build_ulb_index(self) -> Dict[str, Dict]:
        """Index: ulb_name -> {district, ulb_type, ward_count, ac, pc, hierarchy}"""
        index = {}
        
        districts = self.constituencies.get("districts", {})
        for dist_name, dist_data in districts.items():
            ulbs = dist_data.get("ulb_names", [])
            for ulb in ulbs:
                index[ulb] = {
                    "district": dist_name,
                    "ulb_type": "ULB", # Generic if not known
                    "ward_count": 0,
                    "assembly": dist_data.get("assembly"), # Default to district HQ assembly or similar
                    "parliamentary": dist_data.get("parliamentary"),
                    "hierarchy_path": ["‡§õ‡§§‡•ç‡§§‡•Ä‡§∏‡§ó‡§¢‡§º", f"{dist_name} ‡§ú‡§ø‡§≤‡§æ", ulb],
                    "type": "urban"
                }

        # From urban_data (if available, richer data)
        for row in self.urban_data:
            ulb = row.get("ulb") or row.get("nagar_nigam") or row.get("nagar_palika")
            if ulb:
                index[ulb] = {
                    "district": row.get("district"),
                    "ulb_type": row.get("ulb_type"),  # "‡§®‡§ó‡§∞ ‡§®‡§ø‡§ó‡§Æ", "‡§®‡§ó‡§∞ ‡§™‡§æ‡§≤‡§ø‡§ï‡§æ", etc
                    "ward_count": row.get("ward_count", 0),
                    "assembly": row.get("assembly"),
                    "parliamentary": row.get("parliamentary"),
                    "hierarchy_path": [
                        "‡§õ‡§§‡•ç‡§§‡•Ä‡§∏‡§ó‡§¢‡§º", 
                        f"{row.get('district', '')} ‡§ú‡§ø‡§≤‡§æ", 
                        ulb
                    ],
                    "type": "urban"
                }
        return index

    def _build_district_map(self) -> Dict[str, Dict]:
        """Index: district_name -> details"""
        index = {}
        districts = self.constituencies.get("districts", {})
        for dist_name, dist_data in districts.items():
            index[dist_name] = {
                "canonical": dist_name,
                "hierarchy": ["‡§õ‡§§‡•ç‡§§‡•Ä‡§∏‡§ó‡§¢‡§º", f"{dist_name} ‡§ú‡§ø‡§≤‡§æ"],
                "assembly": dist_data.get("assembly"),
                "parliamentary": dist_data.get("parliamentary")
            }
        return index
    
    def resolve_hierarchy(self, location_name: str, context_text: str = "") -> Optional[Dict]:
        """
        Resolve complete hierarchy for any location mention.
        """
        # Priority 1: Village lookup
        if location_name in self.village_index:
            v = self.village_index[location_name]
            return {
                "district": v["district"],
                "assembly": v["assembly"],
                "parliamentary": v["parliamentary"],
                "block": v["block"],
                "gp": v["gp"],
                "village": location_name,
                "ulb": None,
                "ward": None,
                "zone": None,
                "hierarchy_path": [p for p in v["hierarchy_path"] if p],
                "canonical": location_name,
                "canonical_key": f"CG_VILLAGE_{location_name}",
                "location_type": "rural",
                "source": "hierarchy_resolver"
            }
        
        # Priority 2: ULB lookup
        if location_name in self.ulb_index:
            u = self.ulb_index[location_name]
            
            # Extract ward/zone from context
            ward = self._extract_ward(context_text)
            zone = self._extract_zone(context_text)
            
            hierarchy = u["hierarchy_path"] + ([f"‡§µ‡§æ‡§∞‡•ç‡§° {ward}"] if ward else [])
            
            return {
                "district": u["district"],
                "assembly": u["assembly"],
                "parliamentary": u["parliamentary"],
                "block": None,
                "gp": None,
                "village": None,
                "ulb": location_name,
                "ulb_type": u["ulb_type"],
                "ward": ward,
                "zone": zone,
                "hierarchy_path": [p for p in hierarchy if p],
                "canonical": location_name,
                "canonical_key": f"CG_ULB_{location_name}",
                "location_type": "urban",
                "source": "hierarchy_resolver"
            }
        
        # Priority 3: District/Tehsil/Block (from constituencies.json blocks or district map)
        if location_name in self.district_map:
             d = self.district_map[location_name]
             return {
                "district": d["canonical"],
                "assembly": d["assembly"],
                "parliamentary": d["parliamentary"],
                "hierarchy_path": d["hierarchy"],
                "canonical": d["canonical"],
                "canonical_key": f"CG_DISTRICT_{d['canonical']}",
                "location_type": "district",
                "source": "hierarchy_resolver"
             }

        return None
    
    def _extract_ward(self, text: str) -> Optional[str]:
        """Extract ward number from context: '‡§µ‡§æ‡§∞‡•ç‡§° 12', 'ward 12'"""
        patterns = [
            r"‡§µ‡§æ‡§∞‡•ç‡§°\s*(?:‡§®‡§Ç‡§¨‡§∞\s*)?(\d+)",
            r"ward\s*(?:no\.IBLE\s*)?(\d+)",
            r"‡§µ‡§æ‡§∞‡•ç‡§°\s+([‡§Ö-‡§π]+)"  # Hindi number words could be handled here
        ]
        for pattern in patterns:
            match = re.search(pattern, text, re.IGNORECASE)
            if match:
                return match.group(1)
        return None
    
    def _extract_zone(self, text: str) -> Optional[str]:
        """Extract zone info: '‡§ú‡•ã‡§® 1', 'zone A'"""
        patterns = [
             r"‡§ú‡•ã‡§®\s*(?:‡§®‡§Ç‡§¨‡§∞\s*)?(\d+)",
             r"zone\s*(?:no\.IBLE\s*)?(\d+)",
             r"‡§ú‡•ã‡§®\s+([‡§Ö-‡§π]+)"
        ]
        for pattern in patterns:
            match = re.search(pattern, text, re.IGNORECASE)
            if match:
                return match.group(1)
        return None

# ==========================================
# LOCATION RESOLVER
# ==========================================

class HybridLocationResolver:
    """Combines dictionary, semantic search, and hierarchy resolution"""
    
    def __init__(self, enable_semantic=True, data_dir: Optional[Path] = None):
        self.enable_semantic = enable_semantic
        self.semantic_linker = None
        
        if enable_semantic:
            try:
                from api.src.parsing.semantic_location_linker import MultilingualFAISSLocationLinker
                self.semantic_linker = MultilingualFAISSLocationLinker()
                # Ensure data is loaded (this might take a moment)
                self.semantic_linker.load_multilingual_data()
            except ImportError:
                print("‚ö†Ô∏è  Semantic linker not available. Falling back to rule-based only.")
                self.enable_semantic = False
            except Exception as e:
                print(f"‚ö†Ô∏è  Semantic linker init failed: {e}")
                self.enable_semantic = False
                
        self.geo_resolver = GeoHierarchyResolver()
        self.dictionary = CANONICAL_LOCATIONS
        
        self.stats = {
            'dict_hits': 0,
            'semantic_hits': 0,
            'hierarchy_enrichments': 0,
            'not_found': 0
        }
    
    def resolve(self, text: str, old_location: Optional[Dict] = None) -> Tuple[Optional[Dict], float]:
        """
        Three-stage resolution:
        1. Dictionary lookup (fast, high confidence)
        2. FAISS semantic search (medium confidence)
        3. Geo-hierarchy enrichment (adds missing fields)
        """
        # Stage 1: Dictionary
        dict_match, dict_conf = self._dictionary_lookup(text, old_location)
        if dict_match and dict_conf >= 0.88:
            self.stats['dict_hits'] += 1
            # Enrich with full hierarchy
            enriched = self.geo_resolver.resolve_hierarchy(
                dict_match["canonical"],
                context_text=text
            )
            if enriched:
                self.stats['hierarchy_enrichments'] += 1
                return enriched, dict_conf
            return dict_match, dict_conf
        
        # Stage 2: Extract candidates and run semantic search
        if self.enable_semantic and self.semantic_linker:
            candidates = self._extract_location_candidates(text)
            semantic_matches = []
            
            for candidate in candidates:
                # Skip short candidates
                if len(candidate) < 3: continue
                
                matches = self.semantic_linker.find_semantic_matches(
                    candidate, 
                    limit=3, 
                    min_score=0.75
                )
                if matches:
                    semantic_matches.extend(matches)
            
            if semantic_matches:
                # Take best match
                best = max(semantic_matches, key=lambda x: x['similarity_score'])
                best_name = best['name']
                best_score = best['similarity_score']
                
                # Stage 3: Resolve full hierarchy
                full_location = self.geo_resolver.resolve_hierarchy(best_name, text)
                
                if full_location:
                    self.stats['semantic_hits'] += 1
                    # Slight discount for semantic match compared to exact dictionary match
                    final_score = best_score * 0.95 
                    full_location["source"] = "semantic"
                    return full_location, final_score
        
        # Fallback: partial location from dictionary if it was found but low confidence
        if dict_match:
            self.stats['dict_hits'] += 1
            return dict_match, dict_conf
        
        self.stats['not_found'] += 1
        return None, 0.0

    def _dictionary_lookup(self, text: str, old_location: Optional[Dict]) -> Tuple[Optional[Dict], float]:
        candidates = []
        if old_location and old_location.get("canonical"):
            candidates.append(old_location["canonical"])

        text_lower = text.lower()
        for key, info in self.dictionary.items():
            if key in text or key.lower() in text_lower:
                candidates.append(key)
        
        # Also check inline candidates against dictionary
        inline_candidates = self._extract_location_candidates(text)
        for cand in inline_candidates:
            if cand in self.dictionary:
                candidates.append(cand)

        if not candidates:
            return None, 0.0

        best_raw = Counter(candidates).most_common(1)[0][0]
        loc_info = self.dictionary.get(best_raw)
        
        if not loc_info:
             return None, 0.0

        return {
            "district": loc_info.get("hierarchy", ["", ""])[-1].replace(" ‡§ú‡§ø‡§≤‡§æ", ""),
            "canonical": loc_info["canonical"],
            "hierarchy_path": loc_info.get("hierarchy", []),
            "visit_count": 1,
            "canonical_key": f"CG_{loc_info['canonical']}",
            "source": "dictionary"
        }, 0.88

    def _extract_location_candidates(self, text: str) -> List[str]:
        """Extract location names with administrative markers"""
        # Simplified pattern to avoid range errors. Matches Devanagari and Latin chars.
        # \u0900-\u097F is the Devanagari block.
        name_pattern = r"([‡§Ö-‡§πA-Za-z\u0900-\u097F]+)"
        
        patterns = [
            name_pattern + r"\s+‡§ú‡§ø‡§≤‡§æ",
            name_pattern + r"\s+‡§µ‡§ø‡§ß‡§æ‡§®‡§∏‡§≠‡§æ",
            name_pattern + r"\s+‡§§‡§π‡§∏‡•Ä‡§≤",
            name_pattern + r"\s+‡§•‡§æ‡§®‡§æ",
            name_pattern + r"\s+‡§µ‡§ø‡§ï‡§æ‡§∏‡§ñ‡§Ç‡§°",
            name_pattern + r"\s+‡§ö‡•å‡§ï‡•Ä",
            name_pattern + r"\s+‡§ó‡•ç‡§∞‡§æ‡§Æ\s+‡§™‡§Ç‡§ö‡§æ‡§Ø‡§§",
            name_pattern + r"\s+‡§ó‡§æ‡§Å‡§µ",
            name_pattern + r"\s+‡§®‡§ó‡§∞\s+(?:‡§®‡§ø‡§ó‡§Æ|‡§™‡§æ‡§≤‡§ø‡§ï‡§æ|‡§™‡§Ç‡§ö‡§æ‡§Ø‡§§)"
        ]
        candidates = []
        for pattern in patterns:
            for match in re.finditer(pattern, text):
                name = match.group(1).strip()
                if len(name) >= 2:
                    candidates.append(name)
        return list(set(candidates))  # Deduplicate

    def get_stats(self):
        return self.stats

# ==========================================
# ENTITY EXTRACTOR
# ==========================================

class EntityExtractor:
    """
    Entity Extraction Module
    
    Enhanced extraction with INTRA-TWEET entity deduplication.
    All extractors return deduplicated lists.
    """
    
    def extract_schemes(self, text: str) -> Tuple[List[str], float]:
        schemes = set()
        for pattern, canonical in SCHEME_PATTERNS.items():
            if re.search(pattern, text, flags=re.IGNORECASE):
                schemes.add(canonical)
        return sorted(list(schemes)), 0.0

    def extract_target_groups(self, text: str) -> Tuple[List[str], float]:
        groups = set()
        keywords = ["‡§ï‡§ø‡§∏‡§æ‡§®", "‡§Æ‡§π‡§ø‡§≤‡§æ", "‡§Ø‡•Å‡§µ‡§æ", "‡§õ‡§æ‡§§‡•ç‡§∞", "‡§Ü‡§¶‡§ø‡§µ‡§æ‡§∏‡•Ä", "farmers", "women", "youth", "students", "tribals"]
        text_lower = text.lower()
        for kw in keywords:
            if kw in text_lower:
                groups.add(kw)
        return sorted(list(groups)), 0.0

    def extract_communities(self, text: str) -> Tuple[List[str], float]:
        # Placeholder
        return [], 0.0

    def extract_orgs(self, text: str) -> Tuple[List[str], float]:
        # Placeholder
        return [], 0.0

    def extract_word_buckets(self, text: str) -> Tuple[List[str], float]:
        # Placeholder
        return [], 0.0

# ==========================================
# RESCUE DETECTOR
# ==========================================

class RescueDetector:
    """Detect specific event types to rescue them from 'Other' classification"""
    
    def rescue(self, text: str, current_event: str, location: Optional[Dict], schemes: List[str]) -> Dict[str, Any]:
        text_l = text.lower()
        
        rescue_info = {
            "event_type": current_event,
            "content_mode": "‡§°‡§ø‡§ú‡§ø‡§ü‡§≤ / ‡§∏‡•ã‡§∂‡§≤-media ‡§™‡•ã‡§∏‡•ç‡§ü",
            "is_rescued": False,
            "rescue_tag": None,
            "confidence_bonus": 0.0
        }

        # --- Priority 1: High Specificity ---
        if self._looks_like_sports_tweet(text_l) or self._looks_like_sports_achievement(text_l):
            rescue_info.update({
                "event_type": "‡§ñ‡•á‡§≤ / ‡§ó‡•å‡§∞‡§µ", 
                "content_mode": "‡§ñ‡•á‡§≤ / ‡§â‡§™‡§≤‡§¨‡•ç‡§ß‡§ø ‡§™‡§∞ ‡§™‡•ç‡§∞‡§§‡§ø‡§ï‡•ç‡§∞‡§ø‡§Ø‡§æ",
                "is_rescued": current_event == "‡§Ö‡§®‡•ç‡§Ø",
                "rescue_tag": "sports_v8",
                "confidence_bonus": 0.18
            })
            return rescue_info

        if self._looks_like_security_context(text_l):
            rescue_info.update({
                "event_type": "‡§Ü‡§Ç‡§§‡§∞‡§ø‡§ï ‡§∏‡•Å‡§∞‡§ï‡•ç‡§∑‡§æ / ‡§™‡•Å‡§≤‡§ø‡§∏", 
                "content_mode": "‡§®‡•Ä‡§§‡§ø / ‡§µ‡§ï‡•ç‡§§‡§µ‡•ç‡§Ø",
                "is_rescued": current_event == "‡§Ö‡§®‡•ç‡§Ø",
                "rescue_tag": "security_v8",
                "confidence_bonus": 0.20
            })
            return rescue_info

        # --- Priority 2: Governance ---
        if self._looks_like_administrative_update(text_l):
            rescue_info.update({
                "event_type": "‡§™‡•ç‡§∞‡§∂‡§æ‡§∏‡§®‡§ø‡§ï ‡§∏‡§Æ‡•Ä‡§ï‡•ç‡§∑‡§æ ‡§¨‡•à‡§†‡§ï", 
                "content_mode": "‡§®‡•Ä‡§§‡§ø / ‡§µ‡§ï‡•ç‡§§‡§µ‡•ç‡§Ø",
                "is_rescued": current_event == "‡§Ö‡§®‡•ç‡§Ø",
                "rescue_tag": "admin_v8",
                "confidence_bonus": 0.15
            })
            return rescue_info

        if self._looks_like_election_politics(text_l):
            rescue_info.update({
                "event_type": "‡§ö‡•Å‡§®‡§æ‡§µ ‡§™‡•ç‡§∞‡§ö‡§æ‡§∞", 
                "content_mode": "‡§Æ‡•à‡§¶‡§æ‡§®-‡§∏‡•ç‡§§‡§∞ ‡§ï‡§æ‡§∞‡•ç‡§Ø‡§ï‡•ç‡§∞‡§Æ",
                "is_rescued": current_event == "‡§Ö‡§®‡•ç‡§Ø",
                "rescue_tag": "election_v8",
                "confidence_bonus": 0.17
            })
            return rescue_info

        # --- Priority 3: Development & Schemes ---
        if self._looks_like_industrial_development(text_l) or self._looks_like_infrastructure_work(text_l):
            rescue_info.update({
                "event_type": "‡§â‡§¶‡•ç‡§ò‡§æ‡§ü‡§®", 
                "content_mode": "‡§Æ‡•à‡§¶‡§æ‡§®-‡§∏‡•ç‡§§‡§∞ ‡§ï‡§æ‡§∞‡•ç‡§Ø‡§ï‡•ç‡§∞‡§Æ",
                "is_rescued": current_event == "‡§Ö‡§®‡•ç‡§Ø",
                "rescue_tag": "infra_dev",
                "confidence_bonus": 0.16
            })
            return rescue_info

        if self._looks_like_scheme_implementation(text_l, schemes) or self._looks_like_relief_humanitarian(text_l):
            rescue_info.update({
                "event_type": "‡§Ø‡•ã‡§ú‡§®‡§æ ‡§ò‡•ã‡§∑‡§£‡§æ", 
                "content_mode": "‡§Æ‡•à‡§¶‡§æ‡§®-‡§∏‡•ç‡§§‡§∞ ‡§ï‡§æ‡§∞‡•ç‡§Ø‡§ï‡•ç‡§∞‡§Æ",
                "is_rescued": current_event == "‡§Ö‡§®‡•ç‡§Ø",
                "rescue_tag": "scheme_v8",
                "confidence_bonus": 0.15
            })
            return rescue_info

        # --- Priority 4: Political / Social ---
        if self._looks_like_general_political(text_l) or self._looks_like_policy_statement(text_l):
            rescue_info.update({
                "event_type": "‡§∞‡§æ‡§ú‡§®‡•Ä‡§§‡§ø‡§ï ‡§µ‡§ï‡•ç‡§§‡§µ‡•ç‡§Ø", 
                "content_mode": "‡§®‡•Ä‡§§‡§ø / ‡§µ‡§ï‡•ç‡§§‡§µ‡•ç‡§Ø",
                "is_rescued": current_event == "‡§Ö‡§®‡•ç‡§Ø",
                "rescue_tag": "political_v8",
                "confidence_bonus": 0.15
            })
            return rescue_info

        if self._looks_like_cultural_religious(text_l):
            rescue_info.update({
                "event_type": "‡§ß‡§æ‡§∞‡•ç‡§Æ‡§ø‡§ï / ‡§∏‡§æ‡§Ç‡§∏‡•ç‡§ï‡•É‡§§‡§ø‡§ï ‡§ï‡§æ‡§∞‡•ç‡§Ø‡§ï‡•ç‡§∞‡§Æ", 
                "content_mode": "‡§∏‡§æ‡§Æ‡§æ‡§®‡•ç‡§Ø ‡§∂‡•Å‡§≠‡§ï‡§æ‡§Æ‡§®‡§æ‡§è‡§Å / ‡§™‡§∞‡•ç‡§µ",
                "is_rescued": current_event == "‡§Ö‡§®‡•ç‡§Ø",
                "rescue_tag": "cultural_v8",
                "confidence_bonus": 0.14
            })
            return rescue_info

        if self._looks_like_congratulatory_general(text_l):
            rescue_info.update({
                "event_type": "‡§∂‡•Å‡§≠‡§ï‡§æ‡§Æ‡§®‡§æ / ‡§¨‡§ß‡§æ‡§à", 
                "content_mode": "‡§∏‡§æ‡§Æ‡§æ‡§®‡•ç‡§Ø ‡§∂‡•Å‡§≠‡§ï‡§æ‡§Æ‡§®‡§æ‡§è‡§Å / ‡§™‡§∞‡•ç‡§µ",
                "is_rescued": current_event == "‡§Ö‡§®‡•ç‡§Ø",
                "rescue_tag": "greetings_v8",
                "confidence_bonus": 0.10
            })
            return rescue_info

        return rescue_info

    def _looks_like_sports_tweet(self, text_l: str) -> bool:
        SPORTS_SPECIFIC = ["‡§ï‡•ç‡§∞‡§ø‡§ï‡•á‡§ü", "‡§ü‡•Ä‡§Æ ‡§á‡§Ç‡§°‡§ø‡§Ø‡§æ", "world cup", "t20", "ipl", "odi", "bcci", "‡§∞‡§£‡§ú‡•Ä"]
        if any(kw in text_l for kw in SPORTS_SPECIFIC): return True
        if "‡§Æ‡•à‡§ö" in text_l and any(kw in text_l for kw in ["‡§ú‡•Ä‡§§", "‡§π‡§æ‡§∞", "‡§µ‡§ø‡§ï‡•á‡§ü", "‡§∞‡§®", "won", "lost"]): return True
        return False

    def _looks_like_sports_achievement(self, text_l: str) -> bool:
        return any(kw in text_l for kw in ["‡§∏‡•ç‡§µ‡§∞‡•ç‡§£ ‡§™‡§¶‡§ï", "‡§∞‡§ú‡§§ ‡§™‡§¶‡§ï", "‡§ï‡§æ‡§Ç‡§∏‡•ç‡§Ø ‡§™‡§¶‡§ï", "medal", "gold medal", "championship"])

    def _looks_like_security_context(self, text_l: str) -> bool:
        return any(kw in text_l for kw in ["‡§Æ‡§æ‡§ì‡§µ‡§æ‡§¶‡•Ä", "‡§Æ‡§æ‡§ì‡§µ‡§æ‡§¶", "‡§®‡§ï‡•ç‡§∏‡§≤", "‡§Ü‡§§‡§Ç‡§ï", "‡§â‡§ó‡•ç‡§∞‡§µ‡§æ‡§¶", "‡§∂‡§π‡•Ä‡§¶", "jawan", "encounter"])

    def _looks_like_administrative_update(self, text_l: str) -> bool:
        return any(kw in text_l for kw in ["‡§¨‡•à‡§†‡§ï", "‡§∏‡§Æ‡•Ä‡§ï‡•ç‡§∑‡§æ", "‡§ï‡§≤‡•á‡§ï‡•ç‡§ü‡§∞", "‡§®‡§ø‡§∞‡•ç‡§¶‡•á‡§∂", "‡§Ö‡§ß‡§ø‡§ï‡§æ‡§∞‡•Ä", "progress", "status", "‡§®‡§ø‡§∞‡•Ä‡§ï‡•ç‡§∑‡§£", "inspection"])

    def _looks_like_scheme_implementation(self, text_l: str, schemes: List) -> bool:
        return bool(schemes) or any(kw in text_l for kw in ["‡§≤‡§æ‡§≠‡§æ‡§∞‡•ç‡§•‡•Ä", "‡§µ‡§ø‡§§‡§∞‡§£", "‡§ñ‡§æ‡§§‡§æ", "subsidy", "dbt", "installments"])

    def _looks_like_election_politics(self, text_l: str) -> bool:
        return any(kw in text_l for kw in ["‡§ö‡•Å‡§®‡§æ‡§µ", "‡§Æ‡§§‡§¶‡§æ‡§®", "‡§µ‡•ã‡§ü", "‡§™‡•ç‡§∞‡§ö‡§æ‡§∞", "‡§ï‡•à‡§Ç‡§™‡•á‡§®", "‡§™‡•ç‡§∞‡§§‡•ç‡§Ø‡§æ‡§∂‡•Ä", "nomination"])

    def _looks_like_industrial_development(self, text_l: str) -> bool:
        return any(kw in text_l for kw in ["‡§â‡§¶‡•ç‡§Ø‡•ã‡§ó", "‡§®‡§ø‡§µ‡•á‡§∂", "‡§´‡•à‡§ï‡•ç‡§ü‡•ç‡§∞‡•Ä", "‡§∞‡•ã‡§ú‡§ó‡§æ‡§∞", "infotech", "industrial", "mou"])

    def _looks_like_infrastructure_work(self, text_l: str) -> bool:
        return any(kw in text_l for kw in ["‡§∏‡§°‡§º‡§ï", "‡§™‡•Å‡§≤", "‡§≠‡§µ‡§®", "‡§®‡§ø‡§∞‡•ç‡§Æ‡§æ‡§£", "construction", "bridge", "highway"])

    def _looks_like_relief_humanitarian(self, text_l: str) -> bool:
        return any(kw in text_l for kw in ["‡§∞‡§æ‡§π‡§§", "‡§Ü‡§™‡§¶‡§æ", "‡§¨‡§æ‡§¢‡§º", "‡§Æ‡•Å‡§Ü‡§µ‡§ú‡§æ", "‡§ï‡•ç‡§∑‡§§‡§ø‡§™‡•Ç‡§∞‡•ç‡§§‡§ø", "‡§π‡§æ‡§¶‡§∏‡§æ", "‡§¶‡•Å‡§∞‡•ç‡§ò‡§ü‡§®‡§æ"])

    def _looks_like_general_political(self, text_l: str) -> bool:
        return any(kw in text_l for kw in ["‡§°‡§¨‡§≤ ‡§á‡§Ç‡§ú‡§®", "‡§ï‡§æ‡§Ç‡§ó‡•ç‡§∞‡•á‡§∏", "‡§≠‡§æ‡§ú‡§™‡§æ", "‡§µ‡§ø‡§™‡§ï‡•ç‡§∑", "‡§§‡•Å‡§∑‡•ç‡§ü‡§ø‡§ï‡§∞‡§£", "‡§≠‡•ç‡§∞‡§∑‡•ç‡§ü‡§æ‡§ö‡§æ‡§∞", "‡§Ü‡§∞‡•ã‡§™"])

    def _looks_like_policy_statement(self, text_l: str) -> bool:
        return any(kw in text_l for kw in ["‡§µ‡§ø‡§ï‡§∏‡§ø‡§§ ‡§≠‡§æ‡§∞‡§§", "‡§Æ‡•ã‡§¶‡•Ä ‡§ï‡•Ä ‡§ó‡§æ‡§∞‡§Ç‡§ü‡•Ä", "‡§∏‡§¨‡§ï‡§æ ‡§∏‡§æ‡§•", "‡§∏‡§Ç‡§ï‡§≤‡•ç‡§™"])

    def _looks_like_cultural_religious(self, text_l: str) -> bool:
        return any(kw in text_l for kw in ["‡§Æ‡§Ç‡§¶‡§ø‡§∞", "‡§™‡•Ç‡§ú‡§æ", "‡§¶‡§∞‡•ç‡§∂‡§®", "‡§ú‡§Ø‡§Ç‡§§‡•Ä", "‡§Æ‡§π‡•ã‡§§‡•ç‡§∏‡§µ", "‡§™‡§∞‡•ç‡§µ", "arti"])

    def _looks_like_congratulatory_general(self, text_l: str) -> bool:
        return any(kw in text_l for kw in ["‡§¨‡§ß‡§æ‡§à", "‡§∂‡•Å‡§≠‡§ï‡§æ‡§Æ‡§®‡§æ", "best wishes"])

# ==========================================
# CONFIDENCE SCORER
# ==========================================

class ConfidenceScorer:
    """Compute confidence score for parsed tweet"""
    
    def compute(self, event_type: str, location: Optional[Dict], schemes: List[str], 
                target_groups: List[str], communities: List[str], orgs: List[str], 
                people: List[str], text_len: int, rescue_info: Dict[str, Any]) -> float:
        
        # Base confidence
        base_conf = 0.4
        if event_type != "‡§Ö‡§®‡•ç‡§Ø":
            base_conf = 0.85
        
        # Add rescue bonus
        final_conf = base_conf + rescue_info.get("confidence_bonus", 0.0)
        
        # Location boost
        if location and location.get("canonical"):
            if event_type != "‡§Ö‡§®‡•ç‡§Ø":
                final_conf += 0.08
        
        # Length validation
        is_substantial = text_len > 20
        
        # High Precision Boost
        if event_type in HIGH_PRECISION_EVENTS and is_substantial:
            if rescue_info.get("is_rescued") or final_conf > 0.7:
                final_conf = max(final_conf, 0.92)
        
        # Cap at 0.99
        return round(min(final_conf, 0.99), 3)

    def determine_review_status(self, confidence: float) -> Tuple[str, bool]:
        if confidence >= CONFIDENCE_AUTO_APPROVE:
            return "auto_approved", False
        elif confidence < CONFIDENCE_NEEDS_REVIEW:
            return "pending", True
        else:
            return "pending", True

# ==========================================
# MAIN PARSER CLASS
# ==========================================

class GeminiParserV1:
    """
    Gemini Parser V1 - Consolidated single-file parser. 
    
    Features:
    - Hybrid location resolution (dictionary + FAISS semantic + geo-hierarchy)
    - Enhanced entity extraction with intra-tweet deduplication
    - Contextual rescue logic
    - Weighted confidence scoring
    """
    
    def __init__(self, enable_full_hierarchy=True,
                 enable_semantic=True, data_dir: Optional[Path] = None):
        print("Initializing Gemini Parser V1...")
        
        self.enable_full_hierarchy = enable_full_hierarchy
        
        # Initialize components
        self.location_resolver = HybridLocationResolver(
            enable_semantic=enable_semantic,
            data_dir=data_dir
        )
        self.entity_extractor = EntityExtractor()
        self.rescue_detector = RescueDetector()
        self.confidence_scorer = ConfidenceScorer()
        
        self.stats = {
            'total_tweets': 0,
            'processing_times': [],
            'faiss_queries': 0
        }
        
        print("‚úÖ Parser initialized")
    
    def parse_tweet(self, record: Dict[str, Any]) -> Dict[str, Any]:
        """
        Parse single tweet to v8 format.
        """
        start_time = time.time()
        
        # Extract text
        text = record.get("raw_text") or record.get("text") or ""
        
        # Get hints from older parser versions
        old_pd = (
            record.get("parsed_data_v7") or 
            record.get("parsed_data_v6") or
            record.get("parsed_data_v5") or 
            {}
        )
        
        # Extract entities (already deduplicated within each extractor)
        schemes, _ = self.entity_extractor.extract_schemes(text)
        target_groups, _ = self.entity_extractor.extract_target_groups(text)
        communities, _ = self.entity_extractor.extract_communities(text)
        orgs, _ = self.entity_extractor.extract_orgs(text)
        word_buckets, _ = self.entity_extractor.extract_word_buckets(text)
        
        # Entity Resolution: Deduplicate people (from old_pd)
        people_mentioned = list(set(old_pd.get("people_mentioned", [])))
        people_canonical = list(set(old_pd.get("people_canonical", [])))
        
        # Hybrid location resolution
        location, loc_conf = self.location_resolver.resolve(text, old_pd.get("location"))
        
        # Base event detection from keyword clusters
        base_event, base_conf = self._detect_base_event(text)
        
        # Rescue logic
        rescue_info = self.rescue_detector.rescue(text, base_event, location, schemes)
        final_event = rescue_info.get("event_type", base_event)
        content_mode = rescue_info.get("content_mode", "‡§°‡§ø‡§ú‡§ø‡§ü‡§≤ / ‡§∏‡•ã‡§∂‡§≤-media ‡§™‡•ã‡§∏‡•ç‡§ü")
        
        # Confidence scoring
        final_confidence = self.confidence_scorer.compute(
            event_type=final_event,
            location=location,
            schemes=schemes,
            target_groups=target_groups,
            communities=communities,
            orgs=orgs,
            people=people_mentioned,
            text_len=len(text),
            rescue_info=rescue_info
        )
        
        # Review status
        review_status, needs_review = self.confidence_scorer.determine_review_status(
            final_confidence
        )
        
        # Build parsed_data_v8
        parsed_data_v8 = {
            # Core fields (backward compatible)
            "event_type": final_event,
            "event_type_secondary": [],
            "event_date": record.get("created_at", "")[:10] if record.get("created_at") else None,
            "location": location,
            "people_mentioned": people_mentioned,
            "people_canonical": people_canonical,
            
            # Entity fields
            "schemes_mentioned": schemes,
            "word_buckets": word_buckets,
            "target_groups": target_groups,
            "communities": communities,
            "organizations": orgs,
            
            # Location hierarchy
            "hierarchy_path": (location or {}).get("hierarchy_path", []),
            "visit_count": (location or {}).get("visit_count", 1),
            "vector_embedding_id": (location or {}).get("vector_embedding_id"),
            
            # Confidence & review
            "confidence": final_confidence,
            "review_status": review_status,
            "needs_review": needs_review,
            
            # Content mode
            "content_mode": content_mode,
            
            # Rescue metadata
            "is_other_original": (base_event == "‡§Ö‡§®‡•ç‡§Ø"),
            "is_rescued_other": rescue_info.get("is_rescued", False),
            "rescue_tag": rescue_info.get("rescue_tag"),
            "rescue_confidence_bonus": rescue_info.get("confidence_bonus", 0.0),
            
            # V8 specific fields
            "semantic_location_used": (location or {}).get("source") == "semantic",
            "location_type": (location or {}).get("location_type")  # rural/urban/district
        }
        
        processing_time = int((time.time() - start_time) * 1000)
        self.stats['processing_times'].append(processing_time)
        
        if processing_time > PROCESSING_TIMEOUT_MS:
            print(f"‚ö†Ô∏è  Slow processing: {processing_time}ms for tweet {record.get('tweet_id')}")
        
        return {
            **record,
            "parsed_data_v8": parsed_data_v8,
            "metadata_v8": {
                "model": "gemini-parser-v1",
                "processing_time_ms": processing_time,
                "version": "1.0.0"
            }
        }
    
    def _detect_base_event(self, text: str) -> tuple:
        """
        Detect base event type from keyword clusters.
        """
        text_l = text.lower()
        
        for keywords, label in EVENT_KEYWORD_CLUSTERS:
            if any(k.lower() in text_l for k in keywords):
                return label, 0.85
        
        return "‡§Ö‡§®‡•ç‡§Ø", 0.4
    
    def parse_file(self, input_path: Path, output_dir: Path):
        """
        Parse entire JSONL file.
        """
        output_dir.mkdir(parents=True, exist_ok=True)
        
        print(f"\nüöÄ Parsing: {input_path}")
        print(f"   Output: {output_dir}/")
        
        tweets = []
        with input_path.open("r", encoding=OUTPUT_ENCODING) as f:
            for line_num, line in enumerate(f, 1):
                if not line.strip():
                    continue
                
                try:
                    record = json.loads(line)
                    parsed = self.parse_tweet(record)
                    tweets.append(parsed)
                    
                    if line_num % 100 == 0:
                        print(f"   Processed {line_num} tweets...")
                except Exception as e:
                    print(f"   Error on line {line_num}: {e}")
                    continue
        
        self.stats['total_tweets'] = len(tweets)
        
        # Write all tweets (No tweet-level deduplication as per user request)
        output_file = output_dir / "parsed_tweets_v8.jsonl"
        
        with output_file.open("w", encoding=OUTPUT_ENCODING) as f:
            for tweet in tweets:
                f.write(json.dumps(tweet, ensure_ascii=False) + "\n")
        
        # Collect stats
        all_stats = self._collect_stats(tweets, None)
        stats_file = output_dir / "parsed_tweets_v8_stats.json"
        
        with stats_file.open("w", encoding=OUTPUT_ENCODING) as f:
            json.dump(all_stats, f, ensure_ascii=False, indent=2)
        
        print(f"\n‚úÖ Parsing complete!")
        print(f"   Total: {len(tweets)} tweets")
        print(f"   Output: {output_file}")
    
    def _collect_stats(self, tweets, dup_stats):
        """Collect comprehensive parsing statistics"""
        event_counts = {}
        location_types = {'rural': 0, 'urban': 0, 'district': 0, 'none': 0}
        confidence_bins = {'high': 0, 'medium': 0, 'low': 0}
        
        for tweet in tweets:
            pd = tweet.get("parsed_data_v8", {})
            
            # Event counts
            event = pd.get("event_type", "‡§Ö‡§®‡•ç‡§Ø")
            event_counts[event] = event_counts.get(event, 0) + 1
            
            # Location types
            loc_type = pd.get("location_type")
            if loc_type in location_types:
                location_types[loc_type] += 1
            else:
                location_types['none'] += 1
            
            # Confidence bins
            conf = pd.get("confidence", 0)
            if conf >= 0.9:
                confidence_bins['high'] += 1
            elif conf >= 0.75:
                confidence_bins['medium'] += 1
            else:
                confidence_bins['low'] += 1
        
        stats = {
            "parser_version": "1.0.0",
            "total_tweets": len(tweets),
            "event_distribution": event_counts,
            "location_type_distribution": location_types,
            "confidence_distribution": confidence_bins,
            "avg_processing_time_ms": (
                sum(self.stats['processing_times']) / len(self.stats['processing_times'])
                if self.stats['processing_times'] else 0
            ),
            "location_resolver_stats": self.location_resolver.get_stats()
        }
        
        if dup_stats:
            stats["deduplication"] = dup_stats
        
        return stats

def main():
    """CLI entry point"""
    parser = argparse.ArgumentParser(
        description="Gemini Parser V1 - Hybrid Rule-Based + Semantic + Entity Resolution",
        formatter_class=argparse.RawDescriptionHelpFormatter
    )
    
    parser.add_argument("input", type=Path, help="Input JSONL file")
    parser.add_argument("output_dir", type=Path, help="Output directory")
    parser.add_argument("--no-hierarchy", action="store_true",
                       help="Disable full hierarchy resolution (faster)")
    parser.add_argument("--no-semantic", action="store_true",
                       help="Disable FAISS semantic search (faster)")
    parser.add_argument("--data-dir", type=Path,
                       help="Custom data directory path")
    
    args = parser.parse_args()
    
    # Initialize parser
    gemini_parser = GeminiParserV1(
        enable_full_hierarchy=not args.no_hierarchy,
        enable_semantic=not args.no_semantic,
        data_dir=args.data_dir
    )
    
    # Parse file
    gemini_parser.parse_file(args.input, args.output_dir)

if __name__ == "__main__":
    main()
