import json
import re
import time
import hashlib
import argparse
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Any, Set
from collections import Counter, defaultdict

# ==========================================
# CONFIGURATION & CONSTANTS
# ==========================================

VERSION = "4.0.0"
OUTPUT_ENCODING = "utf-8"
PROCESSING_TIMEOUT_MS = 200
CONFIDENCE_AUTO_APPROVE = 0.85
CONFIDENCE_NEEDS_REVIEW = 0.40
FINGERPRINT_SIMILARITY_THRESHOLD = 0.85

# Paths (V2 Dependency)
DATA_DIR = Path("data")
GEO_FILE = DATA_DIR / "chhattisgarh_complete_geography.json"
URBAN_FILE = DATA_DIR / "datasets/chhattisgarh_urban.ndjson"
CONSTITUENCIES_FILE = DATA_DIR / "constituencies.json"  # Optional, can fallback

# ==========================================
# EMBEDDED TAXONOMIES (From V3)
# ==========================================

EVENT_KEYWORD_CLUSTERS_WEIGHTED = [
    {
        "event_type": "‡§ú‡§®‡§∏‡§Æ‡•ç‡§™‡§∞‡•ç‡§ï / ‡§ú‡§®‡§¶‡§∞‡•ç‡§∂‡§®",
        "weight": 1.2,
        "tier_1": ["‡§ú‡§®‡§∏‡§Æ‡•ç‡§™‡§∞‡•ç‡§ï", "‡§ú‡§®‡§¶‡§∞‡•ç‡§∂‡§®", "‡§Æ‡•Å‡§≤‡§æ‡§ï‡§æ‡§§", "‡§≠‡•á‡§Ç‡§ü", "‡§¶‡•å‡§∞‡§æ", "‡§™‡•ç‡§∞‡§µ‡§æ‡§∏", "‡§Ü‡§ó‡§Æ‡§®"],
        "tier_2": ["‡§∏‡•ç‡§µ‡§æ‡§ó‡§§", "‡§Ö‡§≠‡§ø‡§®‡§Ç‡§¶‡§®", "‡§ö‡§∞‡•ç‡§ö‡§æ", "‡§∏‡§Ç‡§µ‡§æ‡§¶"],
        "tier_3": ["‡§∂‡§æ‡§Æ‡§ø‡§≤", "‡§â‡§™‡§∏‡•ç‡§•‡§ø‡§§", "‡§ï‡§æ‡§∞‡•ç‡§Ø‡§ï‡•ç‡§∞‡§Æ"]
    },
    {
        "event_type": "‡§∞‡§æ‡§ú‡§®‡•Ä‡§§‡§ø‡§ï ‡§µ‡§ï‡•ç‡§§‡§µ‡•ç‡§Ø",
        "weight": 1.0,
        "tier_1": ["‡§™‡•ç‡§∞‡•á‡§∏ ‡§µ‡§æ‡§∞‡•ç‡§§‡§æ", "‡§¨‡§Ø‡§æ‡§®", "‡§∏‡§Ç‡§¨‡•ã‡§ß‡§®", "‡§Ü‡§∞‡•ã‡§™", "‡§™‡•ç‡§∞‡§§‡•ç‡§Ø‡§æ‡§∞‡•ã‡§™", "‡§ï‡§æ‡§Ç‡§ó‡•ç‡§∞‡•á‡§∏", "‡§≠‡§æ‡§ú‡§™‡§æ"],
        "tier_2": ["‡§∏‡§∞‡§ï‡§æ‡§∞", "‡§µ‡§ø‡§™‡§ï‡•ç‡§∑", "‡§ò‡•ã‡§ü‡§æ‡§≤‡§æ", "‡§≠‡•ç‡§∞‡§∑‡•ç‡§ü‡§æ‡§ö‡§æ‡§∞", "‡§µ‡§ø‡§ï‡§æ‡§∏"],
        "tier_3": ["‡§ü‡•ç‡§µ‡•Ä‡§ü", "‡§Æ‡•Ä‡§°‡§ø‡§Ø‡§æ", "‡§™‡§§‡•ç‡§∞‡§ï‡§æ‡§∞"]
    },
    {
        "event_type": "‡§ß‡§æ‡§∞‡•ç‡§Æ‡§ø‡§ï / ‡§∏‡§æ‡§Ç‡§∏‡•ç‡§ï‡•É‡§§‡§ø‡§ï ‡§ï‡§æ‡§∞‡•ç‡§Ø‡§ï‡•ç‡§∞‡§Æ",
        "weight": 1.1,
        "tier_1": ["‡§™‡•Ç‡§ú‡§æ", "‡§Ö‡§∞‡•ç‡§ö‡§®‡§æ", "‡§¶‡§∞‡•ç‡§∂‡§®", "‡§Ü‡§∞‡§§‡•Ä", "‡§Æ‡§Ç‡§¶‡§ø‡§∞", "‡§Æ‡§π‡•ã‡§§‡•ç‡§∏‡§µ", "‡§ú‡§Ø‡§Ç‡§§‡•Ä", "‡§™‡§∞‡•ç‡§µ"],
        "tier_2": ["‡§™‡•Å‡§£‡•ç‡§Ø‡§§‡§ø‡§•‡§ø", "‡§∂‡•ç‡§∞‡§¶‡•ç‡§ß‡§æ‡§Ç‡§ú‡§≤‡§ø", "‡§®‡§Æ‡§®", "‡§∏‡•ç‡§Æ‡§∞‡§£"],
        "tier_3": ["‡§Ü‡§Ø‡•ã‡§ú‡§®", "‡§∏‡§Æ‡§æ‡§∞‡•ã‡§π", "‡§â‡§§‡•ç‡§∏‡§µ"]
    },
    {
        "event_type": "‡§Ü‡§Ç‡§§‡§∞‡§ø‡§ï ‡§∏‡•Å‡§∞‡§ï‡•ç‡§∑‡§æ / ‡§™‡•Å‡§≤‡§ø‡§∏",
        "weight": 1.5,
        "tier_1": ["‡§®‡§ï‡•ç‡§∏‡§≤", "‡§Æ‡§æ‡§ì‡§µ‡§æ‡§¶‡•Ä", "‡§∂‡§π‡•Ä‡§¶", "‡§Æ‡•Å‡§†‡§≠‡•á‡§°‡§º", "‡§ó‡§ø‡§∞‡§´‡•ç‡§§‡§æ‡§∞", "‡§Ü‡§§‡•ç‡§Æ‡§∏‡§Æ‡§∞‡•ç‡§™‡§£", "‡§¨‡§∞‡§æ‡§Æ‡§¶"],
        "tier_2": ["‡§™‡•Å‡§≤‡§ø‡§∏", "‡§ú‡§µ‡§æ‡§®", "‡§∏‡•Å‡§∞‡§ï‡•ç‡§∑‡§æ", "‡§¨‡§≤", "‡§Ü‡§à‡§à‡§°‡•Ä"],
        "tier_3": ["‡§∏‡§∞‡•ç‡§ö‡§ø‡§Ç‡§ó", "‡§Ö‡§≠‡§ø‡§Ø‡§æ‡§®", "‡§•‡§æ‡§®‡§æ"]
    },
    {
        "event_type": "‡§ñ‡•á‡§≤ / ‡§ó‡•å‡§∞‡§µ",
        "weight": 1.4,
        "tier_1": ["‡§™‡§¶‡§ï", "‡§Æ‡•á‡§°‡§≤", "‡§µ‡§ø‡§ú‡•á‡§§‡§æ", "‡§ö‡•à‡§Ç‡§™‡§ø‡§Ø‡§®", "‡§ñ‡•á‡§≤", "‡§ñ‡§ø‡§≤‡§æ‡§°‡§º‡•Ä", "‡§ú‡•Ä‡§§"],
        "tier_2": ["‡§™‡•ç‡§∞‡§§‡§ø‡§Ø‡•ã‡§ó‡§ø‡§§‡§æ", "‡§ü‡•Ç‡§∞‡•ç‡§®‡§æ‡§Æ‡•á‡§Ç‡§ü", "‡§Ü‡§Ø‡•ã‡§ú‡§®"],
        "tier_3": ["‡§¨‡§ß‡§æ‡§à", "‡§∂‡•Å‡§≠‡§ï‡§æ‡§Æ‡§®‡§æ‡§è‡§Ç"]
    },
    {
        "event_type": "‡§Ü‡§™‡§¶‡§æ / ‡§¶‡•Å‡§∞‡•ç‡§ò‡§ü‡§®‡§æ",
        "weight": 1.3,
        "tier_1": ["‡§π‡§æ‡§¶‡§∏‡§æ", "‡§¶‡•Å‡§∞‡•ç‡§ò‡§ü‡§®‡§æ", "‡§Æ‡•å‡§§", "‡§ò‡§æ‡§Ø‡§≤", "‡§Ü‡§ó", "‡§¨‡§æ‡§¢‡§º", "‡§∏‡•Ç‡§ñ‡§æ"],
        "tier_2": ["‡§∞‡§æ‡§π‡§§", "‡§¨‡§ö‡§æ‡§µ", "‡§Æ‡•Å‡§Ü‡§µ‡§ú‡§æ"],
        "tier_3": ["‡§®‡•Å‡§ï‡§∏‡§æ‡§®", "‡§ï‡•ç‡§∑‡§§‡§ø"]
    },
    {
        "event_type": "‡§¨‡•à‡§†‡§ï",
        "weight": 1.0,
        "tier_1": ["‡§¨‡•à‡§†‡§ï", "‡§∏‡§Æ‡•Ä‡§ï‡•ç‡§∑‡§æ", "‡§Æ‡•Ä‡§ü‡§ø‡§Ç‡§ó"],
        "tier_2": ["‡§Ö‡§ß‡§ø‡§ï‡§æ‡§∞‡•Ä", "‡§®‡§ø‡§∞‡•ç‡§¶‡•á‡§∂", "‡§ö‡§∞‡•ç‡§ö‡§æ"],
        "tier_3": ["‡§Ü‡§Ø‡•ã‡§ú‡§ø‡§§", "‡§∏‡§Ç‡§™‡§®‡•ç‡§®"]
    },
    {
        "event_type": "‡§â‡§¶‡•ç‡§ò‡§æ‡§ü‡§®",
        "weight": 1.2,
        "tier_1": ["‡§â‡§¶‡•ç‡§ò‡§æ‡§ü‡§®", "‡§≤‡•ã‡§ï‡§æ‡§∞‡•ç‡§™‡§£", "‡§∂‡§ø‡§≤‡§æ‡§®‡•ç‡§Ø‡§æ‡§∏", "‡§≠‡•Ç‡§Æ‡§ø‡§™‡•Ç‡§ú‡§®"],
        "tier_2": ["‡§∏‡•å‡§ó‡§æ‡§§", "‡§∂‡•Å‡§≠‡§æ‡§∞‡§Ç‡§≠"],
        "tier_3": ["‡§µ‡§ø‡§ï‡§æ‡§∏ ‡§ï‡§æ‡§∞‡•ç‡§Ø"]
    },
    {
        "event_type": "‡§Ø‡•ã‡§ú‡§®‡§æ ‡§ò‡•ã‡§∑‡§£‡§æ",
        "weight": 1.2,
        "tier_1": ["‡§Ø‡•ã‡§ú‡§®‡§æ", "‡§ò‡•ã‡§∑‡§£‡§æ", "‡§≤‡§æ‡§ó‡•Ç", "‡§∂‡•Å‡§≠‡§æ‡§∞‡§Ç‡§≠"],
        "tier_2": ["‡§≤‡§æ‡§≠‡§æ‡§∞‡•ç‡§•‡•Ä", "‡§µ‡§ø‡§§‡§∞‡§£", "‡§ñ‡§æ‡§§‡§æ"],
        "tier_3": ["‡§∏‡§∞‡§ï‡§æ‡§∞", "‡§™‡§π‡§≤"]
    },
    {
        "event_type": "‡§∂‡•Å‡§≠‡§ï‡§æ‡§Æ‡§®‡§æ / ‡§¨‡§ß‡§æ‡§à",
        "weight": 0.8,
        "tier_1": ["‡§¨‡§ß‡§æ‡§à", "‡§∂‡•Å‡§≠‡§ï‡§æ‡§Æ‡§®‡§æ‡§è‡§Ç", "‡§π‡§æ‡§∞‡•ç‡§¶‡§ø‡§ï"],
        "tier_2": ["‡§™‡•ç‡§∞‡§∏‡§®‡•ç‡§®‡§§‡§æ", "‡§ñ‡•Å‡§∂‡•Ä"],
        "tier_3": ["‡§Æ‡§Ç‡§ó‡§≤‡§Æ‡§Ø"]
    },
    {
        "event_type": "‡§∂‡•ã‡§ï ‡§∏‡§Ç‡§¶‡•á‡§∂",
        "weight": 1.3,
        "tier_1": ["‡§®‡§ø‡§ß‡§®", "‡§∂‡•ã‡§ï", "‡§¶‡•Å‡§ñ‡§¶", "‡§∂‡•ç‡§∞‡§¶‡•ç‡§ß‡§æ‡§Ç‡§ú‡§≤‡§ø", "‡§à‡§∂‡•ç‡§µ‡§∞"],
        "tier_2": ["‡§Ü‡§§‡•ç‡§Æ‡§æ", "‡§∂‡§æ‡§Ç‡§§‡§ø", "‡§∏‡§Ç‡§µ‡•á‡§¶‡§®‡§æ"],
        "tier_3": ["‡§™‡§∞‡§ø‡§µ‡§æ‡§∞"]
    },
    {
        "event_type": "‡§ú‡§®‡•ç‡§Æ‡§¶‡§ø‡§® ‡§∂‡•Å‡§≠‡§ï‡§æ‡§Æ‡§®‡§æ",
        "weight": 1.2,
        "tier_1": ["‡§ú‡§®‡•ç‡§Æ‡§¶‡§ø‡§®", "‡§Ö‡§µ‡§§‡§∞‡§£ ‡§¶‡§ø‡§µ‡§∏", "‡§¶‡•Ä‡§∞‡•ç‡§ò‡§æ‡§Ø‡•Å"],
        "tier_2": ["‡§∏‡•ç‡§µ‡§∏‡•ç‡§•", "‡§ú‡•Ä‡§µ‡§®"],
        "tier_3": ["‡§ï‡§æ‡§Æ‡§®‡§æ"]
    },
    {
        "event_type": "‡§∏‡§Æ‡•ç‡§Æ‡§æ‡§® / Felicitation",
        "weight": 1.1,
        "tier_1": ["‡§∏‡§Æ‡•ç‡§Æ‡§æ‡§®", "‡§™‡•Å‡§∞‡§∏‡•ç‡§ï‡§æ‡§∞", "‡§∏‡§Æ‡•ç‡§Æ‡§æ‡§®‡§ø‡§§", "‡§™‡•ç‡§∞‡§∂‡§∏‡•ç‡§§‡§ø"],
        "tier_2": ["‡§ó‡•å‡§∞‡§µ", "‡§â‡§™‡§≤‡§¨‡•ç‡§ß‡§ø"],
        "tier_3": ["‡§∏‡§Æ‡§æ‡§∞‡•ã‡§π"]
    },
    {
        "event_type": "‡§®‡§ø‡§∞‡•Ä‡§ï‡•ç‡§∑‡§£",
        "weight": 1.1,
        "tier_1": ["‡§®‡§ø‡§∞‡•Ä‡§ï‡•ç‡§∑‡§£", "‡§ú‡§æ‡§Ø‡§ú‡§æ", "‡§Ö‡§µ‡§≤‡•ã‡§ï‡§®"],
        "tier_2": ["‡§∏‡•ç‡§•‡§≤", "‡§ï‡§æ‡§∞‡•ç‡§Ø"],
        "tier_3": ["‡§≠‡•ç‡§∞‡§Æ‡§£"]
    },
    {
        "event_type": "‡§™‡•ç‡§∞‡§∂‡§æ‡§∏‡§®‡§ø‡§ï ‡§∏‡§Æ‡•Ä‡§ï‡•ç‡§∑‡§æ ‡§¨‡•à‡§†‡§ï",
        "weight": 1.2,
        "tier_1": ["‡§ï‡§≤‡•á‡§ï‡•ç‡§ü‡§∞", "‡§è‡§∏‡§™‡•Ä", "‡§ï‡§Æ‡§ø‡§∂‡•ç‡§®‡§∞", "‡§∏‡§Æ‡•Ä‡§ï‡•ç‡§∑‡§æ ‡§¨‡•à‡§†‡§ï", "‡§ü‡•Ä‡§è‡§≤"],
        "tier_2": ["‡§®‡§ø‡§∞‡•ç‡§¶‡•á‡§∂", "‡§™‡§æ‡§≤‡§®", "‡§∞‡§ø‡§™‡•ã‡§∞‡•ç‡§ü"],
        "tier_3": ["‡§µ‡§ø‡§≠‡§æ‡§ó"]
    },
    {
        "event_type": "‡§∞‡•à‡§≤‡•Ä",
        "weight": 1.1,
        "tier_1": ["‡§∞‡•à‡§≤‡•Ä", "‡§ú‡•Å‡§≤‡•Ç‡§∏", "‡§™‡•ç‡§∞‡§¶‡§∞‡•ç‡§∂‡§®", "‡§∏‡§≠‡§æ"],
        "tier_2": ["‡§®‡§æ‡§∞‡•á‡§¨‡§æ‡§ú‡•Ä", "‡§≠‡•Ä‡§°‡§º"],
        "tier_3": ["‡§∂‡§æ‡§Æ‡§ø‡§≤"]
    },
    {
        "event_type": "‡§ö‡•Å‡§®‡§æ‡§µ ‡§™‡•ç‡§∞‡§ö‡§æ‡§∞",
        "weight": 1.2,
        "tier_1": ["‡§™‡•ç‡§∞‡§ö‡§æ‡§∞", "‡§ú‡§®‡§∏‡§Ç‡§™‡§∞‡•ç‡§ï", "‡§µ‡•ã‡§ü", "‡§Æ‡§§‡§¶‡§æ‡§®"],
        "tier_2": ["‡§™‡•ç‡§∞‡§§‡•ç‡§Ø‡§æ‡§∂‡•Ä", "‡§â‡§Æ‡•ç‡§Æ‡•Ä‡§¶‡§µ‡§æ‡§∞"],
        "tier_3": ["‡§∏‡§Æ‡§∞‡•ç‡§•‡§®"]
    }
]

SCHEME_PATTERNS = {
    r"‡§™‡•Ä‡§è‡§Æ\s*‡§Ü‡§µ‡§æ‡§∏": "‡§™‡•ç‡§∞‡§ß‡§æ‡§®‡§Æ‡§Ç‡§§‡•ç‡§∞‡•Ä ‡§Ü‡§µ‡§æ‡§∏ ‡§Ø‡•ã‡§ú‡§®‡§æ",
    r"PMAY": "‡§™‡•ç‡§∞‡§ß‡§æ‡§®‡§Æ‡§Ç‡§§‡•ç‡§∞‡•Ä ‡§Ü‡§µ‡§æ‡§∏ ‡§Ø‡•ã‡§ú‡§®‡§æ",
    r"‡§Æ‡§π‡§§‡§æ‡§∞‡•Ä\s*‡§µ‡§Ç‡§¶‡§®": "‡§Æ‡§π‡§§‡§æ‡§∞‡•Ä ‡§µ‡§Ç‡§¶‡§® ‡§Ø‡•ã‡§ú‡§®‡§æ",
    r"‡§ï‡§ø‡§∏‡§æ‡§®\s*‡§®‡•ç‡§Ø‡§æ‡§Ø": "‡§∞‡§æ‡§ú‡•Ä‡§µ ‡§ó‡§æ‡§Ç‡§ß‡•Ä ‡§ï‡§ø‡§∏‡§æ‡§® ‡§®‡•ç‡§Ø‡§æ‡§Ø ‡§Ø‡•ã‡§ú‡§®‡§æ",
    r"‡§ó‡•ã‡§ß‡§®\s*‡§®‡•ç‡§Ø‡§æ‡§Ø": "‡§ó‡•ã‡§ß‡§® ‡§®‡•ç‡§Ø‡§æ‡§Ø ‡§Ø‡•ã‡§ú‡§®‡§æ",
    r"‡§Æ‡§®‡§∞‡•á‡§ó‡§æ": "‡§Æ‡§®‡§∞‡•á‡§ó‡§æ",
    r"MNREGA": "‡§Æ‡§®‡§∞‡•á‡§ó‡§æ",
    r"‡§Ü‡§Ø‡•Å‡§∑‡•ç‡§Æ‡§æ‡§®": "‡§Ü‡§Ø‡•Å‡§∑‡•ç‡§Æ‡§æ‡§® ‡§≠‡§æ‡§∞‡§§",
    r"‡§â‡§ú‡•ç‡§ú‡•ç‡§µ‡§≤‡§æ": "‡§™‡•ç‡§∞‡§ß‡§æ‡§®‡§Æ‡§Ç‡§§‡•ç‡§∞‡•Ä ‡§â‡§ú‡•ç‡§ú‡•ç‡§µ‡§≤‡§æ ‡§Ø‡•ã‡§ú‡§®‡§æ",
    r"‡§ú‡§®\s*‡§ß‡§®": "‡§™‡•ç‡§∞‡§ß‡§æ‡§®‡§Æ‡§Ç‡§§‡•ç‡§∞‡•Ä ‡§ú‡§® ‡§ß‡§® ‡§Ø‡•ã‡§ú‡§®‡§æ",
    r"‡§∏‡•ç‡§µ‡§ö‡•ç‡§õ\s*‡§≠‡§æ‡§∞‡§§": "‡§∏‡•ç‡§µ‡§ö‡•ç‡§õ ‡§≠‡§æ‡§∞‡§§ ‡§Æ‡§ø‡§∂‡§®",
    r"‡§ú‡§≤\s*‡§ú‡•Ä‡§µ‡§®": "‡§ú‡§≤ ‡§ú‡•Ä‡§µ‡§® ‡§Æ‡§ø‡§∂‡§®",
    r"GST": "GST"
}

WORD_BUCKETS = {
    "agriculture": ["‡§ï‡§ø‡§∏‡§æ‡§®", "‡§ï‡•É‡§∑‡§ø", "‡§ß‡§æ‡§®", "‡§´‡§∏‡§≤", "‡§¨‡•Ä‡§ú", "‡§ñ‡§æ‡§¶", "‡§∏‡§ø‡§Ç‡§ö‡§æ‡§à", "‡§¨‡•ã‡§®‡§∏", "‡§∏‡§Æ‡§∞‡•ç‡§•‡§® ‡§Æ‡•Ç‡§≤‡•ç‡§Ø", "MSP"],
    "education": ["‡§∂‡§ø‡§ï‡•ç‡§∑‡§æ", "‡§∏‡•ç‡§ï‡•Ç‡§≤", "‡§ï‡•â‡§≤‡•á‡§ú", "‡§µ‡§ø‡§¶‡•ç‡§Ø‡§æ‡§∞‡•ç‡§•‡•Ä", "‡§õ‡§æ‡§§‡•ç‡§∞", "‡§∂‡§ø‡§ï‡•ç‡§∑‡§ï", "‡§≠‡§∞‡•ç‡§§‡•Ä", "‡§™‡§∞‡•Ä‡§ï‡•ç‡§∑‡§æ", "‡§™‡§∞‡§ø‡§£‡§æ‡§Æ"],
    "health": ["‡§∏‡•ç‡§µ‡§æ‡§∏‡•ç‡§•‡•ç‡§Ø", "‡§Ö‡§∏‡•ç‡§™‡§§‡§æ‡§≤", "‡§á‡§≤‡§æ‡§ú", "‡§°‡•â‡§ï‡•ç‡§ü‡§∞", "‡§¶‡§µ‡§æ", "‡§Æ‡•á‡§°‡§ø‡§ï‡§≤", "‡§è‡§Æ‡•ç‡§¨‡•Å‡§≤‡•á‡§Ç‡§∏", "‡§ü‡•Ä‡§ï‡§æ‡§ï‡§∞‡§£"],
    "infrastructure": ["‡§∏‡§°‡§º‡§ï", "‡§¨‡§ø‡§ú‡§≤‡•Ä", "‡§™‡§æ‡§®‡•Ä", "‡§®‡§ø‡§∞‡•ç‡§Æ‡§æ‡§£", "‡§™‡•Å‡§≤", "‡§≠‡§µ‡§®", "‡§∞‡•á‡§≤‡§µ‡•á", "‡§ï‡§®‡•á‡§ï‡•ç‡§ü‡§ø‡§µ‡§ø‡§ü‡•Ä"],
    "welfare": ["‡§∞‡§æ‡§∂‡§®", "‡§™‡•á‡§Ç‡§∂‡§®", "‡§Ü‡§µ‡§æ‡§∏", "‡§ó‡§∞‡•Ä‡§¨", "‡§ï‡§≤‡•ç‡§Ø‡§æ‡§£", "‡§∏‡§π‡§æ‡§Ø‡§§‡§æ", "‡§Ö‡§®‡•Å‡§¶‡§æ‡§®"],
    "governance": ["‡§™‡•ç‡§∞‡§∂‡§æ‡§∏‡§®", "‡§Ø‡•ã‡§ú‡§®‡§æ", "‡§¨‡•à‡§†‡§ï", "‡§∏‡§Æ‡•Ä‡§ï‡•ç‡§∑‡§æ", "‡§®‡§ø‡§∞‡•Ä‡§ï‡•ç‡§∑‡§£", "‡§â‡§¶‡•ç‡§ò‡§æ‡§ü‡§®", "‡§≤‡•ã‡§ï‡§æ‡§∞‡•ç‡§™‡§£"],
    "security": ["‡§™‡•Å‡§≤‡§ø‡§∏", "‡§®‡§ï‡•ç‡§∏‡§≤", "‡§∏‡•Å‡§∞‡§ï‡•ç‡§∑‡§æ", "‡§ï‡§æ‡§®‡•Ç‡§®", "‡§Ö‡§™‡§∞‡§æ‡§ß", "‡§ó‡§ø‡§∞‡§´‡•ç‡§§‡§æ‡§∞", "‡§ú‡§µ‡§æ‡§®"],
    "culture": ["‡§∏‡§Ç‡§∏‡•ç‡§ï‡•É‡§§‡§ø", "‡§§‡•ç‡§Ø‡•ã‡§π‡§æ‡§∞", "‡§™‡§∞‡§Ç‡§™‡§∞‡§æ", "‡§Æ‡•á‡§≤‡§æ", "‡§Æ‡§π‡•ã‡§§‡•ç‡§∏‡§µ", "‡§ï‡§≤‡§æ", "‡§™‡§∞‡•ç‡§Ø‡§ü‡§®"],
    "employment": ["‡§∞‡•ã‡§ú‡§ó‡§æ‡§∞", "‡§®‡•å‡§ï‡§∞‡•Ä", "‡§≠‡§∞‡•ç‡§§‡•Ä", "‡§∏‡•ç‡§µ‡§∞‡•ã‡§ú‡§ó‡§æ‡§∞", "‡§ï‡•å‡§∂‡§≤", "‡§™‡•ç‡§∞‡§∂‡§ø‡§ï‡•ç‡§∑‡§£"],
    "development": ["‡§µ‡§ø‡§ï‡§æ‡§∏", "‡§™‡•ç‡§∞‡§ó‡§§‡§ø", "‡§∏‡•å‡§ó‡§æ‡§§", "‡§Ü‡§ß‡§æ‡§∞‡§∂‡§ø‡§≤‡§æ", "‡§µ‡§ø‡§ï‡§∏‡§ø‡§§"]
}

COMMUNITIES = {
    "farmers": ["‡§ï‡§ø‡§∏‡§æ‡§®", "‡§ï‡•É‡§∑‡§ï", "‡§Ö‡§®‡•ç‡§®‡§¶‡§æ‡§§‡§æ"],
    "women": ["‡§Æ‡§π‡§ø‡§≤‡§æ", "‡§®‡§æ‡§∞‡•Ä", "‡§Æ‡§æ‡§§‡§æ", "‡§¨‡§π‡§®", "‡§¨‡•á‡§ü‡•Ä", "‡§∂‡§ï‡•ç‡§§‡§ø", "‡§Æ‡§π‡§§‡§æ‡§∞‡•Ä"],
    "youth": ["‡§Ø‡•Å‡§µ‡§æ", "‡§õ‡§æ‡§§‡•ç‡§∞", "‡§µ‡§ø‡§¶‡•ç‡§Ø‡§æ‡§∞‡•ç‡§•‡•Ä", "‡§¨‡•á‡§∞‡•ã‡§ú‡§ó‡§æ‡§∞"],
    "scheduled_tribes": ["‡§Ü‡§¶‡§ø‡§µ‡§æ‡§∏‡•Ä", "‡§µ‡§®‡§µ‡§æ‡§∏‡•Ä", "‡§ú‡§®‡§ú‡§æ‡§§‡§ø", "ST"],
    "scheduled_castes": ["‡§¶‡§≤‡§ø‡§§", "‡§Ö‡§®‡•Å‡§∏‡•Ç‡§ö‡§ø‡§§ ‡§ú‡§æ‡§§‡§ø", "SC"],
    "obc": ["‡§™‡§ø‡§õ‡§°‡§º‡§æ ‡§µ‡§∞‡•ç‡§ó", "OBC", "‡§∏‡§æ‡§π‡•Ç", "‡§ï‡•Å‡§∞‡•ç‡§Æ‡•Ä", "‡§Ø‡§æ‡§¶‡§µ"],
    "general": ["‡§∏‡§æ‡§Æ‡§æ‡§®‡•ç‡§Ø ‡§µ‡§∞‡•ç‡§ó", "‡§∏‡§µ‡§∞‡•ç‡§£"],
    "students": ["‡§õ‡§æ‡§§‡•ç‡§∞", "‡§õ‡§æ‡§§‡•ç‡§∞‡§æ‡§è‡§Ç", "‡§µ‡§ø‡§¶‡•ç‡§Ø‡§æ‡§∞‡•ç‡§•‡•Ä"]
}

ORGANIZATIONS = {
    "political": ["‡§≠‡§æ‡§ú‡§™‡§æ", "‡§ï‡§æ‡§Ç‡§ó‡•ç‡§∞‡•á‡§∏", "‡§¨‡•Ä‡§ú‡•á‡§™‡•Ä", "INC", "‡§Ü‡§™", "‡§¨‡§∏‡§™‡§æ", "‡§ú‡§ï‡§æ‡§Ç‡§õ"],
    "government": ["‡§∏‡§∞‡§ï‡§æ‡§∞", "‡§∂‡§æ‡§∏‡§®", "‡§™‡•ç‡§∞‡§∂‡§æ‡§∏‡§®", "‡§µ‡§ø‡§≠‡§æ‡§ó", "‡§Æ‡§Ç‡§§‡•ç‡§∞‡§æ‡§≤‡§Ø", "‡§Ü‡§Ø‡•ã‡§ó", "‡§®‡§ø‡§ó‡§Æ", "‡§Æ‡§Ç‡§°‡§≤"],
    "corporate": ["‡§Ö‡§°‡§æ‡§®‡•Ä", "‡§Ö‡§Ç‡§¨‡§æ‡§®‡•Ä", "‡§ü‡§æ‡§ü‡§æ", "‡§ú‡§ø‡§Ç‡§¶‡§≤", "‡§¨‡§æ‡§≤‡§ï‡•ã", "‡§è‡§®‡§ü‡•Ä‡§™‡•Ä‡§∏‡•Ä", "SECL"],
    "ngo": ["‡§∏‡§Æ‡§ø‡§§‡§ø", "‡§∏‡§Ç‡§ó‡§†‡§®", "‡§∏‡§Ç‡§ò", "‡§´‡§æ‡§â‡§Ç‡§°‡•á‡§∂‡§®", "‡§ü‡•ç‡§∞‡§∏‡•ç‡§ü", "‡§∏‡•á‡§µ‡§æ"]
}

CONSENSUS_WEIGHTS = {
    'keyword': 0.25,
    'semantic': 0.20,
    'hierarchy': 0.20,
    'rescue': 0.15,
    'dictionary': 0.10,
    'faiss_agreement': 0.10
}

# Manual mappings for villages where Hindi name is missing or in English script in dataset
MANUAL_VILLAGE_MAPPING = {
    "‡§∏‡§ø‡§≤‡•ã‡§§‡§∞‡§æ": "Siltara",
    "‡§ï‡•Å‡§ï‡•Å‡§∞‡•ç‡§¶‡§æ": "Kukurda",
    "‡§≤‡•à‡§≤‡•Ç‡§Ç‡§ó‡§æ": "Lailunga",
    "‡§§‡§Æ‡§®‡§æ‡§∞": "Tamnar",
    "‡§™‡§§‡•ç‡§•‡§≤‡§ó‡§æ‡§Ç‡§µ": "Pathalgaon",
    "‡§ß‡§∞‡§Æ‡§ú‡§Ø‡§ó‡§¢‡§º": "Dharamjaigarh",
    "‡§¨‡§∏‡•ç‡§§‡§∞": "Bastar",
    "‡§ï‡•ã‡§Ç‡§°‡§æ‡§ó‡§æ‡§Å‡§µ": "Kondagaon",
    "‡§ï‡•ã‡§Ç‡§ü‡§æ": "Konta",
    "‡§ó‡•Ä‡§¶‡§Æ": "Geedam",
    "‡§¨‡§∏‡§®‡§æ": "Basna",
    "‡§Æ‡§®‡•á‡§Ç‡§¶‡•ç‡§∞‡§ó‡§¢‡§º": "Manendragarh",
    "‡§®‡§æ‡§∞‡§æ‡§Ø‡§£‡§™‡•Å‡§∞": "Narayanpur",
    "‡§≠‡§æ‡§®‡•Å‡§™‡•ç‡§∞‡§§‡§æ‡§™‡§™‡•Å‡§∞": "Bhanupratappur",
    "‡§°‡•ã‡§Ç‡§ó‡§∞‡§ó‡§¢‡§º": "Dongargarh",
    "‡§ñ‡•à‡§∞‡§æ‡§ó‡§¢‡§º": "Khairagarh",
    "‡§™‡•á‡§Ç‡§°‡•ç‡§∞‡§æ": "Pendra",
    "‡§Æ‡§∞‡§µ‡§æ‡§π‡•Ä": "Marwahi",
    "‡§∏‡§æ‡§∞‡§Ç‡§ó‡§¢‡§º": "Sarangarh",
    "‡§¨‡§ø‡§≤‡§æ‡§à‡§ó‡§¢‡§º": "Bilaigarh",
    "‡§∂‡§ï‡•ç‡§§‡§ø": "Sakti",
    "‡§Æ‡•ã‡§π‡§≤‡§æ": "Mohla",
    "‡§Æ‡§æ‡§®‡§™‡•Å‡§∞": "Manpur",
    "‡§∞‡§æ‡§Ø‡§™‡•Å‡§∞": "Raipur"
}

# ==========================================
# UTILITY FUNCTIONS
# ==========================================

def load_json(path: Path) -> Dict:
    """Load JSON file safely."""
    try:
        with open(path, 'r', encoding='utf-8') as f:
            return json.load(f)
    except Exception as e:
        print(f"Error loading {path}: {e}")
        return {}

def load_ndjson(path: Path) -> List[Dict]:
    """Load NDJSON file safely."""
    data = []
    try:
        with open(path, 'r', encoding='utf-8') as f:
            for line in f:
                if line.strip():
                    data.append(json.loads(line))
    except Exception as e:
        print(f"Error loading {path}: {e}")
    return data

def clean_text(text: str) -> str:
    """Basic text cleaning."""
    if not text: return ""
    text = re.sub(r'http\S+', '', text)  # Remove URLs
    text = re.sub(r'\s+', ' ', text).strip()
    return text

def normalize_text(text: str) -> str:
    """Normalize text for matching."""
    text = clean_text(text).lower()
    text = re.sub(r'[^\w\s]', '', text)
    text = re.sub(r'\s+', ' ', text)
    return text.strip()

# ==========================================
# LOGIC CLASSES (From V3)
# ==========================================

class MultiSignalEventDetector:
    """Advanced event detection using multiple weighted signals."""
    def __init__(self):
        self.clusters = EVENT_KEYWORD_CLUSTERS_WEIGHTED
    
    def detect(self, text: str) -> Tuple[str, float, List[str]]:
        text_lower = text.lower()
        scores = {}
        for cluster in self.clusters:
            event_type = cluster["event_type"]
            score = 0.0
            tier_1_matches = sum(1 for kw in cluster["tier_1"] if kw in text_lower)
            if tier_1_matches > 0: score += min(tier_1_matches * 0.6, 1.0)
            tier_2_matches = sum(1 for kw in cluster["tier_2"] if kw in text_lower)
            if tier_2_matches > 0: score += min(tier_2_matches * 0.3, 0.6)
            tier_3_matches = sum(1 for kw in cluster["tier_3"] if kw in text_lower)
            if tier_3_matches > 0: score += min(tier_3_matches * 0.1, 0.3)
            score *= cluster["weight"]
            if score > 0: scores[event_type] = min(score, 1.0)
        
        if not scores: return "‡§Ö‡§®‡•ç‡§Ø", 0.3, []
        sorted_events = sorted(scores.items(), key=lambda x: x[1], reverse=True)
        primary_event, primary_score = sorted_events[0]
        secondary_events = [e for e, s in sorted_events[1:] if s > 0.4][:3]
        return primary_event, primary_score, secondary_events

class TieredRescueDetector:
    """Sophisticated rescue logic with tiered scoring."""
    RESCUE_TIERS = {
        'sports_critical': {'patterns': [r'(‡§Æ‡•à‡§ö|match)\s*(‡§ú‡•Ä‡§§|won|win)', r'(‡§™‡§¶‡§ï|medal)\s*(‡§ú‡•Ä‡§§|won)', r'(‡§ì‡§≤‡§Ç‡§™‡§ø‡§ï|olympic)', r'(‡§ö‡•à‡§Ç‡§™‡§ø‡§Ø‡§®|champion)'], 'weight': 1.0, 'confidence_boost': 0.25, 'target_event': '‡§ñ‡•á‡§≤ / ‡§ó‡•å‡§∞‡§µ'},
        'security_critical': {'patterns': [r'(‡§Æ‡§æ‡§ì‡§µ‡§æ‡§¶|naxal|‡§®‡§ï‡•ç‡§∏‡§≤)', r'(‡§∂‡§π‡•Ä‡§¶|martyr)', r'(‡§Ü‡§§‡•ç‡§Æ‡§∏‡§Æ‡§∞‡•ç‡§™‡§£|surrender)', r'(encounter|‡§Æ‡•Å‡§†‡§≠‡•á‡§°‡§º)'], 'weight': 1.0, 'confidence_boost': 0.25, 'target_event': '‡§Ü‡§Ç‡§§‡§∞‡§ø‡§ï ‡§∏‡•Å‡§∞‡§ï‡•ç‡§∑‡§æ / ‡§™‡•Å‡§≤‡§ø‡§∏'},
        'admin_high': {'patterns': [r'(‡§∏‡§Æ‡•Ä‡§ï‡•ç‡§∑‡§æ\s*‡§¨‡•à‡§†‡§ï)', r'(‡§ï‡§≤‡•á‡§ï‡•ç‡§ü‡§∞|collector)', r'(‡§Ö‡§ß‡§ø‡§ï‡§æ‡§∞‡§ø‡§Ø‡•ã‡§Ç\s*‡§ï‡•á\s*‡§∏‡§æ‡§•)'], 'weight': 0.8, 'confidence_boost': 0.18, 'target_event': '‡§™‡•ç‡§∞‡§∂‡§æ‡§∏‡§®‡§ø‡§ï ‡§∏‡§Æ‡•Ä‡§ï‡•ç‡§∑‡§æ ‡§¨‡•à‡§†‡§ï'},
        'political_high': {'patterns': [r'(‡§°‡§¨‡§≤\s*‡§á‡§Ç‡§ú‡§®)', r'(‡§≠‡•ç‡§∞‡§∑‡•ç‡§ü‡§æ‡§ö‡§æ‡§∞|corruption)', r'(‡§µ‡§ø‡§ï‡§∏‡§ø‡§§\s*‡§≠‡§æ‡§∞‡§§)', r'(‡§Æ‡•ã‡§¶‡•Ä\s*‡§ï‡•Ä\s*‡§ó‡§æ‡§∞‡§Ç‡§ü‡•Ä)'], 'weight': 0.8, 'confidence_boost': 0.18, 'target_event': '‡§∞‡§æ‡§ú‡§®‡•Ä‡§§‡§ø‡§ï ‡§µ‡§ï‡•ç‡§§‡§µ‡•ç‡§Ø'}
    }
    
    def rescue(self, text: str, current_event: str, location: Optional[Dict], schemes: List[str]) -> Dict[str, Any]:
        rescue_info = {"event_type": current_event, "content_mode": "‡§°‡§ø‡§ú‡§ø‡§ü‡§≤ / ‡§∏‡•ã‡§∂‡§≤-‡§Æ‡•Ä‡§°‡§ø‡§Ø‡§æ ‡§™‡•ã‡§∏‡•ç‡§ü", "is_rescued": False, "rescue_tag": None, "confidence_bonus": 0.0}
        if current_event != "‡§Ö‡§®‡•ç‡§Ø": return rescue_info
        text_lower = text.lower()
        tier_scores = {}
        for tier_name, tier_config in self.RESCUE_TIERS.items():
            matches = sum(1 for p in tier_config['patterns'] if re.search(p, text_lower))
            if matches > 0:
                score = min(matches / len(tier_config['patterns']), 1.0) * tier_config['weight']
                tier_scores[tier_name] = {'score': score, 'config': tier_config}
        if not tier_scores: return rescue_info
        best_tier = max(tier_scores.items(), key=lambda x: x[1]['score'])
        tier_name, tier_data = best_tier
        if tier_data['score'] > 0.5:
            rescue_info.update({"event_type": tier_data['config']['target_event'], "is_rescued": True, "rescue_tag": tier_name, "confidence_bonus": tier_data['config']['confidence_boost']})
            if 'sports' in tier_name: rescue_info["content_mode"] = "‡§ñ‡•á‡§≤ / ‡§â‡§™‡§≤‡§¨‡•ç‡§ß‡§ø ‡§™‡§∞ ‡§™‡•ç‡§∞‡§§‡§ø‡§ï‡•ç‡§∞‡§ø‡§Ø‡§æ"
            elif 'security' in tier_name or 'political' in tier_name: rescue_info["content_mode"] = "‡§®‡•Ä‡§§‡§ø / ‡§µ‡§ï‡•ç‡§§‡§µ‡•ç‡§Ø"
            else: rescue_info["content_mode"] = "‡§Æ‡•à‡§¶‡§æ‡§®-‡§∏‡•ç‡§§‡§∞ ‡§ï‡§æ‡§∞‡•ç‡§Ø‡§ï‡•ç‡§∞‡§Æ"
        return rescue_info

class EnhancedEntityExtractor:
    """Comprehensive entity extraction."""
    def extract_schemes(self, text: str) -> List[str]:
        schemes = set()
        for pattern, canonical in SCHEME_PATTERNS.items():
            if re.search(pattern, text, flags=re.IGNORECASE): schemes.add(canonical)
        return sorted(list(schemes))
    
    def extract_word_buckets(self, text: str) -> List[str]:
        text_lower = text.lower()
        buckets = set()
        for bucket_name, keywords in WORD_BUCKETS.items():
            if any(kw in text_lower for kw in keywords): buckets.add(bucket_name)
        return sorted(list(buckets))
    
    def extract_communities(self, text: str) -> List[str]:
        text_lower = text.lower()
        communities = set()
        for community_name, keywords in COMMUNITIES.items():
            if any(kw in text_lower for kw in keywords): communities.add(community_name)
        return sorted(list(communities))
    
    def extract_organizations(self, text: str) -> List[str]:
        text_lower = text.lower()
        orgs = set()
        for org_type, keywords in ORGANIZATIONS.items():
            if any(kw in text_lower for kw in keywords): orgs.add(org_type)
        return sorted(list(orgs))
    
    def extract_target_groups(self, text: str) -> List[str]:
        groups = set()
        text_lower = text.lower()
        for community_name, keywords in COMMUNITIES.items():
            if any(kw in text_lower for kw in keywords): groups.add(community_name)
        return sorted(list(groups))

class ConsensusConfidenceScorer:
    """Multi-signal consensus-based confidence scoring."""
    def calculate(self, signals: Dict[str, float]) -> float:
        weighted_sum = 0.0
        total_weight = 0.0
        for signal_name, weight in CONSENSUS_WEIGHTS.items():
            if signal_name in signals and signals[signal_name] is not None:
                weighted_sum += signals[signal_name] * weight
                total_weight += weight
        if total_weight == 0: return 0.3
        base_confidence = weighted_sum / total_weight
        high_conf_signals = sum(1 for s in signals.values() if s and s > 0.8)
        if high_conf_signals >= 3: base_confidence *= 1.1
        return min(base_confidence, 1.0)
    
    def determine_review_status(self, confidence: float, event_type: str) -> Tuple[str, bool]:
        high_precision_events = ["‡§∂‡•ã‡§ï ‡§∏‡§Ç‡§¶‡•á‡§∂", "‡§ú‡§®‡•ç‡§Æ‡§¶‡§ø‡§® ‡§∂‡•Å‡§≠‡§ï‡§æ‡§Æ‡§®‡§æ", "‡§Ü‡§Ç‡§§‡§∞‡§ø‡§ï ‡§∏‡•Å‡§∞‡§ï‡•ç‡§∑‡§æ / ‡§™‡•Å‡§≤‡§ø‡§∏", "‡§ñ‡•á‡§≤ / ‡§ó‡•å‡§∞‡§µ", "‡§Ü‡§™‡§¶‡§æ / ‡§¶‡•Å‡§∞‡•ç‡§ò‡§ü‡§®‡§æ"]
        threshold = 0.92 if event_type in high_precision_events else CONFIDENCE_AUTO_APPROVE
        if confidence >= threshold: return "auto_approved", False
        return "pending", True

# ==========================================
# GEO HIERARCHY RESOLVER (From V2)
# ==========================================

class GeoHierarchyResolver:
    """Resolve complete administrative hierarchy using comprehensive external data."""
    
    def __init__(self):
        print("Loading geography data...")
        self.villages_data = self._load_geography_ndjson()
        self.urban_data = self._load_urban_data()
        
        # Build indexes
        self.village_index = self._build_village_index()
        self.ulb_index = self._build_ulb_index()
        self.district_map = self._build_district_map()
        
        print(f"Loaded {len(self.village_index)} villages, {len(self.ulb_index)} ULBs, {len(self.district_map)} districts")
        
        self.stats = {'dict_hits': 0, 'hierarchy_hits': 0, 'not_found': 0}
        
        # Context Keywords for Disambiguation
        self.CONTEXT_KEYWORDS = {
            'urban': {
                'ward', 'zone', 'parshad', 'parishad', 'nagar', 'nigam', 'palika', 'cm', 'mayor', 'mahapaur', 
                'chairman', 'sabhapati', 'alderman', 'smart city', 'traffic', 'sadak', 'naali',
                '‡§µ‡§æ‡§∞‡•ç‡§°', '‡§ú‡•ã‡§®', '‡§™‡§æ‡§∞‡•ç‡§∑‡§¶', '‡§™‡§∞‡§ø‡§∑‡§¶', '‡§®‡§ó‡§∞', '‡§®‡§ø‡§ó‡§Æ', '‡§™‡§æ‡§≤‡§ø‡§ï‡§æ', '‡§Æ‡§π‡§æ‡§™‡•å‡§∞', '‡§∏‡§≠‡§æ‡§™‡§§‡§ø', '‡§∏‡•ç‡§Æ‡§æ‡§∞‡•ç‡§ü ‡§∏‡§ø‡§ü‡•Ä', '‡§∏‡§°‡§º‡§ï', '‡§®‡§æ‡§≤‡•Ä'
            },
            'rural': {
                'gram', 'panchayat', 'sarpanch', 'sachiv', 'janpad', 'mnrega', 'kisan', 'khet', 'fasal', 
                'kharif', 'rabi', 'paddy', 'dhan', 'gothan', 'aadiwasi', 'van', 'jungle',
                '‡§ó‡•ç‡§∞‡§æ‡§Æ', '‡§™‡§Ç‡§ö‡§æ‡§Ø‡§§', '‡§∏‡§∞‡§™‡§Ç‡§ö', '‡§∏‡§ö‡§ø‡§µ', '‡§ú‡§®‡§™‡§¶', '‡§Æ‡§®‡§∞‡•á‡§ó‡§æ', '‡§ï‡§ø‡§∏‡§æ‡§®', '‡§ñ‡•á‡§§', '‡§´‡§∏‡§≤', '‡§ñ‡§∞‡•Ä‡§´', '‡§∞‡§¨‡•Ä', '‡§ß‡§æ‡§®', '‡§ó‡•å‡§†‡§æ‡§®', '‡§Ü‡§¶‡§ø‡§µ‡§æ‡§∏‡•Ä', '‡§µ‡§®', '‡§ú‡§Ç‡§ó‡§≤'
            }
        }

    def _detect_context(self, text: str) -> str:
        """Detect if text context is predominantly urban or rural."""
        text_lower = text.lower()
        urban_score = sum(1 for kw in self.CONTEXT_KEYWORDS['urban'] if kw in text_lower)
        rural_score = sum(1 for kw in self.CONTEXT_KEYWORDS['rural'] if kw in text_lower)
        
        if urban_score > rural_score: return 'urban'
        if rural_score > urban_score: return 'rural'
        return 'neutral'

    def _load_geography_ndjson(self) -> List[Dict]:
        """Load comprehensive geography from NDJSON (17MB)."""
        ndjson_path = DATA_DIR / "datasets/chhattisgarh_geography.ndjson"
        if ndjson_path.exists():
            return load_ndjson(ndjson_path)
        print(f"‚ö†Ô∏è  Geography file not found: {ndjson_path}")
        return []

    def _load_urban_data(self) -> List[Dict]:
        if URBAN_FILE.exists(): return load_ndjson(URBAN_FILE)
        return []

    def _build_village_index(self) -> Dict[str, Dict]:
        """Build index from flat NDJSON records."""
        index = {}
        for row in self.villages_data:
            # NDJSON fields: district, block, gram_panchayat, village
            v_name_en = row.get("village")
            
            # Get Hindi name from variants
            v_name_hi = None
            if "variants" in row and "village" in row["variants"]:
                v_name_hi = row["variants"]["village"].get("hindi")
            
            dist_name = row.get("district")
            block_name = row.get("block")
            gp_name = row.get("gram_panchayat")
            
            # Build hierarchy
            hierarchy = [
                "‡§õ‡§§‡•ç‡§§‡•Ä‡§∏‡§ó‡§¢‡§º",
                f"{dist_name} ‡§ú‡§ø‡§≤‡§æ",
                f"{block_name} ‡§µ‡§ø‡§ï‡§æ‡§∏‡§ñ‡§Ç‡§°",
                f"{gp_name} ‡§™‡§Ç‡§ö‡§æ‡§Ø‡§§",
                v_name_en
            ]
            
            # Create location object
            loc_data = {
                "district": dist_name,
                "block": block_name,
                "gp": gp_name,
                "assembly": None,
                "hierarchy_path": hierarchy,
                "type": "rural",
                "canonical": v_name_en  # Use English as canonical for consistency, or Hindi if preferred
            }
            
            # Index English name
            if v_name_en:
                index[v_name_en] = loc_data
            
            # Index Hindi name
            if v_name_hi:
                # Create a copy with Hindi canonical if we want, or keep English canonical
                # For now, let's keep English canonical but allow lookup by Hindi
                index[v_name_hi] = loc_data
        
        # Apply Manual Mappings
        for hindi_name, english_name in MANUAL_VILLAGE_MAPPING.items():
            if english_name in index:
                index[hindi_name] = index[english_name]
                
        return index
    
    def _build_ulb_index(self) -> Dict[str, Dict]:
        index = {}
        for row in self.urban_data:
            ulb = row.get("ulb") or row.get("nagar_nigam") or row.get("nagar_palika")
            if ulb:
                index[ulb] = {
                    "district": row.get("district"), "ulb_type": row.get("ulb_type"),
                    "assembly": row.get("assembly"), "hierarchy_path": ["‡§õ‡§§‡•ç‡§§‡•Ä‡§∏‡§ó‡§¢‡§º", f"{row.get('district', '')} ‡§ú‡§ø‡§≤‡§æ", ulb],
                    "type": "urban", "canonical": ulb
                }
        return index

    def _build_district_map(self) -> Dict[str, Dict]:
        index = {}
        # Extract unique districts from village data
        for row in self.villages_data:
            name = row.get("district")
            if name and name not in index:
                index[name] = {"canonical": name, "hierarchy": ["‡§õ‡§§‡•ç‡§§‡•Ä‡§∏‡§ó‡§¢‡§º", f"{name} ‡§ú‡§ø‡§≤‡§æ"], "type": "district"}
        return index
    
    def resolve(self, text: str) -> Tuple[Optional[Dict], float]:
        """Multi-stage location resolution."""
        potential_matches = []
        
        # Extract candidates using V3's robust regex
        candidates = self._extract_location_candidates(text)
        all_tokens = self._extract_all_tokens(text)
        all_candidates = list(set(candidates + all_tokens))
        
        for candidate in all_candidates:
            if len(candidate) < 2: continue
            
            # Check village index
            if candidate in self.village_index:
                loc = self.village_index[candidate]
                potential_matches.append((self._format_location(loc), 0.95, 'hierarchy'))
            
            # Check ULB index
            if candidate in self.ulb_index:
                loc = self.ulb_index[candidate]
                
                # Extract ward/zone from context
                ward = self._extract_ward(text)
                zone = self._extract_zone(text)
                
                # Create a copy to avoid modifying the index
                loc_copy = loc.copy()
                loc_copy['ward'] = ward
                loc_copy['zone'] = zone
                
                if ward:
                    loc_copy['hierarchy_path'] = loc['hierarchy_path'] + [f"‡§µ‡§æ‡§∞‡•ç‡§° {ward}"]
                
                potential_matches.append((self._format_location(loc_copy), 0.90, 'hierarchy'))
            
            # Check district
            if candidate in self.district_map:
                loc = self.district_map[candidate]
                potential_matches.append((self._format_location(loc), 0.85, 'hierarchy'))
        
        if potential_matches:
            best_match = self._select_best_match(potential_matches, text)
            self.stats['hierarchy_hits'] += 1
            return best_match[0], best_match[1]
        
        self.stats['not_found'] += 1
        return None, 0.0

    def _extract_location_candidates(self, text: str) -> List[str]:
        patterns = [
            r"‡§ú‡§ø‡§≤‡§æ\s+([^\s,‡•§]+)", r"‡§µ‡§ø‡§ß‡§æ‡§®‡§∏‡§≠‡§æ\s+([^\s,‡•§]+)", r"‡§§‡§π‡§∏‡•Ä‡§≤\s+([^\s,‡•§]+)",
            r"‡§•‡§æ‡§®‡§æ\s+([^\s,‡•§]+)", r"‡§µ‡§ø‡§ï‡§æ‡§∏‡§ñ‡§Ç‡§°\s+([^\s,‡•§]+)", r"‡§ó‡•ç‡§∞‡§æ‡§Æ\s+‡§™‡§Ç‡§ö‡§æ‡§Ø‡§§\s+([^\s,‡•§]+)",
            r"‡§ó‡§æ‡§Å‡§µ\s+([^\s,‡•§]+)", r"‡§ó‡§æ‡§Ç‡§µ\s+([^\s,‡•§]+)", r"‡§ó‡•ç‡§∞‡§æ‡§Æ\s+([^\s,‡•§]+)",
            r"([^\s,‡•§]+)\s+‡§ú‡§ø‡§≤‡§æ", r"([^\s,‡•§]+)\s+‡§µ‡§ø‡§ß‡§æ‡§®‡§∏‡§≠‡§æ",
            r"‡§®‡§ó‡§∞\s+‡§®‡§ø‡§ó‡§Æ\s+([^\s,‡•§]+)", r"‡§®‡§ó‡§∞\s+‡§™‡§æ‡§≤‡§ø‡§ï‡§æ\s+([^\s,‡•§]+)", r"‡§®‡§ó‡§∞\s+‡§™‡§Ç‡§ö‡§æ‡§Ø‡§§\s+([^\s,‡•§]+)"
        ]
        candidates = []
        for pattern in patterns:
            for match in re.finditer(pattern, text):
                name = match.group(1).strip()
                if len(name) >= 2: candidates.append(name)
        return list(set(candidates))
    
    def _extract_all_tokens(self, text: str) -> List[str]:
        tokens = re.split(r'[\s,‡•§\-!?;:"]+', text)
        stop_words = {"‡§ï‡§æ", "‡§ï‡•á", "‡§ï‡•Ä", "‡§Æ‡•á‡§Ç", "‡§∏‡•á", "‡§ï‡•ã", "‡§™‡§∞", "‡§î‡§∞", "‡§π‡•à", "‡§π‡•à‡§Ç", "‡§ï‡§ø", "‡§≠‡•Ä", "‡§π‡•Ä", "‡§®‡•á", "‡§è‡§ï", "‡§ï‡§ø‡§Ø‡§æ", "‡§ï‡§∞", "‡§∞‡§π‡•á", "‡§•‡•Ä", "‡§•‡•á"}
        return [t for t in tokens if len(t) >= 2 and t not in stop_words]
    
    def _extract_ward(self, text: str) -> Optional[str]:
        """Extract ward number."""
        match = re.search(r'(?:‡§µ‡§æ‡§∞‡•ç‡§°|ward)\s*(?:‡§ï‡•ç‡§∞‡§Æ‡§æ‡§Ç‡§ï|no|number|‡§®‡§Ç‡§¨‡§∞|‡§®‡§Ç)?\s*[\.:-]?\s*(\d+)', text, re.IGNORECASE)
        if match:
            return match.group(1)
        return None

    def _extract_zone(self, text: str) -> Optional[str]:
        """Extract zone number."""
        match = re.search(r'(?:‡§ú‡•ã‡§®|zone)\s*(?:‡§ï‡•ç‡§∞‡§Æ‡§æ‡§Ç‡§ï|no|number|‡§®‡§Ç‡§¨‡§∞|‡§®‡§Ç)?\s*[\.:-]?\s*(\d+)', text, re.IGNORECASE)
        if match:
            return match.group(1)
        return None
    
    def _format_location(self, loc: Dict) -> Dict:
        return {
            'canonical': loc['canonical'],
            'district': loc.get('district', loc.get('canonical')), # For districts, district IS the canonical name
            'location_type': loc['type'],
            'hierarchy_path': loc.get('hierarchy_path', []),
            'assembly': loc.get('assembly'),
            'block': loc.get('block'),
            'gp': loc.get('gp'),
            'village': loc['canonical'] if loc['type'] == 'rural' else None,
            'ulb': loc['canonical'] if loc['type'] == 'urban' else None,
            'ward': loc.get('ward'),
            'zone': loc.get('zone'),
            'canonical_key': f"CG_{loc['type'].upper()}_{loc['canonical']}",
            'source': 'hierarchy_resolver',
            'visit_count': 1
        }
    
    def _select_best_match(self, matches: List[Tuple], text: str) -> Tuple:
        """Select best match using smart context scoring."""
        context_type = self._detect_context(text)
        text_lower = text.lower()
        
        def specificity_score(match_tuple):
            loc, conf, src = match_tuple
            score = 0.5 # Base score
            
            ltype = loc.get('location_type')
            canonical = loc.get('canonical', '').lower()
            
            # 1. Specificity Bonus
            if ltype == 'rural': score += 0.3
            elif ltype == 'urban': score += 0.2
            elif ltype == 'district': score += 0.1
            
            # 2. Context Bonus
            if context_type == 'urban' and ltype == 'urban': score += 0.5
            if context_type == 'rural' and ltype == 'rural': score += 0.5
            
            # 3. Explicit Prefix/Suffix Bonus
            # Check for "Gram <Name>" or "<Name> Gram"
            if ltype == 'rural':
                if re.search(f"(gram|panchayat)\\s+{re.escape(canonical)}", text_lower) or \
                   re.search(f"{re.escape(canonical)}\\s+(gram|panchayat)", text_lower):
                    score += 1.0
            
            # Check for "Nagar <Name>" or "<Name> Nagar" or "Ward <Name>"
            if ltype == 'urban':
                if re.search(f"(nagar|ward|zone|parshad|parishad)\\s+.*{re.escape(canonical)}", text_lower) or \
                   re.search(f"{re.escape(canonical)}\\s+(nagar|ward|zone|parshad|parishad)", text_lower):
                    score += 1.0
                
            # 4. Hierarchy Depth Bonus
            score += len(loc.get('hierarchy_path', [])) * 0.05
            
            # 5. Confidence
            score += conf
            
            return score
            
        return max(matches, key=specificity_score)
    
    def get_stats(self) -> Dict:
        return self.stats

# ==========================================
# MAIN PARSER CLASS
# ==========================================

class GeminiParserFinal:
    """
    Definitive Parser (V4) merging V2 data loading with V3 logic.
    """
    def __init__(self):
        print("Initializing Gemini Parser Final (V4)...")
        self.event_detector = MultiSignalEventDetector()
        self.rescue_detector = TieredRescueDetector()
        self.entity_extractor = EnhancedEntityExtractor()
        self.location_resolver = GeoHierarchyResolver()
        self.confidence_scorer = ConsensusConfidenceScorer()
        
        self.stats = {
            'total_tweets': 0,
            'processing_times': [],
            'event_distribution': Counter(),
            'location_type_distribution': Counter(),
            'confidence_distribution': {'high': 0, 'medium': 0, 'low': 0}
        }
        print("‚úÖ Parser initialized")
    
    def parse_tweet(self, tweet: Dict) -> Dict:
        start_time = time.time()
        text = tweet.get('text', '')
        
        # Stage 1: Event Detection
        primary_event, event_confidence, secondary_events = self.event_detector.detect(text)
        
        # Stage 2: Location Resolution
        location, location_confidence = self.location_resolver.resolve(text)
        
        # Stage 3: Entity Extraction
        schemes = self.entity_extractor.extract_schemes(text)
        word_buckets = self.entity_extractor.extract_word_buckets(text)
        communities = self.entity_extractor.extract_communities(text)
        organizations = self.entity_extractor.extract_organizations(text)
        target_groups = self.entity_extractor.extract_target_groups(text)
        
        # Stage 4: Rescue Detection
        rescue_info = self.rescue_detector.rescue(text, primary_event, location, schemes)
        if rescue_info['is_rescued']:
            primary_event = rescue_info['event_type']
            content_mode = rescue_info['content_mode']
            rescue_bonus = rescue_info['confidence_bonus']
        else:
            content_mode = "‡§°‡§ø‡§ú‡§ø‡§ü‡§≤ / ‡§∏‡•ã‡§∂‡§≤-‡§Æ‡•Ä‡§°‡§ø‡§Ø‡§æ ‡§™‡•ã‡§∏‡•ç‡§ü"
            rescue_bonus = 0.0
        
        # Stage 5: Confidence Scoring
        confidence_signals = {
            'keyword': event_confidence,
            'hierarchy': location_confidence if location else 0.0,
            'rescue': rescue_bonus,
            'dictionary': 0.0 # Dictionary lookup merged into hierarchy resolver
        }
        final_confidence = self.confidence_scorer.calculate(confidence_signals)
        review_status, needs_review = self.confidence_scorer.determine_review_status(final_confidence, primary_event)
        
        # Build parsed data
        parsed_data = {
            'event_type': primary_event,
            'event_type_secondary': secondary_events,
            'event_date': tweet.get('created_at', '')[:10] if tweet.get('created_at') else '',
            'location': location if location else {},
            'people_mentioned': [],
            'people_canonical': [],
            'schemes_mentioned': schemes,
            'word_buckets': word_buckets,
            'target_groups': target_groups,
            'communities': communities,
            'organizations': organizations,
            'hierarchy_path': location.get('hierarchy_path', []) if location else [],
            'visit_count': 1,
            'vector_embedding_id': None,
            'confidence': round(final_confidence, 2),
            'review_status': review_status,
            'needs_review': needs_review,
            'content_mode': content_mode,
            'is_other_original': primary_event == "‡§Ö‡§®‡•ç‡§Ø" and not rescue_info['is_rescued'],
            'is_rescued_other': rescue_info['is_rescued'],
            'rescue_tag': rescue_info.get('rescue_tag'),
            'rescue_confidence_bonus': rescue_bonus,
            'semantic_location_used': False,
            'location_type': location.get('location_type', '') if location else ''
        }
        
        # Stats
        processing_time = int((time.time() - start_time) * 1000)
        self.stats['processing_times'].append(processing_time)
        self.stats['event_distribution'][primary_event] += 1
        if location: self.stats['location_type_distribution'][location.get('location_type', 'unknown')] += 1
        
        output_tweet = tweet.copy()
        output_tweet['parsed_data_v8'] = parsed_data
        output_tweet['metadata_v8'] = {'model': 'gemini-parser-final', 'processing_time_ms': processing_time, 'version': VERSION}
        return output_tweet

    def parse_file(self, input_path: Path, output_dir: Path):
        input_path = Path(input_path)
        output_dir = Path(output_dir)
        output_dir.mkdir(parents=True, exist_ok=True)
        
        print(f"\nüöÄ Parsing: {input_path}")
        print(f"   Output: {output_dir}/\n")
        
        tweets = []
        with input_path.open('r', encoding='utf-8') as f:
            for line in f:
                if line.strip(): tweets.append(json.loads(line))
        
        self.stats['total_tweets'] = len(tweets)
        parsed_tweets = []
        
        for i, tweet in enumerate(tweets):
            parsed = self.parse_tweet(tweet)
            parsed_tweets.append(parsed)
            if (i + 1) % 100 == 0: print(f"   Processed {i + 1} tweets...")
        
        output_file = output_dir / "parsed_tweets_v8.jsonl"
        with output_file.open('w', encoding=OUTPUT_ENCODING) as f:
            for tweet in parsed_tweets:
                f.write(json.dumps(tweet, ensure_ascii=False) + '\n')
        
        # Stats
        avg_time = sum(self.stats['processing_times']) / len(self.stats['processing_times']) if self.stats['processing_times'] else 0
        stats_output = {
            'total_tweets': self.stats['total_tweets'],
            'event_distribution': dict(self.stats['event_distribution']),
            'location_type_distribution': dict(self.stats['location_type_distribution']),
            'average_processing_time_ms': round(avg_time, 2),
            'location_resolver_stats': self.location_resolver.get_stats(),
            'version': VERSION
        }
        with (output_dir / "parsed_tweets_v8_stats.json").open('w', encoding=OUTPUT_ENCODING) as f:
            f.write(json.dumps(stats_output, ensure_ascii=False, indent=2))
        
        print(f"\n‚úÖ Parsing complete!")
        print(f"   Total: {len(parsed_tweets)} tweets")
        print(f"   Output: {output_file}")

def main():
    parser = argparse.ArgumentParser(description='Gemini Parser Final (V4)')
    parser.add_argument('input_file', type=str, help='Input JSONL file')
    parser.add_argument('output_dir', type=str, help='Output directory')
    args = parser.parse_args()
    
    gemini_parser = GeminiParserFinal()
    gemini_parser.parse_file(Path(args.input_file), Path(args.output_dir))

if __name__ == '__main__':
    main()
